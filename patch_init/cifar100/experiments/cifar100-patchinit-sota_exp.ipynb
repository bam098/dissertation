{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5RjWLMT30mk"
   },
   "source": [
    "# CIFAR-100 Fixup Patch\n",
    "source: https://github.com/hongyi-zhang/Fixup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 75032,
     "status": "ok",
     "timestamp": 1599741648714,
     "user": {
      "displayName": "Bam Power",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgSMnvpPSpQHyFMhcdXXX00QS82LMCMbDQiszLr=s64",
      "userId": "17908920294202220192"
     },
     "user_tz": -120
    },
    "id": "x6ECwKsw3-Rf",
    "outputId": "56e78421-0f61-4c0e-8f16-d722c16ffec2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling torch-1.4.0:\n",
      "  Successfully uninstalled torch-1.4.0\n",
      "Uninstalling torchvision-0.5.0:\n",
      "  Successfully uninstalled torchvision-0.5.0\n",
      "Collecting torch==1.4.0\n",
      "  Using cached https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting torchvision==0.5.0\n",
      "  Using cached https://files.pythonhosted.org/packages/7e/90/6141bf41f5655c78e24f40f710fdd4f8a8aff6c8b7c6f0328240f649bdbe/torchvision-0.5.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (7.0.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.15.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.18.5)\n",
      "Installing collected packages: torch, torchvision\n",
      "Successfully installed torch-1.4.0 torchvision-0.5.0\n"
     ]
    }
   ],
   "source": [
    "! pip uninstall torch torchvision -y\n",
    "! pip install torch==1.4.0 torchvision==0.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 66817,
     "status": "ok",
     "timestamp": 1599741652305,
     "user": {
      "displayName": "Bam Power",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgSMnvpPSpQHyFMhcdXXX00QS82LMCMbDQiszLr=s64",
      "userId": "17908920294202220192"
     },
     "user_tz": -120
    },
    "id": "tDMEu1204CBm",
    "outputId": "ed8bd6db-3809-4e16-81f0-fe2dbee91181"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version:      3.6.9\n",
      "torch version:       1.4.0\n",
      "torchvision version: 0.5.0\n",
      "numpy version:       1.18.5\n",
      "matplotlib version:  3.2.2\n",
      "pickle version:      4.0\n",
      "CUDA available:      True\n",
      "cuDNN enabled:       True\n",
      "num gpus:            1\n",
      "gpu:                 Tesla T4\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "import numpy\n",
    "import platform\n",
    "\n",
    "print('python version:      {}'.format(platform.python_version()))\n",
    "print('torch version:       {}'.format(torch.__version__))\n",
    "print('torchvision version: {}'.format(torchvision.__version__))\n",
    "print('numpy version:       {}'.format(numpy.__version__))\n",
    "print('matplotlib version:  {}'.format(matplotlib.__version__))\n",
    "print('pickle version:      {}'.format(pickle.format_version))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('CUDA available:      {}'.format(use_cuda))\n",
    "print('cuDNN enabled:       {}'.format(torch.backends.cudnn.enabled))\n",
    "print('num gpus:            {}'.format(torch.cuda.device_count()))\n",
    "print('gpu:                 {}'.format(torch.cuda.get_device_name(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tzNULh0T-tpZ"
   },
   "source": [
    "Set CUDNN to deterministic for reproducibility reasons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0U49ghbL-xCv"
   },
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Ij5tZW6-1-O"
   },
   "source": [
    "Mount GDrive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1755,
     "status": "ok",
     "timestamp": 1599741657478,
     "user": {
      "displayName": "Bam Power",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgSMnvpPSpQHyFMhcdXXX00QS82LMCMbDQiszLr=s64",
      "userId": "17908920294202220192"
     },
     "user_tz": -120
    },
    "id": "ji6Cufbl-5M2",
    "outputId": "adcca3e4-a520-41a3-96e7-5d47aab496e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D0TbPcxf-8BX"
   },
   "outputs": [],
   "source": [
    "base_url = '/content/gdrive/My Drive/neuralnet-patchinit/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hEvH2PPf_Wau"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uyZTiIJX_Z4_"
   },
   "source": [
    "### Specify Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hiqa4ok2_lnS"
   },
   "source": [
    "Define data transform functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VaqxyTPs_jX_"
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hAiJcjAx_5F_"
   },
   "source": [
    "Define training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2389,
     "status": "ok",
     "timestamp": 1599741663150,
     "user": {
      "displayName": "Bam Power",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgSMnvpPSpQHyFMhcdXXX00QS82LMCMbDQiszLr=s64",
      "userId": "17908920294202220192"
     },
     "user_tz": -120
    },
    "id": "9ZMmNbrL_-hv",
    "outputId": "7a1c9eca-9ccb-4692-9482-6d8bd9f26b77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# training set\n",
    "trainset = torchvision.datasets.CIFAR100(\n",
    "    root='./data', train=True, download=True, transform=transform_train\n",
    ")\n",
    "\n",
    "# test set\n",
    "testset = torchvision.datasets.CIFAR100(\n",
    "    root='./data', train=False, download=True, transform=transform_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SadYE4yWAPsV"
   },
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1NnEwwFuARTL"
   },
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 635,
     "status": "ok",
     "timestamp": 1599741664293,
     "user": {
      "displayName": "Bam Power",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgSMnvpPSpQHyFMhcdXXX00QS82LMCMbDQiszLr=s64",
      "userId": "17908920294202220192"
     },
     "user_tz": -120
    },
    "id": "x9yvn84dA7l-",
    "outputId": "a2c05eb7-a3ca-49b9-f891-878222c5a035"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_gpu: 1\n",
      "batch_size: 128\n"
     ]
    }
   ],
   "source": [
    "if use_cuda:\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    batch_size *= n_gpu\n",
    "\n",
    "    print('n_gpu: {}'.format(n_gpu))\n",
    "    print('batch_size: {}'.format(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "08cmpiFnAVhc"
   },
   "source": [
    "Create train and test data loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QQKQUX2IAXIF"
   },
   "outputs": [],
   "source": [
    "# create training data loader\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=batch_size, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "# create test data loader\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=batch_size, shuffle=False, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CZuoVVikBwx4"
   },
   "source": [
    "## Define Training and Test Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mbsCKTzELGLY"
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    net.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    print('\\n=> Training Epoch #%d' % (epoch))\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        # generate mixed inputs, two one-hot label vectors and mixing coefficient\n",
    "        inputs, targets_a, targets_b, lam = mixup_data(inputs, targets, alpha, use_cuda)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        loss_func = mixup_criterion(targets_a, targets_b, lam)\n",
    "        loss = loss_func(criterion, outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (lam * predicted.eq(targets_a.data).float()).cpu().sum() + ((1 - lam) * predicted.eq(targets_b.data).float()).cpu().sum()\n",
    "        acc = 100.*float(correct)/float(total)\n",
    "\n",
    "        if batch_idx % 20 == 0:\n",
    "            print('| Epoch [%3d/%3d] Iter[%3d/%3d]\\t\\tLoss: %.4f Acc@1: %.3f%% [%3d/%3d]' % (\n",
    "                epoch, n_epoch, batch_idx+1, (len(trainset)//batch_size)+1, loss.item(), acc, correct, total\n",
    "            ))\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    net.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    all_targs = []\n",
    "    all_preds = []\n",
    "\n",
    "    print('\\n=> Testing Epoch #%d' % (epoch))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Run testing\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = cel(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "            all_preds.append(predicted.cpu().numpy())\n",
    "            all_targs.append(targets.cpu().numpy())\n",
    "\n",
    "        # Print Results\n",
    "        acc = 100.*float(correct)/float(total)\n",
    "        print(\"\\n| Validation Epoch #%d\\t\\t\\tLoss: %.4f Acc@1: %.2f%%\" %(epoch, loss.item(), acc))\n",
    "\n",
    "    all_preds = np.hstack(np.array(all_preds))\n",
    "    all_targs = np.hstack(np.array(all_targs))\n",
    "\n",
    "    return (acc, all_preds, all_targs)\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"decrease the learning rate at 100 and 150 epoch\"\"\"\n",
    "    lr = base_learning_rate\n",
    "    if epoch <= 9 and lr > 0.1:\n",
    "        # warm-up training for large minibatch\n",
    "        lr = 0.1 + (base_learning_rate - 0.1) * epoch / 10.\n",
    "    if epoch >= 100:\n",
    "        lr /= 10\n",
    "    if epoch >= 150:\n",
    "        lr /= 10\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if param_group['initial_lr'] == base_learning_rate:\n",
    "            param_group['lr'] = lr\n",
    "        else:\n",
    "            if epoch <= 9:\n",
    "                param_group['lr'] = param_group['initial_lr'] * lr / base_learning_rate\n",
    "            elif epoch < 100:\n",
    "                param_group['lr'] = param_group['initial_lr']\n",
    "            elif epoch < 150:\n",
    "                param_group['lr'] = param_group['initial_lr'] / 10.\n",
    "            else:\n",
    "                param_group['lr'] = param_group['initial_lr'] / 100.\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5QrSV6DPCTMG"
   },
   "source": [
    "## FixUp Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a1r9tITQwqPb"
   },
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=1.0, use_cuda=True, per_sample=False):\n",
    "\n",
    "    '''Compute the mixup data. Return mixed inputs, pairs of targets, and lambda'''\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    if alpha > 0. and not per_sample:\n",
    "        lam = torch.zeros(y.size()).fill_(np.random.beta(alpha, alpha)).cuda()\n",
    "        mixed_x = lam.view(-1, 1, 1, 1) * x + (1 - lam.view(-1, 1, 1, 1)) * x[index,:]\n",
    "    elif alpha > 0.:\n",
    "        lam = torch.Tensor(np.random.beta(alpha, alpha, size=y.size())).cuda()\n",
    "        mixed_x = lam.view(-1, 1, 1, 1) * x + (1 - lam.view(-1, 1, 1, 1)) * x[index,:]\n",
    "    else:\n",
    "        lam = torch.ones(y.size()).cuda()\n",
    "        mixed_x = x\n",
    "\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def mixup_lam_idx(batch_size, alpha, use_cuda=True):\n",
    "    '''Compute the mixup data. Return mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0.:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1.\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    return lam, index    \n",
    "\n",
    "\n",
    "def mixup_criterion(y_a, y_b, lam):\n",
    "    return lambda criterion, pred: criterion(pred, y_a, lam) + criterion(pred, y_b, 1 - lam)\n",
    "\n",
    "\n",
    "def get_mean_and_std(dataset):\n",
    "    '''Compute the mean and std value of dataset.'''\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n",
    "    mean = torch.zeros(3)\n",
    "    std = torch.zeros(3)\n",
    "    print('==> Computing mean and std..')\n",
    "    for inputs, targets in dataloader:\n",
    "        for i in range(3):\n",
    "            mean[i] += inputs[:,i,:,:].mean()\n",
    "            std[i] += inputs[:,i,:,:].std()\n",
    "    mean.div_(len(dataset))\n",
    "    std.div_(len(dataset))\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "def init_params(net):\n",
    "    '''Init layer parameters.'''\n",
    "    for m in net.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            init.kaiming_normal(m.weight, mode='fan_out')\n",
    "            if m.bias:\n",
    "                init.constant(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            init.constant(m.weight, 1)\n",
    "            init.constant(m.bias, 0)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            init.normal(m.weight, std=1e-3)\n",
    "            if m.bias:\n",
    "                init.constant(m.bias, 0)\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class FixupBasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(FixupBasicBlock, self).__init__()\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.bias1a = nn.Parameter(torch.zeros(1))\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bias1b = nn.Parameter(torch.zeros(1))\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.bias2a = nn.Parameter(torch.zeros(1))\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.scale = nn.Parameter(torch.ones(1))\n",
    "        self.bias2b = nn.Parameter(torch.zeros(1))\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x + self.bias1a)\n",
    "        out = self.relu(out + self.bias1b)\n",
    "\n",
    "        out = self.conv2(out + self.bias2a)\n",
    "        out = out * self.scale + self.bias2b\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x + self.bias1a)\n",
    "            identity = torch.cat((identity, torch.zeros_like(identity)), 1)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class FixupResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=100):\n",
    "        super(FixupResNet, self).__init__()\n",
    "        self.num_layers = sum(layers)\n",
    "        self.inplanes = 16\n",
    "        self.conv1 = conv3x3(3, 16)\n",
    "        self.bias1 = nn.Parameter(torch.zeros(1))\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self._make_layer(block, 16, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.bias2 = nn.Parameter(torch.zeros(1))\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, FixupBasicBlock):\n",
    "                nn.init.normal_(m.conv1.weight, mean=0, std=np.sqrt(2 / (m.conv1.weight.shape[0] * np.prod(m.conv1.weight.shape[2:]))) * self.num_layers ** (-0.5))\n",
    "                nn.init.constant_(m.conv2.weight, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.weight, 0)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1:\n",
    "            downsample = nn.AvgPool2d(1, stride=stride)\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(planes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x + self.bias1)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x + self.bias2)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def fixup_resnet20(**kwargs):\n",
    "    \"\"\"Constructs a Fixup-ResNet-20 model.\n",
    "    \"\"\"\n",
    "    model = FixupResNet(FixupBasicBlock, [3, 3, 3], **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JfIXZD1ugk_r"
   },
   "source": [
    "## Create Patch-based Weight Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZVaSWZCjHnjl"
   },
   "outputs": [],
   "source": [
    "random_seed = 1\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "patch_layer1_path = '/content/gdrive/My Drive/neuralnet-patchinit/results/cifar100-fixup-tests/patches/patches-cifar100-fixup-layer1-pca-kmeans.txt'\n",
    "patch_layer2_path = '/content/gdrive/My Drive/neuralnet-patchinit/results/cifar100-fixup-tests/patches/patches-cifar100-fixup-layer2-pca-kmeans.txt'\n",
    "patch_layer3_path = '/content/gdrive/My Drive/neuralnet-patchinit/results/cifar100-fixup-tests/patches/patches-cifar100-fixup-layer3-pca-kmeans.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AgN2iXe53Y7T"
   },
   "source": [
    "### Load Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e9dpxKLEOgsx"
   },
   "outputs": [],
   "source": [
    "def show_patches(patches):\n",
    "    print(\"tensor shape: \" + str(patches.shape))\n",
    "    \n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    for i in range(patches.shape[0]):\n",
    "        plt.subplot(4,4,i+1)\n",
    "        plt.tight_layout()\n",
    "        patch = np.moveaxis(patches[i], 0, -1)\n",
    "        plt.imshow(patch, cmap='gray', interpolation='none')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GLSQmtaLOj66"
   },
   "source": [
    "#### Patches Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1545,
     "status": "ok",
     "timestamp": 1599741673282,
     "user": {
      "displayName": "Bam Power",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgSMnvpPSpQHyFMhcdXXX00QS82LMCMbDQiszLr=s64",
      "userId": "17908920294202220192"
     },
     "user_tz": -120
    },
    "id": "5YaBHzIROmjh",
    "outputId": "beab2fed-e127-4a10-c9b9-1439a35ed4ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16, 3, 15, 15), dtype('float32'))"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(patch_layer1_path, \"rb\") as fp:\n",
    "    patches_layer1 = pickle.load(fp)\n",
    "\n",
    "patches_layer1 = patches_layer1.astype(np.float32)\n",
    "patches_layer1.shape, patches_layer1.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 574
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2145,
     "status": "ok",
     "timestamp": 1599741674683,
     "user": {
      "displayName": "Bam Power",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgSMnvpPSpQHyFMhcdXXX00QS82LMCMbDQiszLr=s64",
      "userId": "17908920294202220192"
     },
     "user_tz": -120
    },
    "id": "p4BKC_RLOrQy",
    "outputId": "4231cf1b-a78e-4877-83c4-4a9ecc8fceb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor shape: (16, 3, 15, 15)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 16 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_patches(patches_layer1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wl4JKTcJKseH"
   },
   "source": [
    "#### Patches Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 719,
     "status": "ok",
     "timestamp": 1599741676941,
     "user": {
      "displayName": "Bam Power",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgSMnvpPSpQHyFMhcdXXX00QS82LMCMbDQiszLr=s64",
      "userId": "17908920294202220192"
     },
     "user_tz": -120
    },
    "id": "snGX7Ly2rZZF",
    "outputId": "c3c9e98c-9acc-4fb9-e479-c9ccec058110"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((86, 3, 15, 15), dtype('float32'))"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(patch_layer2_path, \"rb\") as fp:\n",
    "    patches_layer2 = pickle.load(fp)\n",
    "\n",
    "patches_layer2 = patches_layer2.astype(np.float32)\n",
    "patches_layer2.shape, patches_layer2.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 574
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2072,
     "status": "ok",
     "timestamp": 1599741679417,
     "user": {
      "displayName": "Bam Power",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgSMnvpPSpQHyFMhcdXXX00QS82LMCMbDQiszLr=s64",
      "userId": "17908920294202220192"
     },
     "user_tz": -120
    },
    "id": "OVZR0QBQrZZJ",
    "outputId": "0e3553cd-c6ce-45e8-c55e-69b41584f15d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor shape: (16, 3, 15, 15)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 16 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_patches(patches_layer2[:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 481,
     "status": "ok",
     "timestamp": 1599741680257,
     "user": {
      "displayName": "Bam Power",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgSMnvpPSpQHyFMhcdXXX00QS82LMCMbDQiszLr=s64",
      "userId": "17908920294202220192"
     },
     "user_tz": -120
    },
    "id": "aoAfDCPJ8OM3",
    "outputId": "ccc95150-9850-4f68-8286-f2cbda0a3b30"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 16, 15, 15)"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches_layer2_a = patches_layer2[:85]\n",
    "patches_layer2_a = patches_layer2_a.reshape(255,1,15,15)\n",
    "patches_layer2_b = patches_layer2[85]\n",
    "patches_layer2_b_m = np.expand_dims(np.expand_dims(np.mean(patches_layer2_b, axis=0),0),0)\n",
    "patches_layer2_c = np.vstack((patches_layer2_a,patches_layer2_b_m))\n",
    "patches_layer2 = patches_layer2_c.reshape(16,16,15,15)\n",
    "\n",
    "patches_layer2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 620,
     "status": "ok",
     "timestamp": 1599741681394,
     "user": {
      "displayName": "Bam Power",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgSMnvpPSpQHyFMhcdXXX00QS82LMCMbDQiszLr=s64",
      "userId": "17908920294202220192"
     },
     "user_tz": -120
    },
    "id": "GCQGr-Xh9Ga1",
    "outputId": "596ffbc1-3887-4d40-a12f-b2ecedbedb0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff256b0bf98>"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.moveaxis(patches_layer2[3,:3], 0, -1), cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 604,
     "status": "ok",
     "timestamp": 1599741682475,
     "user": {
      "displayName": "Bam Power",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgSMnvpPSpQHyFMhcdXXX00QS82LMCMbDQiszLr=s64",
      "userId": "17908920294202220192"
     },
     "user_tz": -120
    },
    "id": "XHeO4NhYNxsQ",
    "outputId": "2eefb427-51b6-4e3a-c34e-007f661fd684"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff256bc40f0>"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.moveaxis(patches_layer2[6,:3], 0, -1), cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eM_jIl5zKy7L"
   },
   "source": [
    "#### Patches Layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1259,
     "status": "ok",
     "timestamp": 1599741685272,
     "user": {
      "displayName": "Bam Power",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgSMnvpPSpQHyFMhcdXXX00QS82LMCMbDQiszLr=s64",
      "userId": "17908920294202220192"
     },
     "user_tz": -120
    },
    "id": "UxcDyGszrflx",
    "outputId": "b645ca90-cb65-459a-813c-10deff29b6ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((342, 3, 15, 15), dtype('float32'))"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(patch_layer3_path, \"rb\") as fp:\n",
    "    patches_layer3 = pickle.load(fp)\n",
    "\n",
    "patches_layer3 = patches_layer3.astype(np.float32)\n",
    "patches_layer3.shape, patches_layer3.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 574
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1863,
     "status": "ok",
     "timestamp": 1599741686706,
     "user": {
      "displayName": "Bam Power",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgSMnvpPSpQHyFMhcdXXX00QS82LMCMbDQiszLr=s64",
      "userId": "17908920294202220192"
     },
     "user_tz": -120
    },
    "id": "hM5CmZlnrfl1",
    "outputId": "b032d404-5e23-416a-d271-27711d0ebdd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor shape: (16, 3, 15, 15)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 16 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_patches(patches_layer3[:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 456,
     "status": "ok",
     "timestamp": 1599741687485,
     "user": {
      "displayName": "Bam Power",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgSMnvpPSpQHyFMhcdXXX00QS82LMCMbDQiszLr=s64",
      "userId": "17908920294202220192"
     },
     "user_tz": -120
    },
    "id": "wpiYI59uEzUh",
    "outputId": "f85346c7-80e7-4cee-9e1a-4fd4462930a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 15, 15)"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches_layer3_a = patches_layer3[:341]\n",
    "patches_layer3_a = patches_layer3_a.reshape(1023,1,15,15)\n",
    "patches_layer3_b = patches_layer3[341]\n",
    "patches_layer3_b_m = np.expand_dims(np.expand_dims(np.mean(patches_layer3_b, axis=0),0),0)\n",
    "patches_layer3_c = np.vstack((patches_layer3_a,patches_layer3_b_m))\n",
    "patches_layer3 = patches_layer3_c.reshape(32,32,15,15)\n",
    "\n",
    "patches_layer3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 873,
     "status": "ok",
     "timestamp": 1599741688771,
     "user": {
      "displayName": "Bam Power",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgSMnvpPSpQHyFMhcdXXX00QS82LMCMbDQiszLr=s64",
      "userId": "17908920294202220192"
     },
     "user_tz": -120
    },
    "id": "0uj3STu6IuY4",
    "outputId": "dd546bac-a8da-4f41-8d03-7344870f314e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff25368b438>"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.moveaxis(patches_layer3[0,:3], 0, -1), cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 657,
     "status": "ok",
     "timestamp": 1599741689667,
     "user": {
      "displayName": "Bam Power",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgSMnvpPSpQHyFMhcdXXX00QS82LMCMbDQiszLr=s64",
      "userId": "17908920294202220192"
     },
     "user_tz": -120
    },
    "id": "KztXTSJmOQV9",
    "outputId": "ff366548-0419-4279-abdc-e5bee5e26662"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff2539a0400>"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.moveaxis(patches_layer3[21,:3], 0, -1), cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-lxBIZijOYUa"
   },
   "source": [
    "### Normalize Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h2wmg4fQiwBI"
   },
   "outputs": [],
   "source": [
    "def normalize(patches):\n",
    "    a = np.mean(patches, axis=(1,2))\n",
    "    \n",
    "    for i in range(patches.shape[0]):\n",
    "        p = patches[i]\n",
    "        p -= a[i]\n",
    "        patches[i] = p\n",
    "        \n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EdQtesKli0yA"
   },
   "outputs": [],
   "source": [
    "patches_layer1 = normalize(patches_layer1)\n",
    "patches_layer2 = normalize(patches_layer2)\n",
    "patches_layer3 = normalize(patches_layer3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oia7jJY5i_fn"
   },
   "source": [
    "### Downsample Patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7o5m_F3rLpdB"
   },
   "source": [
    "#### Downsample Layer 1 Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 517,
     "status": "ok",
     "timestamp": 1599741694739,
     "user": {
      "displayName": "Bam Power",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgSMnvpPSpQHyFMhcdXXX00QS82LMCMbDQiszLr=s64",
      "userId": "17908920294202220192"
     },
     "user_tz": -120
    },
    "id": "hL1dHC8Ai_u6",
    "outputId": "275e23d7-6573-4330-8edd-797011edf4fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor shape: torch.Size([16, 3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "patches_layer1 = torch.from_numpy(patches_layer1)\n",
    "patches_layer1 = F.interpolate(patches_layer1, size=3)\n",
    "\n",
    "print(\"tensor shape: \" + str(patches_layer1.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c4K4PCZfLtxe"
   },
   "source": [
    "#### Downsample Layer 2 Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 495,
     "status": "ok",
     "timestamp": 1599741696148,
     "user": {
      "displayName": "Bam Power",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgSMnvpPSpQHyFMhcdXXX00QS82LMCMbDQiszLr=s64",
      "userId": "17908920294202220192"
     },
     "user_tz": -120
    },
    "id": "VCkiASy2PiQ5",
    "outputId": "bf5792cf-b5f6-4b6c-a153-fc81e83fbe47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor shape: torch.Size([16, 16, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "patches_layer2 = torch.from_numpy(patches_layer2)\n",
    "patches_layer2 = F.interpolate(patches_layer2, size=3)\n",
    "\n",
    "print(\"tensor shape: \" + str(patches_layer2.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iotp2QfGLzjl"
   },
   "source": [
    "#### Downsample Layer 3 Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1176,
     "status": "ok",
     "timestamp": 1599741698729,
     "user": {
      "displayName": "Bam Power",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgSMnvpPSpQHyFMhcdXXX00QS82LMCMbDQiszLr=s64",
      "userId": "17908920294202220192"
     },
     "user_tz": -120
    },
    "id": "N4XRPCRiPirI",
    "outputId": "d8d3b8c7-4fe8-4a8f-afa5-3bfb478afee2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor shape: torch.Size([32, 32, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "patches_layer3 = torch.from_numpy(patches_layer3)\n",
    "patches_layer3 = F.interpolate(patches_layer3, size=3)\n",
    "\n",
    "print(\"tensor shape: \" + str(patches_layer3.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aYrmT3VCC2ll"
   },
   "source": [
    "## Create Random Seed Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JbIvDvCNEIas"
   },
   "source": [
    "Create 5 random numbers (between 0 and 99) for seed values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UW4ZD5sdEL2L"
   },
   "outputs": [],
   "source": [
    "#random_seed_list = np.random.choice(np.arange(100), 5)\n",
    "#random_seed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YIWmZWX9EMd2"
   },
   "outputs": [],
   "source": [
    "#with open(base_url + \"random-seeds.txt\", \"wb\") as fp:\n",
    "#  pickle.dump(random_seed_list, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2092,
     "status": "ok",
     "timestamp": 1599741703954,
     "user": {
      "displayName": "Bam Power",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgSMnvpPSpQHyFMhcdXXX00QS82LMCMbDQiszLr=s64",
      "userId": "17908920294202220192"
     },
     "user_tz": -120
    },
    "id": "opy0incyERK0",
    "outputId": "df40b238-d08d-4cd3-f751-c12961d5f504"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[76]"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(base_url + \"seeds/random-seeds.txt\", \"rb\") as fp:\n",
    "  random_seed_list = pickle.load(fp)\n",
    "\n",
    "random_seed_list = [random_seed_list[0]]; random_seed_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YHAPlgKkEYT1"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JimWFMrT0p5P"
   },
   "outputs": [],
   "source": [
    "n_epoch = 200\n",
    "\n",
    "alpha = 1.\n",
    "\n",
    "base_lr = 0.1\n",
    "base_learning_rate = base_lr * batch_size / 128.\n",
    "mom = 0.9\n",
    "decay = 1e-4\n",
    "\n",
    "use_sgdr = True\n",
    "\n",
    "result_accs_base_path = base_url + 'results/accs-cifar100-fixup-patch'\n",
    "result_preds_base_path = base_url + 'results/preds-cifar100-fixup-patch'\n",
    "result_targs_base_path = base_url + 'results/targs-cifar100-fixup-patch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f8aV0Du6_mNE"
   },
   "outputs": [],
   "source": [
    "def get_hms(seconds):\n",
    "    m, s = divmod(seconds, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "\n",
    "    return h, m, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 414,
     "status": "ok",
     "timestamp": 1599741708696,
     "user": {
      "displayName": "Bam Power",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgSMnvpPSpQHyFMhcdXXX00QS82LMCMbDQiszLr=s64",
      "userId": "17908920294202220192"
     },
     "user_tz": -120
    },
    "id": "zAoS6n9mEy-P",
    "outputId": "636bdeca-59dd-4a7c-cd92-c82890e077e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_gpu: 1\n",
      "base_learning_rate: 0.1\n"
     ]
    }
   ],
   "source": [
    "if use_cuda:\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    base_learning_rate *= n_gpu\n",
    "\n",
    "    print('n_gpu: {}'.format(n_gpu))\n",
    "    print('base_learning_rate: {}'.format(base_learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 579,
     "status": "ok",
     "timestamp": 1599741711056,
     "user": {
      "displayName": "Bam Power",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgSMnvpPSpQHyFMhcdXXX00QS82LMCMbDQiszLr=s64",
      "userId": "17908920294202220192"
     },
     "user_tz": -120
    },
    "id": "ImyDM5rg_rMI",
    "outputId": "82446190-195d-43c8-d4fa-a75d23f2043a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FixupResNet(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): FixupBasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (1): FixupBasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (2): FixupBasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): FixupBasicBlock(\n",
       "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (downsample): AvgPool2d(kernel_size=1, stride=2, padding=0)\n",
       "    )\n",
       "    (1): FixupBasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (2): FixupBasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): FixupBasicBlock(\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (downsample): AvgPool2d(kernel_size=1, stride=2, padding=0)\n",
       "    )\n",
       "    (1): FixupBasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (2): FixupBasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=64, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = fixup_resnet20(); net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "md_hqlOwHpVk"
   },
   "source": [
    "Create model and start training for each of the 5 seeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 42408,
     "status": "ok",
     "timestamp": 1599745285917,
     "user": {
      "displayName": "Bam Power",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgSMnvpPSpQHyFMhcdXXX00QS82LMCMbDQiszLr=s64",
      "userId": "17908920294202220192"
     },
     "user_tz": -120
    },
    "id": "HVQhVdiBHn5m",
    "outputId": "241c7aa4-e062-420d-bdae-e552b21b64b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mDie letzten 5000 Zeilen der Streamingausgabe wurden abgeschnitten.\u001b[0m\n",
      "| Epoch [ 34/200] Iter[301/391]\t\tLoss: 2.3664 Acc@1: 31.083% [11975/38528]\n",
      "| Epoch [ 34/200] Iter[321/391]\t\tLoss: 3.3758 Acc@1: 31.150% [12798/41088]\n",
      "| Epoch [ 34/200] Iter[341/391]\t\tLoss: 3.4409 Acc@1: 31.247% [13638/43648]\n",
      "| Epoch [ 34/200] Iter[361/391]\t\tLoss: 3.7646 Acc@1: 30.905% [14280/46208]\n",
      "| Epoch [ 34/200] Iter[381/391]\t\tLoss: 2.5240 Acc@1: 30.905% [15071/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #34\n",
      "\n",
      "| Validation Epoch #34\t\t\tLoss: 2.0833 Acc@1: 44.21%\n",
      "\n",
      "----- Elapsed time : 0:10:08\n",
      "\n",
      "\n",
      "=> Training Epoch #35\n",
      "| Epoch [ 35/200] Iter[  1/391]\t\tLoss: 3.2981 Acc@1: 28.935% [ 37/128]\n",
      "| Epoch [ 35/200] Iter[ 21/391]\t\tLoss: 3.3339 Acc@1: 33.761% [907/2688]\n",
      "| Epoch [ 35/200] Iter[ 41/391]\t\tLoss: 3.0873 Acc@1: 32.464% [1703/5248]\n",
      "| Epoch [ 35/200] Iter[ 61/391]\t\tLoss: 2.6635 Acc@1: 33.246% [2595/7808]\n",
      "| Epoch [ 35/200] Iter[ 81/391]\t\tLoss: 3.6910 Acc@1: 33.067% [3428/10368]\n",
      "| Epoch [ 35/200] Iter[101/391]\t\tLoss: 3.0872 Acc@1: 33.149% [4285/12928]\n",
      "| Epoch [ 35/200] Iter[121/391]\t\tLoss: 2.8990 Acc@1: 32.392% [5016/15488]\n",
      "| Epoch [ 35/200] Iter[141/391]\t\tLoss: 2.6187 Acc@1: 32.105% [5794/18048]\n",
      "| Epoch [ 35/200] Iter[161/391]\t\tLoss: 3.5951 Acc@1: 32.318% [6660/20608]\n",
      "| Epoch [ 35/200] Iter[181/391]\t\tLoss: 3.1763 Acc@1: 31.867% [7383/23168]\n",
      "| Epoch [ 35/200] Iter[201/391]\t\tLoss: 2.9364 Acc@1: 31.542% [8115/25728]\n",
      "| Epoch [ 35/200] Iter[221/391]\t\tLoss: 2.5217 Acc@1: 31.587% [8935/28288]\n",
      "| Epoch [ 35/200] Iter[241/391]\t\tLoss: 3.4810 Acc@1: 31.345% [9669/30848]\n",
      "| Epoch [ 35/200] Iter[261/391]\t\tLoss: 2.6602 Acc@1: 31.224% [10431/33408]\n",
      "| Epoch [ 35/200] Iter[281/391]\t\tLoss: 3.5097 Acc@1: 31.060% [11171/35968]\n",
      "| Epoch [ 35/200] Iter[301/391]\t\tLoss: 3.5414 Acc@1: 30.910% [11908/38528]\n",
      "| Epoch [ 35/200] Iter[321/391]\t\tLoss: 3.2262 Acc@1: 30.859% [12679/41088]\n",
      "| Epoch [ 35/200] Iter[341/391]\t\tLoss: 2.6158 Acc@1: 30.915% [13493/43648]\n",
      "| Epoch [ 35/200] Iter[361/391]\t\tLoss: 3.3916 Acc@1: 30.855% [14257/46208]\n",
      "| Epoch [ 35/200] Iter[381/391]\t\tLoss: 2.3012 Acc@1: 30.901% [15069/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #35\n",
      "\n",
      "| Validation Epoch #35\t\t\tLoss: 2.0568 Acc@1: 46.81%\n",
      "\n",
      "----- Elapsed time : 0:10:26\n",
      "\n",
      "\n",
      "=> Training Epoch #36\n",
      "| Epoch [ 36/200] Iter[  1/391]\t\tLoss: 3.3962 Acc@1: 28.021% [ 35/128]\n",
      "| Epoch [ 36/200] Iter[ 21/391]\t\tLoss: 3.6109 Acc@1: 32.011% [860/2688]\n",
      "| Epoch [ 36/200] Iter[ 41/391]\t\tLoss: 3.6167 Acc@1: 33.323% [1748/5248]\n",
      "| Epoch [ 36/200] Iter[ 61/391]\t\tLoss: 3.4870 Acc@1: 33.949% [2650/7808]\n",
      "| Epoch [ 36/200] Iter[ 81/391]\t\tLoss: 3.4635 Acc@1: 33.906% [3515/10368]\n",
      "| Epoch [ 36/200] Iter[101/391]\t\tLoss: 3.4842 Acc@1: 33.489% [4329/12928]\n",
      "| Epoch [ 36/200] Iter[121/391]\t\tLoss: 3.4679 Acc@1: 33.278% [5154/15488]\n",
      "| Epoch [ 36/200] Iter[141/391]\t\tLoss: 2.5858 Acc@1: 32.863% [5931/18048]\n",
      "| Epoch [ 36/200] Iter[161/391]\t\tLoss: 3.7775 Acc@1: 32.183% [6632/20608]\n",
      "| Epoch [ 36/200] Iter[181/391]\t\tLoss: 3.5237 Acc@1: 32.098% [7436/23168]\n",
      "| Epoch [ 36/200] Iter[201/391]\t\tLoss: 1.8748 Acc@1: 32.263% [8300/25728]\n",
      "| Epoch [ 36/200] Iter[221/391]\t\tLoss: 2.6063 Acc@1: 31.947% [9037/28288]\n",
      "| Epoch [ 36/200] Iter[241/391]\t\tLoss: 2.4933 Acc@1: 32.024% [9878/30848]\n",
      "| Epoch [ 36/200] Iter[261/391]\t\tLoss: 3.3424 Acc@1: 31.859% [10643/33408]\n",
      "| Epoch [ 36/200] Iter[281/391]\t\tLoss: 3.2922 Acc@1: 31.957% [11494/35968]\n",
      "| Epoch [ 36/200] Iter[301/391]\t\tLoss: 1.9550 Acc@1: 32.081% [12360/38528]\n",
      "| Epoch [ 36/200] Iter[321/391]\t\tLoss: 3.2696 Acc@1: 32.086% [13183/41088]\n",
      "| Epoch [ 36/200] Iter[341/391]\t\tLoss: 2.0660 Acc@1: 32.118% [14018/43648]\n",
      "| Epoch [ 36/200] Iter[361/391]\t\tLoss: 2.4121 Acc@1: 32.054% [14811/46208]\n",
      "| Epoch [ 36/200] Iter[381/391]\t\tLoss: 3.6589 Acc@1: 32.008% [15609/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #36\n",
      "\n",
      "| Validation Epoch #36\t\t\tLoss: 1.8023 Acc@1: 46.33%\n",
      "\n",
      "----- Elapsed time : 0:10:43\n",
      "\n",
      "\n",
      "=> Training Epoch #37\n",
      "| Epoch [ 37/200] Iter[  1/391]\t\tLoss: 3.6134 Acc@1: 18.551% [ 23/128]\n",
      "| Epoch [ 37/200] Iter[ 21/391]\t\tLoss: 3.3755 Acc@1: 33.902% [911/2688]\n",
      "| Epoch [ 37/200] Iter[ 41/391]\t\tLoss: 3.5321 Acc@1: 33.046% [1734/5248]\n",
      "| Epoch [ 37/200] Iter[ 61/391]\t\tLoss: 3.5832 Acc@1: 32.582% [2543/7808]\n",
      "| Epoch [ 37/200] Iter[ 81/391]\t\tLoss: 3.3962 Acc@1: 32.610% [3381/10368]\n",
      "| Epoch [ 37/200] Iter[101/391]\t\tLoss: 3.4277 Acc@1: 32.782% [4238/12928]\n",
      "| Epoch [ 37/200] Iter[121/391]\t\tLoss: 2.4056 Acc@1: 32.505% [5034/15488]\n",
      "| Epoch [ 37/200] Iter[141/391]\t\tLoss: 3.2985 Acc@1: 32.456% [5857/18048]\n",
      "| Epoch [ 37/200] Iter[161/391]\t\tLoss: 3.1126 Acc@1: 32.589% [6715/20608]\n",
      "| Epoch [ 37/200] Iter[181/391]\t\tLoss: 3.4532 Acc@1: 32.165% [7452/23168]\n",
      "| Epoch [ 37/200] Iter[201/391]\t\tLoss: 3.1411 Acc@1: 32.272% [8302/25728]\n",
      "| Epoch [ 37/200] Iter[221/391]\t\tLoss: 3.5927 Acc@1: 31.692% [8965/28288]\n",
      "| Epoch [ 37/200] Iter[241/391]\t\tLoss: 3.2289 Acc@1: 31.384% [9681/30848]\n",
      "| Epoch [ 37/200] Iter[261/391]\t\tLoss: 2.7468 Acc@1: 31.388% [10486/33408]\n",
      "| Epoch [ 37/200] Iter[281/391]\t\tLoss: 3.4363 Acc@1: 31.424% [11302/35968]\n",
      "| Epoch [ 37/200] Iter[301/391]\t\tLoss: 2.1171 Acc@1: 31.587% [12169/38528]\n",
      "| Epoch [ 37/200] Iter[321/391]\t\tLoss: 2.6234 Acc@1: 31.555% [12965/41088]\n",
      "| Epoch [ 37/200] Iter[341/391]\t\tLoss: 2.3461 Acc@1: 31.838% [13896/43648]\n",
      "| Epoch [ 37/200] Iter[361/391]\t\tLoss: 2.7737 Acc@1: 31.757% [14674/46208]\n",
      "| Epoch [ 37/200] Iter[381/391]\t\tLoss: 2.3583 Acc@1: 31.727% [15472/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #37\n",
      "\n",
      "| Validation Epoch #37\t\t\tLoss: 1.9285 Acc@1: 46.52%\n",
      "\n",
      "----- Elapsed time : 0:11:01\n",
      "\n",
      "\n",
      "=> Training Epoch #38\n",
      "| Epoch [ 38/200] Iter[  1/391]\t\tLoss: 1.7580 Acc@1: 60.382% [ 77/128]\n",
      "| Epoch [ 38/200] Iter[ 21/391]\t\tLoss: 3.6312 Acc@1: 32.456% [872/2688]\n",
      "| Epoch [ 38/200] Iter[ 41/391]\t\tLoss: 3.5043 Acc@1: 32.099% [1684/5248]\n",
      "| Epoch [ 38/200] Iter[ 61/391]\t\tLoss: 2.5976 Acc@1: 31.299% [2443/7808]\n",
      "| Epoch [ 38/200] Iter[ 81/391]\t\tLoss: 3.4557 Acc@1: 31.076% [3221/10368]\n",
      "| Epoch [ 38/200] Iter[101/391]\t\tLoss: 3.3399 Acc@1: 31.113% [4022/12928]\n",
      "| Epoch [ 38/200] Iter[121/391]\t\tLoss: 1.8747 Acc@1: 31.964% [4950/15488]\n",
      "| Epoch [ 38/200] Iter[141/391]\t\tLoss: 3.3161 Acc@1: 31.822% [5743/18048]\n",
      "| Epoch [ 38/200] Iter[161/391]\t\tLoss: 3.6050 Acc@1: 32.043% [6603/20608]\n",
      "| Epoch [ 38/200] Iter[181/391]\t\tLoss: 1.9957 Acc@1: 32.250% [7471/23168]\n",
      "| Epoch [ 38/200] Iter[201/391]\t\tLoss: 2.3129 Acc@1: 32.677% [8407/25728]\n",
      "| Epoch [ 38/200] Iter[221/391]\t\tLoss: 3.1355 Acc@1: 32.525% [9200/28288]\n",
      "| Epoch [ 38/200] Iter[241/391]\t\tLoss: 3.3928 Acc@1: 32.672% [10078/30848]\n",
      "| Epoch [ 38/200] Iter[261/391]\t\tLoss: 3.4262 Acc@1: 32.640% [10904/33408]\n",
      "| Epoch [ 38/200] Iter[281/391]\t\tLoss: 3.6119 Acc@1: 32.569% [11714/35968]\n",
      "| Epoch [ 38/200] Iter[301/391]\t\tLoss: 3.4624 Acc@1: 32.895% [12673/38528]\n",
      "| Epoch [ 38/200] Iter[321/391]\t\tLoss: 2.9591 Acc@1: 33.005% [13561/41088]\n",
      "| Epoch [ 38/200] Iter[341/391]\t\tLoss: 3.5834 Acc@1: 32.958% [14385/43648]\n",
      "| Epoch [ 38/200] Iter[361/391]\t\tLoss: 3.5702 Acc@1: 32.860% [15183/46208]\n",
      "| Epoch [ 38/200] Iter[381/391]\t\tLoss: 2.4624 Acc@1: 32.884% [16036/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #38\n",
      "\n",
      "| Validation Epoch #38\t\t\tLoss: 1.7471 Acc@1: 48.00%\n",
      "\n",
      "----- Elapsed time : 0:11:18\n",
      "\n",
      "\n",
      "=> Training Epoch #39\n",
      "| Epoch [ 39/200] Iter[  1/391]\t\tLoss: 3.3790 Acc@1: 23.561% [ 30/128]\n",
      "| Epoch [ 39/200] Iter[ 21/391]\t\tLoss: 2.9179 Acc@1: 30.637% [823/2688]\n",
      "| Epoch [ 39/200] Iter[ 41/391]\t\tLoss: 3.5305 Acc@1: 34.382% [1804/5248]\n",
      "| Epoch [ 39/200] Iter[ 61/391]\t\tLoss: 2.5071 Acc@1: 35.529% [2774/7808]\n",
      "| Epoch [ 39/200] Iter[ 81/391]\t\tLoss: 2.5340 Acc@1: 34.622% [3589/10368]\n",
      "| Epoch [ 39/200] Iter[101/391]\t\tLoss: 3.3100 Acc@1: 34.706% [4486/12928]\n",
      "| Epoch [ 39/200] Iter[121/391]\t\tLoss: 1.8293 Acc@1: 34.855% [5398/15488]\n",
      "| Epoch [ 39/200] Iter[141/391]\t\tLoss: 2.5063 Acc@1: 34.647% [6253/18048]\n",
      "| Epoch [ 39/200] Iter[161/391]\t\tLoss: 2.7705 Acc@1: 34.418% [7092/20608]\n",
      "| Epoch [ 39/200] Iter[181/391]\t\tLoss: 2.0671 Acc@1: 34.124% [7905/23168]\n",
      "| Epoch [ 39/200] Iter[201/391]\t\tLoss: 2.3974 Acc@1: 34.116% [8777/25728]\n",
      "| Epoch [ 39/200] Iter[221/391]\t\tLoss: 3.6018 Acc@1: 33.987% [9614/28288]\n",
      "| Epoch [ 39/200] Iter[241/391]\t\tLoss: 3.4110 Acc@1: 33.823% [10433/30848]\n",
      "| Epoch [ 39/200] Iter[261/391]\t\tLoss: 3.2067 Acc@1: 33.939% [11338/33408]\n",
      "| Epoch [ 39/200] Iter[281/391]\t\tLoss: 3.6854 Acc@1: 33.744% [12136/35968]\n",
      "| Epoch [ 39/200] Iter[301/391]\t\tLoss: 3.5196 Acc@1: 33.783% [13015/38528]\n",
      "| Epoch [ 39/200] Iter[321/391]\t\tLoss: 3.2836 Acc@1: 33.513% [13769/41088]\n",
      "| Epoch [ 39/200] Iter[341/391]\t\tLoss: 3.1441 Acc@1: 33.259% [14516/43648]\n",
      "| Epoch [ 39/200] Iter[361/391]\t\tLoss: 3.3723 Acc@1: 33.374% [15421/46208]\n",
      "| Epoch [ 39/200] Iter[381/391]\t\tLoss: 2.5963 Acc@1: 33.192% [16186/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #39\n",
      "\n",
      "| Validation Epoch #39\t\t\tLoss: 1.9029 Acc@1: 48.43%\n",
      "\n",
      "----- Elapsed time : 0:11:35\n",
      "\n",
      "\n",
      "=> Training Epoch #40\n",
      "| Epoch [ 40/200] Iter[  1/391]\t\tLoss: 3.1528 Acc@1: 30.055% [ 38/128]\n",
      "| Epoch [ 40/200] Iter[ 21/391]\t\tLoss: 2.6009 Acc@1: 32.065% [861/2688]\n",
      "| Epoch [ 40/200] Iter[ 41/391]\t\tLoss: 3.4842 Acc@1: 31.606% [1658/5248]\n",
      "| Epoch [ 40/200] Iter[ 61/391]\t\tLoss: 3.3949 Acc@1: 32.456% [2534/7808]\n",
      "| Epoch [ 40/200] Iter[ 81/391]\t\tLoss: 2.7520 Acc@1: 32.991% [3420/10368]\n",
      "| Epoch [ 40/200] Iter[101/391]\t\tLoss: 2.8705 Acc@1: 32.633% [4218/12928]\n",
      "| Epoch [ 40/200] Iter[121/391]\t\tLoss: 2.8300 Acc@1: 33.222% [5145/15488]\n",
      "| Epoch [ 40/200] Iter[141/391]\t\tLoss: 2.5679 Acc@1: 33.438% [6034/18048]\n",
      "| Epoch [ 40/200] Iter[161/391]\t\tLoss: 3.6258 Acc@1: 33.428% [6888/20608]\n",
      "| Epoch [ 40/200] Iter[181/391]\t\tLoss: 2.3607 Acc@1: 33.580% [7779/23168]\n",
      "| Epoch [ 40/200] Iter[201/391]\t\tLoss: 2.8151 Acc@1: 33.402% [8593/25728]\n",
      "| Epoch [ 40/200] Iter[221/391]\t\tLoss: 3.3601 Acc@1: 33.267% [9410/28288]\n",
      "| Epoch [ 40/200] Iter[241/391]\t\tLoss: 2.6646 Acc@1: 33.580% [10358/30848]\n",
      "| Epoch [ 40/200] Iter[261/391]\t\tLoss: 2.2101 Acc@1: 33.568% [11214/33408]\n",
      "| Epoch [ 40/200] Iter[281/391]\t\tLoss: 3.6079 Acc@1: 33.335% [11989/35968]\n",
      "| Epoch [ 40/200] Iter[301/391]\t\tLoss: 2.1786 Acc@1: 33.723% [12992/38528]\n",
      "| Epoch [ 40/200] Iter[321/391]\t\tLoss: 2.7464 Acc@1: 33.789% [13883/41088]\n",
      "| Epoch [ 40/200] Iter[341/391]\t\tLoss: 3.4401 Acc@1: 33.770% [14739/43648]\n",
      "| Epoch [ 40/200] Iter[361/391]\t\tLoss: 2.4428 Acc@1: 33.979% [15701/46208]\n",
      "| Epoch [ 40/200] Iter[381/391]\t\tLoss: 2.6238 Acc@1: 34.165% [16661/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #40\n",
      "\n",
      "| Validation Epoch #40\t\t\tLoss: 1.7564 Acc@1: 47.71%\n",
      "\n",
      "----- Elapsed time : 0:11:53\n",
      "\n",
      "\n",
      "=> Training Epoch #41\n",
      "| Epoch [ 41/200] Iter[  1/391]\t\tLoss: 3.4487 Acc@1: 24.395% [ 31/128]\n",
      "| Epoch [ 41/200] Iter[ 21/391]\t\tLoss: 3.3055 Acc@1: 32.414% [871/2688]\n",
      "| Epoch [ 41/200] Iter[ 41/391]\t\tLoss: 3.3274 Acc@1: 33.090% [1736/5248]\n",
      "| Epoch [ 41/200] Iter[ 61/391]\t\tLoss: 3.6877 Acc@1: 33.859% [2643/7808]\n",
      "| Epoch [ 41/200] Iter[ 81/391]\t\tLoss: 3.5876 Acc@1: 33.760% [3500/10368]\n",
      "| Epoch [ 41/200] Iter[101/391]\t\tLoss: 3.5679 Acc@1: 32.941% [4258/12928]\n",
      "| Epoch [ 41/200] Iter[121/391]\t\tLoss: 2.7751 Acc@1: 34.098% [5281/15488]\n",
      "| Epoch [ 41/200] Iter[141/391]\t\tLoss: 3.2153 Acc@1: 33.771% [6094/18048]\n",
      "| Epoch [ 41/200] Iter[161/391]\t\tLoss: 2.3870 Acc@1: 33.883% [6982/20608]\n",
      "| Epoch [ 41/200] Iter[181/391]\t\tLoss: 3.2761 Acc@1: 33.650% [7796/23168]\n",
      "| Epoch [ 41/200] Iter[201/391]\t\tLoss: 3.5976 Acc@1: 33.910% [8724/25728]\n",
      "| Epoch [ 41/200] Iter[221/391]\t\tLoss: 3.1650 Acc@1: 34.206% [9676/28288]\n",
      "| Epoch [ 41/200] Iter[241/391]\t\tLoss: 3.7189 Acc@1: 34.298% [10580/30848]\n",
      "| Epoch [ 41/200] Iter[261/391]\t\tLoss: 3.2674 Acc@1: 34.672% [11583/33408]\n",
      "| Epoch [ 41/200] Iter[281/391]\t\tLoss: 2.4847 Acc@1: 34.818% [12523/35968]\n",
      "| Epoch [ 41/200] Iter[301/391]\t\tLoss: 3.2122 Acc@1: 34.517% [13298/38528]\n",
      "| Epoch [ 41/200] Iter[321/391]\t\tLoss: 3.4031 Acc@1: 34.418% [14141/41088]\n",
      "| Epoch [ 41/200] Iter[341/391]\t\tLoss: 3.7215 Acc@1: 34.455% [15038/43648]\n",
      "| Epoch [ 41/200] Iter[361/391]\t\tLoss: 3.5890 Acc@1: 34.238% [15820/46208]\n",
      "| Epoch [ 41/200] Iter[381/391]\t\tLoss: 3.5137 Acc@1: 34.192% [16674/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #41\n",
      "\n",
      "| Validation Epoch #41\t\t\tLoss: 1.8741 Acc@1: 49.49%\n",
      "\n",
      "----- Elapsed time : 0:12:10\n",
      "\n",
      "\n",
      "=> Training Epoch #42\n",
      "| Epoch [ 42/200] Iter[  1/391]\t\tLoss: 3.3482 Acc@1: 29.896% [ 38/128]\n",
      "| Epoch [ 42/200] Iter[ 21/391]\t\tLoss: 2.0640 Acc@1: 28.791% [773/2688]\n",
      "| Epoch [ 42/200] Iter[ 41/391]\t\tLoss: 3.2569 Acc@1: 31.573% [1656/5248]\n",
      "| Epoch [ 42/200] Iter[ 61/391]\t\tLoss: 2.2644 Acc@1: 33.140% [2587/7808]\n",
      "| Epoch [ 42/200] Iter[ 81/391]\t\tLoss: 2.6011 Acc@1: 33.406% [3463/10368]\n",
      "| Epoch [ 42/200] Iter[101/391]\t\tLoss: 2.5840 Acc@1: 34.697% [4485/12928]\n",
      "| Epoch [ 42/200] Iter[121/391]\t\tLoss: 3.4724 Acc@1: 34.156% [5290/15488]\n",
      "| Epoch [ 42/200] Iter[141/391]\t\tLoss: 3.5984 Acc@1: 34.631% [6250/18048]\n",
      "| Epoch [ 42/200] Iter[161/391]\t\tLoss: 2.4937 Acc@1: 34.690% [7148/20608]\n",
      "| Epoch [ 42/200] Iter[181/391]\t\tLoss: 3.6260 Acc@1: 34.486% [7989/23168]\n",
      "| Epoch [ 42/200] Iter[201/391]\t\tLoss: 3.2623 Acc@1: 34.079% [8767/25728]\n",
      "| Epoch [ 42/200] Iter[221/391]\t\tLoss: 3.1760 Acc@1: 34.285% [9698/28288]\n",
      "| Epoch [ 42/200] Iter[241/391]\t\tLoss: 2.5611 Acc@1: 34.525% [10650/30848]\n",
      "| Epoch [ 42/200] Iter[261/391]\t\tLoss: 2.5916 Acc@1: 34.743% [11607/33408]\n",
      "| Epoch [ 42/200] Iter[281/391]\t\tLoss: 3.6436 Acc@1: 34.773% [12507/35968]\n",
      "| Epoch [ 42/200] Iter[301/391]\t\tLoss: 2.6167 Acc@1: 34.617% [13337/38528]\n",
      "| Epoch [ 42/200] Iter[321/391]\t\tLoss: 3.1901 Acc@1: 34.373% [14123/41088]\n",
      "| Epoch [ 42/200] Iter[341/391]\t\tLoss: 2.4324 Acc@1: 34.183% [14920/43648]\n",
      "| Epoch [ 42/200] Iter[361/391]\t\tLoss: 3.7593 Acc@1: 34.125% [15768/46208]\n",
      "| Epoch [ 42/200] Iter[381/391]\t\tLoss: 3.4925 Acc@1: 34.022% [16591/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #42\n",
      "\n",
      "| Validation Epoch #42\t\t\tLoss: 2.3379 Acc@1: 48.50%\n",
      "\n",
      "----- Elapsed time : 0:12:28\n",
      "\n",
      "\n",
      "=> Training Epoch #43\n",
      "| Epoch [ 43/200] Iter[  1/391]\t\tLoss: 3.1918 Acc@1: 27.856% [ 35/128]\n",
      "| Epoch [ 43/200] Iter[ 21/391]\t\tLoss: 2.7686 Acc@1: 35.663% [958/2688]\n",
      "| Epoch [ 43/200] Iter[ 41/391]\t\tLoss: 3.6708 Acc@1: 33.873% [1777/5248]\n",
      "| Epoch [ 43/200] Iter[ 61/391]\t\tLoss: 2.6524 Acc@1: 34.984% [2731/7808]\n",
      "| Epoch [ 43/200] Iter[ 81/391]\t\tLoss: 3.0446 Acc@1: 35.969% [3729/10368]\n",
      "| Epoch [ 43/200] Iter[101/391]\t\tLoss: 3.0439 Acc@1: 35.764% [4623/12928]\n",
      "| Epoch [ 43/200] Iter[121/391]\t\tLoss: 3.4386 Acc@1: 35.598% [5513/15488]\n",
      "| Epoch [ 43/200] Iter[141/391]\t\tLoss: 2.5505 Acc@1: 35.237% [6359/18048]\n",
      "| Epoch [ 43/200] Iter[161/391]\t\tLoss: 2.6212 Acc@1: 35.630% [7342/20608]\n",
      "| Epoch [ 43/200] Iter[181/391]\t\tLoss: 3.5541 Acc@1: 35.203% [8155/23168]\n",
      "| Epoch [ 43/200] Iter[201/391]\t\tLoss: 3.4379 Acc@1: 35.040% [9015/25728]\n",
      "| Epoch [ 43/200] Iter[221/391]\t\tLoss: 3.3835 Acc@1: 35.326% [9993/28288]\n",
      "| Epoch [ 43/200] Iter[241/391]\t\tLoss: 3.3424 Acc@1: 35.571% [10972/30848]\n",
      "| Epoch [ 43/200] Iter[261/391]\t\tLoss: 2.2686 Acc@1: 35.295% [11791/33408]\n",
      "| Epoch [ 43/200] Iter[281/391]\t\tLoss: 2.8187 Acc@1: 35.648% [12822/35968]\n",
      "| Epoch [ 43/200] Iter[301/391]\t\tLoss: 3.1806 Acc@1: 35.485% [13671/38528]\n",
      "| Epoch [ 43/200] Iter[321/391]\t\tLoss: 1.9663 Acc@1: 35.702% [14669/41088]\n",
      "| Epoch [ 43/200] Iter[341/391]\t\tLoss: 2.8846 Acc@1: 35.694% [15579/43648]\n",
      "| Epoch [ 43/200] Iter[361/391]\t\tLoss: 2.8519 Acc@1: 35.662% [16478/46208]\n",
      "| Epoch [ 43/200] Iter[381/391]\t\tLoss: 2.6707 Acc@1: 35.578% [17350/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #43\n",
      "\n",
      "| Validation Epoch #43\t\t\tLoss: 1.5957 Acc@1: 49.99%\n",
      "\n",
      "----- Elapsed time : 0:12:46\n",
      "\n",
      "\n",
      "=> Training Epoch #44\n",
      "| Epoch [ 44/200] Iter[  1/391]\t\tLoss: 3.6253 Acc@1: 18.290% [ 23/128]\n",
      "| Epoch [ 44/200] Iter[ 21/391]\t\tLoss: 2.8906 Acc@1: 37.271% [1001/2688]\n",
      "| Epoch [ 44/200] Iter[ 41/391]\t\tLoss: 3.4126 Acc@1: 37.182% [1951/5248]\n",
      "| Epoch [ 44/200] Iter[ 61/391]\t\tLoss: 3.5653 Acc@1: 35.244% [2751/7808]\n",
      "| Epoch [ 44/200] Iter[ 81/391]\t\tLoss: 2.2502 Acc@1: 35.768% [3708/10368]\n",
      "| Epoch [ 44/200] Iter[101/391]\t\tLoss: 2.0908 Acc@1: 35.741% [4620/12928]\n",
      "| Epoch [ 44/200] Iter[121/391]\t\tLoss: 1.9945 Acc@1: 36.189% [5605/15488]\n",
      "| Epoch [ 44/200] Iter[141/391]\t\tLoss: 3.5748 Acc@1: 36.489% [6585/18048]\n",
      "| Epoch [ 44/200] Iter[161/391]\t\tLoss: 2.2031 Acc@1: 36.343% [7489/20608]\n",
      "| Epoch [ 44/200] Iter[181/391]\t\tLoss: 2.9120 Acc@1: 36.221% [8391/23168]\n",
      "| Epoch [ 44/200] Iter[201/391]\t\tLoss: 3.6053 Acc@1: 36.340% [9349/25728]\n",
      "| Epoch [ 44/200] Iter[221/391]\t\tLoss: 2.5502 Acc@1: 36.117% [10216/28288]\n",
      "| Epoch [ 44/200] Iter[241/391]\t\tLoss: 1.9243 Acc@1: 36.155% [11153/30848]\n",
      "| Epoch [ 44/200] Iter[261/391]\t\tLoss: 3.1800 Acc@1: 35.734% [11938/33408]\n",
      "| Epoch [ 44/200] Iter[281/391]\t\tLoss: 3.3671 Acc@1: 35.453% [12751/35968]\n",
      "| Epoch [ 44/200] Iter[301/391]\t\tLoss: 2.1912 Acc@1: 34.954% [13467/38528]\n",
      "| Epoch [ 44/200] Iter[321/391]\t\tLoss: 2.0689 Acc@1: 34.853% [14320/41088]\n",
      "| Epoch [ 44/200] Iter[341/391]\t\tLoss: 3.5153 Acc@1: 34.699% [15145/43648]\n",
      "| Epoch [ 44/200] Iter[361/391]\t\tLoss: 3.5597 Acc@1: 34.640% [16006/46208]\n",
      "| Epoch [ 44/200] Iter[381/391]\t\tLoss: 2.5063 Acc@1: 34.695% [16920/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #44\n",
      "\n",
      "| Validation Epoch #44\t\t\tLoss: 1.9618 Acc@1: 48.94%\n",
      "\n",
      "----- Elapsed time : 0:13:03\n",
      "\n",
      "\n",
      "=> Training Epoch #45\n",
      "| Epoch [ 45/200] Iter[  1/391]\t\tLoss: 2.5276 Acc@1: 43.955% [ 56/128]\n",
      "| Epoch [ 45/200] Iter[ 21/391]\t\tLoss: 2.1528 Acc@1: 35.806% [962/2688]\n",
      "| Epoch [ 45/200] Iter[ 41/391]\t\tLoss: 2.2774 Acc@1: 35.368% [1856/5248]\n",
      "| Epoch [ 45/200] Iter[ 61/391]\t\tLoss: 3.1160 Acc@1: 35.750% [2791/7808]\n",
      "| Epoch [ 45/200] Iter[ 81/391]\t\tLoss: 3.6151 Acc@1: 35.693% [3700/10368]\n",
      "| Epoch [ 45/200] Iter[101/391]\t\tLoss: 3.3846 Acc@1: 35.060% [4532/12928]\n",
      "| Epoch [ 45/200] Iter[121/391]\t\tLoss: 2.3614 Acc@1: 34.431% [5332/15488]\n",
      "| Epoch [ 45/200] Iter[141/391]\t\tLoss: 2.6269 Acc@1: 35.063% [6328/18048]\n",
      "| Epoch [ 45/200] Iter[161/391]\t\tLoss: 3.2538 Acc@1: 35.091% [7231/20608]\n",
      "| Epoch [ 45/200] Iter[181/391]\t\tLoss: 3.0290 Acc@1: 35.125% [8137/23168]\n",
      "| Epoch [ 45/200] Iter[201/391]\t\tLoss: 2.3227 Acc@1: 35.158% [9045/25728]\n",
      "| Epoch [ 45/200] Iter[221/391]\t\tLoss: 3.2295 Acc@1: 35.309% [9988/28288]\n",
      "| Epoch [ 45/200] Iter[241/391]\t\tLoss: 2.1910 Acc@1: 35.666% [11002/30848]\n",
      "| Epoch [ 45/200] Iter[261/391]\t\tLoss: 3.0656 Acc@1: 35.739% [11939/33408]\n",
      "| Epoch [ 45/200] Iter[281/391]\t\tLoss: 3.4519 Acc@1: 35.531% [12779/35968]\n",
      "| Epoch [ 45/200] Iter[301/391]\t\tLoss: 2.9250 Acc@1: 35.399% [13638/38528]\n",
      "| Epoch [ 45/200] Iter[321/391]\t\tLoss: 2.2611 Acc@1: 35.573% [14616/41088]\n",
      "| Epoch [ 45/200] Iter[341/391]\t\tLoss: 2.2435 Acc@1: 35.757% [15607/43648]\n",
      "| Epoch [ 45/200] Iter[361/391]\t\tLoss: 3.0966 Acc@1: 35.531% [16418/46208]\n",
      "| Epoch [ 45/200] Iter[381/391]\t\tLoss: 3.3127 Acc@1: 35.573% [17348/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #45\n",
      "\n",
      "| Validation Epoch #45\t\t\tLoss: 1.8921 Acc@1: 51.58%\n",
      "\n",
      "----- Elapsed time : 0:13:20\n",
      "\n",
      "\n",
      "=> Training Epoch #46\n",
      "| Epoch [ 46/200] Iter[  1/391]\t\tLoss: 2.9180 Acc@1: 39.786% [ 50/128]\n",
      "| Epoch [ 46/200] Iter[ 21/391]\t\tLoss: 2.5159 Acc@1: 36.442% [979/2688]\n",
      "| Epoch [ 46/200] Iter[ 41/391]\t\tLoss: 2.8268 Acc@1: 35.082% [1841/5248]\n",
      "| Epoch [ 46/200] Iter[ 61/391]\t\tLoss: 3.3550 Acc@1: 35.141% [2743/7808]\n",
      "| Epoch [ 46/200] Iter[ 81/391]\t\tLoss: 2.3408 Acc@1: 34.672% [3594/10368]\n",
      "| Epoch [ 46/200] Iter[101/391]\t\tLoss: 2.6462 Acc@1: 33.669% [4352/12928]\n",
      "| Epoch [ 46/200] Iter[121/391]\t\tLoss: 2.3868 Acc@1: 33.431% [5177/15488]\n",
      "| Epoch [ 46/200] Iter[141/391]\t\tLoss: 3.4578 Acc@1: 33.211% [5993/18048]\n",
      "| Epoch [ 46/200] Iter[161/391]\t\tLoss: 2.7948 Acc@1: 33.977% [7002/20608]\n",
      "| Epoch [ 46/200] Iter[181/391]\t\tLoss: 3.4757 Acc@1: 33.914% [7857/23168]\n",
      "| Epoch [ 46/200] Iter[201/391]\t\tLoss: 3.0684 Acc@1: 33.950% [8734/25728]\n",
      "| Epoch [ 46/200] Iter[221/391]\t\tLoss: 3.1097 Acc@1: 34.119% [9651/28288]\n",
      "| Epoch [ 46/200] Iter[241/391]\t\tLoss: 3.2436 Acc@1: 34.385% [10607/30848]\n",
      "| Epoch [ 46/200] Iter[261/391]\t\tLoss: 2.4741 Acc@1: 34.582% [11553/33408]\n",
      "| Epoch [ 46/200] Iter[281/391]\t\tLoss: 3.4956 Acc@1: 34.600% [12444/35968]\n",
      "| Epoch [ 46/200] Iter[301/391]\t\tLoss: 3.5484 Acc@1: 34.716% [13375/38528]\n",
      "| Epoch [ 46/200] Iter[321/391]\t\tLoss: 3.4064 Acc@1: 34.660% [14241/41088]\n",
      "| Epoch [ 46/200] Iter[341/391]\t\tLoss: 2.9122 Acc@1: 34.410% [15019/43648]\n",
      "| Epoch [ 46/200] Iter[361/391]\t\tLoss: 2.4201 Acc@1: 34.305% [15851/46208]\n",
      "| Epoch [ 46/200] Iter[381/391]\t\tLoss: 3.1215 Acc@1: 34.478% [16814/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #46\n",
      "\n",
      "| Validation Epoch #46\t\t\tLoss: 2.2467 Acc@1: 49.74%\n",
      "\n",
      "----- Elapsed time : 0:13:38\n",
      "\n",
      "\n",
      "=> Training Epoch #47\n",
      "| Epoch [ 47/200] Iter[  1/391]\t\tLoss: 1.6949 Acc@1: 52.724% [ 67/128]\n",
      "| Epoch [ 47/200] Iter[ 21/391]\t\tLoss: 3.4268 Acc@1: 34.550% [928/2688]\n",
      "| Epoch [ 47/200] Iter[ 41/391]\t\tLoss: 3.2528 Acc@1: 34.831% [1827/5248]\n",
      "| Epoch [ 47/200] Iter[ 61/391]\t\tLoss: 3.5071 Acc@1: 34.606% [2702/7808]\n",
      "| Epoch [ 47/200] Iter[ 81/391]\t\tLoss: 3.5401 Acc@1: 34.689% [3596/10368]\n",
      "| Epoch [ 47/200] Iter[101/391]\t\tLoss: 2.3481 Acc@1: 35.251% [4557/12928]\n",
      "| Epoch [ 47/200] Iter[121/391]\t\tLoss: 2.3942 Acc@1: 35.656% [5522/15488]\n",
      "| Epoch [ 47/200] Iter[141/391]\t\tLoss: 3.4186 Acc@1: 35.478% [6403/18048]\n",
      "| Epoch [ 47/200] Iter[161/391]\t\tLoss: 2.2556 Acc@1: 35.556% [7327/20608]\n",
      "| Epoch [ 47/200] Iter[181/391]\t\tLoss: 3.3789 Acc@1: 35.773% [8287/23168]\n",
      "| Epoch [ 47/200] Iter[201/391]\t\tLoss: 3.4536 Acc@1: 35.683% [9180/25728]\n",
      "| Epoch [ 47/200] Iter[221/391]\t\tLoss: 3.3743 Acc@1: 35.835% [10136/28288]\n",
      "| Epoch [ 47/200] Iter[241/391]\t\tLoss: 3.3708 Acc@1: 35.536% [10962/30848]\n",
      "| Epoch [ 47/200] Iter[261/391]\t\tLoss: 2.2056 Acc@1: 35.161% [11746/33408]\n",
      "| Epoch [ 47/200] Iter[281/391]\t\tLoss: 1.8421 Acc@1: 34.950% [12570/35968]\n",
      "| Epoch [ 47/200] Iter[301/391]\t\tLoss: 2.7439 Acc@1: 35.061% [13508/38528]\n",
      "| Epoch [ 47/200] Iter[321/391]\t\tLoss: 2.9054 Acc@1: 35.102% [14422/41088]\n",
      "| Epoch [ 47/200] Iter[341/391]\t\tLoss: 1.8137 Acc@1: 35.113% [15326/43648]\n",
      "| Epoch [ 47/200] Iter[361/391]\t\tLoss: 2.4516 Acc@1: 35.344% [16331/46208]\n",
      "| Epoch [ 47/200] Iter[381/391]\t\tLoss: 2.5960 Acc@1: 35.377% [17252/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #47\n",
      "\n",
      "| Validation Epoch #47\t\t\tLoss: 1.9586 Acc@1: 51.93%\n",
      "\n",
      "----- Elapsed time : 0:13:55\n",
      "\n",
      "\n",
      "=> Training Epoch #48\n",
      "| Epoch [ 48/200] Iter[  1/391]\t\tLoss: 3.0277 Acc@1: 30.953% [ 39/128]\n",
      "| Epoch [ 48/200] Iter[ 21/391]\t\tLoss: 3.1466 Acc@1: 36.154% [971/2688]\n",
      "| Epoch [ 48/200] Iter[ 41/391]\t\tLoss: 3.4089 Acc@1: 34.558% [1813/5248]\n",
      "| Epoch [ 48/200] Iter[ 61/391]\t\tLoss: 3.3066 Acc@1: 34.038% [2657/7808]\n",
      "| Epoch [ 48/200] Iter[ 81/391]\t\tLoss: 2.3252 Acc@1: 34.753% [3603/10368]\n",
      "| Epoch [ 48/200] Iter[101/391]\t\tLoss: 2.2556 Acc@1: 34.987% [4523/12928]\n",
      "| Epoch [ 48/200] Iter[121/391]\t\tLoss: 3.2360 Acc@1: 34.316% [5314/15488]\n",
      "| Epoch [ 48/200] Iter[141/391]\t\tLoss: 3.1190 Acc@1: 35.161% [6345/18048]\n",
      "| Epoch [ 48/200] Iter[161/391]\t\tLoss: 3.4806 Acc@1: 35.226% [7259/20608]\n",
      "| Epoch [ 48/200] Iter[181/391]\t\tLoss: 3.1801 Acc@1: 35.390% [8199/23168]\n",
      "| Epoch [ 48/200] Iter[201/391]\t\tLoss: 3.3864 Acc@1: 35.619% [9164/25728]\n",
      "| Epoch [ 48/200] Iter[221/391]\t\tLoss: 3.0726 Acc@1: 35.424% [10020/28288]\n",
      "| Epoch [ 48/200] Iter[241/391]\t\tLoss: 2.6399 Acc@1: 35.558% [10968/30848]\n",
      "| Epoch [ 48/200] Iter[261/391]\t\tLoss: 3.5271 Acc@1: 35.805% [11961/33408]\n",
      "| Epoch [ 48/200] Iter[281/391]\t\tLoss: 3.4109 Acc@1: 35.550% [12786/35968]\n",
      "| Epoch [ 48/200] Iter[301/391]\t\tLoss: 3.2561 Acc@1: 35.488% [13672/38528]\n",
      "| Epoch [ 48/200] Iter[321/391]\t\tLoss: 3.1474 Acc@1: 35.291% [14500/41088]\n",
      "| Epoch [ 48/200] Iter[341/391]\t\tLoss: 3.4843 Acc@1: 35.471% [15482/43648]\n",
      "| Epoch [ 48/200] Iter[361/391]\t\tLoss: 3.5089 Acc@1: 35.642% [16469/46208]\n",
      "| Epoch [ 48/200] Iter[381/391]\t\tLoss: 3.3187 Acc@1: 35.940% [17527/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #48\n",
      "\n",
      "| Validation Epoch #48\t\t\tLoss: 1.9239 Acc@1: 51.82%\n",
      "\n",
      "----- Elapsed time : 0:14:12\n",
      "\n",
      "\n",
      "=> Training Epoch #49\n",
      "| Epoch [ 49/200] Iter[  1/391]\t\tLoss: 3.4752 Acc@1: 23.461% [ 30/128]\n",
      "| Epoch [ 49/200] Iter[ 21/391]\t\tLoss: 2.9997 Acc@1: 37.251% [1001/2688]\n",
      "| Epoch [ 49/200] Iter[ 41/391]\t\tLoss: 2.6589 Acc@1: 36.687% [1925/5248]\n",
      "| Epoch [ 49/200] Iter[ 61/391]\t\tLoss: 3.1054 Acc@1: 36.858% [2877/7808]\n",
      "| Epoch [ 49/200] Iter[ 81/391]\t\tLoss: 3.3921 Acc@1: 37.071% [3843/10368]\n",
      "| Epoch [ 49/200] Iter[101/391]\t\tLoss: 3.2774 Acc@1: 37.771% [4883/12928]\n",
      "| Epoch [ 49/200] Iter[121/391]\t\tLoss: 2.4921 Acc@1: 38.088% [5899/15488]\n",
      "| Epoch [ 49/200] Iter[141/391]\t\tLoss: 2.9765 Acc@1: 38.632% [6972/18048]\n",
      "| Epoch [ 49/200] Iter[161/391]\t\tLoss: 2.4026 Acc@1: 38.408% [7915/20608]\n",
      "| Epoch [ 49/200] Iter[181/391]\t\tLoss: 2.4253 Acc@1: 38.125% [8832/23168]\n",
      "| Epoch [ 49/200] Iter[201/391]\t\tLoss: 2.0826 Acc@1: 37.854% [9739/25728]\n",
      "| Epoch [ 49/200] Iter[221/391]\t\tLoss: 2.2424 Acc@1: 38.068% [10768/28288]\n",
      "| Epoch [ 49/200] Iter[241/391]\t\tLoss: 3.1739 Acc@1: 38.063% [11741/30848]\n",
      "| Epoch [ 49/200] Iter[261/391]\t\tLoss: 2.6815 Acc@1: 37.959% [12681/33408]\n",
      "| Epoch [ 49/200] Iter[281/391]\t\tLoss: 3.2080 Acc@1: 38.117% [13709/35968]\n",
      "| Epoch [ 49/200] Iter[301/391]\t\tLoss: 3.1243 Acc@1: 37.776% [14554/38528]\n",
      "| Epoch [ 49/200] Iter[321/391]\t\tLoss: 2.5661 Acc@1: 37.686% [15484/41088]\n",
      "| Epoch [ 49/200] Iter[341/391]\t\tLoss: 2.1143 Acc@1: 37.734% [16470/43648]\n",
      "| Epoch [ 49/200] Iter[361/391]\t\tLoss: 1.6775 Acc@1: 37.496% [17326/46208]\n",
      "| Epoch [ 49/200] Iter[381/391]\t\tLoss: 2.5941 Acc@1: 37.431% [18254/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #49\n",
      "\n",
      "| Validation Epoch #49\t\t\tLoss: 2.0049 Acc@1: 52.17%\n",
      "\n",
      "----- Elapsed time : 0:14:30\n",
      "\n",
      "\n",
      "=> Training Epoch #50\n",
      "| Epoch [ 50/200] Iter[  1/391]\t\tLoss: 2.7501 Acc@1: 46.722% [ 59/128]\n",
      "| Epoch [ 50/200] Iter[ 21/391]\t\tLoss: 2.2004 Acc@1: 36.472% [980/2688]\n",
      "| Epoch [ 50/200] Iter[ 41/391]\t\tLoss: 2.5269 Acc@1: 35.066% [1840/5248]\n",
      "| Epoch [ 50/200] Iter[ 61/391]\t\tLoss: 2.4158 Acc@1: 34.645% [2705/7808]\n",
      "| Epoch [ 50/200] Iter[ 81/391]\t\tLoss: 3.3823 Acc@1: 34.866% [3614/10368]\n",
      "| Epoch [ 50/200] Iter[101/391]\t\tLoss: 2.0018 Acc@1: 34.349% [4440/12928]\n",
      "| Epoch [ 50/200] Iter[121/391]\t\tLoss: 3.0749 Acc@1: 34.487% [5341/15488]\n",
      "| Epoch [ 50/200] Iter[141/391]\t\tLoss: 2.4282 Acc@1: 34.744% [6270/18048]\n",
      "| Epoch [ 50/200] Iter[161/391]\t\tLoss: 2.0639 Acc@1: 35.517% [7319/20608]\n",
      "| Epoch [ 50/200] Iter[181/391]\t\tLoss: 3.3252 Acc@1: 35.886% [8314/23168]\n",
      "| Epoch [ 50/200] Iter[201/391]\t\tLoss: 3.5224 Acc@1: 35.949% [9249/25728]\n",
      "| Epoch [ 50/200] Iter[221/391]\t\tLoss: 2.1530 Acc@1: 36.657% [10369/28288]\n",
      "| Epoch [ 50/200] Iter[241/391]\t\tLoss: 2.0914 Acc@1: 36.319% [11203/30848]\n",
      "| Epoch [ 50/200] Iter[261/391]\t\tLoss: 3.6786 Acc@1: 36.247% [12109/33408]\n",
      "| Epoch [ 50/200] Iter[281/391]\t\tLoss: 2.9294 Acc@1: 35.949% [12929/35968]\n",
      "| Epoch [ 50/200] Iter[301/391]\t\tLoss: 2.5725 Acc@1: 35.964% [13856/38528]\n",
      "| Epoch [ 50/200] Iter[321/391]\t\tLoss: 2.8007 Acc@1: 35.866% [14736/41088]\n",
      "| Epoch [ 50/200] Iter[341/391]\t\tLoss: 2.8995 Acc@1: 35.776% [15615/43648]\n",
      "| Epoch [ 50/200] Iter[361/391]\t\tLoss: 3.2100 Acc@1: 35.816% [16549/46208]\n",
      "| Epoch [ 50/200] Iter[381/391]\t\tLoss: 2.1686 Acc@1: 35.751% [17435/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #50\n",
      "\n",
      "| Validation Epoch #50\t\t\tLoss: 1.7708 Acc@1: 51.80%\n",
      "\n",
      "----- Elapsed time : 0:14:47\n",
      "\n",
      "\n",
      "=> Training Epoch #51\n",
      "| Epoch [ 51/200] Iter[  1/391]\t\tLoss: 1.7843 Acc@1: 55.591% [ 71/128]\n",
      "| Epoch [ 51/200] Iter[ 21/391]\t\tLoss: 3.3322 Acc@1: 40.659% [1092/2688]\n",
      "| Epoch [ 51/200] Iter[ 41/391]\t\tLoss: 2.0438 Acc@1: 39.266% [2060/5248]\n",
      "| Epoch [ 51/200] Iter[ 61/391]\t\tLoss: 2.1909 Acc@1: 38.934% [3039/7808]\n",
      "| Epoch [ 51/200] Iter[ 81/391]\t\tLoss: 3.0568 Acc@1: 37.338% [3871/10368]\n",
      "| Epoch [ 51/200] Iter[101/391]\t\tLoss: 3.6114 Acc@1: 37.680% [4871/12928]\n",
      "| Epoch [ 51/200] Iter[121/391]\t\tLoss: 3.2908 Acc@1: 37.865% [5864/15488]\n",
      "| Epoch [ 51/200] Iter[141/391]\t\tLoss: 3.2007 Acc@1: 37.455% [6759/18048]\n",
      "| Epoch [ 51/200] Iter[161/391]\t\tLoss: 2.2225 Acc@1: 37.233% [7673/20608]\n",
      "| Epoch [ 51/200] Iter[181/391]\t\tLoss: 2.8135 Acc@1: 37.180% [8613/23168]\n",
      "| Epoch [ 51/200] Iter[201/391]\t\tLoss: 3.3477 Acc@1: 36.454% [9378/25728]\n",
      "| Epoch [ 51/200] Iter[221/391]\t\tLoss: 3.1033 Acc@1: 36.180% [10234/28288]\n",
      "| Epoch [ 51/200] Iter[241/391]\t\tLoss: 3.3944 Acc@1: 35.797% [11042/30848]\n",
      "| Epoch [ 51/200] Iter[261/391]\t\tLoss: 2.2199 Acc@1: 35.910% [11996/33408]\n",
      "| Epoch [ 51/200] Iter[281/391]\t\tLoss: 3.4719 Acc@1: 35.686% [12835/35968]\n",
      "| Epoch [ 51/200] Iter[301/391]\t\tLoss: 2.9423 Acc@1: 35.773% [13782/38528]\n",
      "| Epoch [ 51/200] Iter[321/391]\t\tLoss: 2.0704 Acc@1: 36.030% [14804/41088]\n",
      "| Epoch [ 51/200] Iter[341/391]\t\tLoss: 1.8525 Acc@1: 36.239% [15817/43648]\n",
      "| Epoch [ 51/200] Iter[361/391]\t\tLoss: 3.5142 Acc@1: 36.082% [16672/46208]\n",
      "| Epoch [ 51/200] Iter[381/391]\t\tLoss: 3.2220 Acc@1: 35.993% [17552/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #51\n",
      "\n",
      "| Validation Epoch #51\t\t\tLoss: 2.2187 Acc@1: 53.23%\n",
      "\n",
      "----- Elapsed time : 0:15:04\n",
      "\n",
      "\n",
      "=> Training Epoch #52\n",
      "| Epoch [ 52/200] Iter[  1/391]\t\tLoss: 2.4313 Acc@1: 52.060% [ 66/128]\n",
      "| Epoch [ 52/200] Iter[ 21/391]\t\tLoss: 3.3057 Acc@1: 34.986% [940/2688]\n",
      "| Epoch [ 52/200] Iter[ 41/391]\t\tLoss: 2.8113 Acc@1: 35.489% [1862/5248]\n",
      "| Epoch [ 52/200] Iter[ 61/391]\t\tLoss: 2.3081 Acc@1: 37.328% [2914/7808]\n",
      "| Epoch [ 52/200] Iter[ 81/391]\t\tLoss: 3.3256 Acc@1: 36.818% [3817/10368]\n",
      "| Epoch [ 52/200] Iter[101/391]\t\tLoss: 3.0705 Acc@1: 36.541% [4724/12928]\n",
      "| Epoch [ 52/200] Iter[121/391]\t\tLoss: 3.1173 Acc@1: 36.254% [5615/15488]\n",
      "| Epoch [ 52/200] Iter[141/391]\t\tLoss: 3.4617 Acc@1: 36.522% [6591/18048]\n",
      "| Epoch [ 52/200] Iter[161/391]\t\tLoss: 3.4073 Acc@1: 36.619% [7546/20608]\n",
      "| Epoch [ 52/200] Iter[181/391]\t\tLoss: 3.3251 Acc@1: 36.511% [8458/23168]\n",
      "| Epoch [ 52/200] Iter[201/391]\t\tLoss: 3.1488 Acc@1: 36.081% [9282/25728]\n",
      "| Epoch [ 52/200] Iter[221/391]\t\tLoss: 1.8814 Acc@1: 36.076% [10205/28288]\n",
      "| Epoch [ 52/200] Iter[241/391]\t\tLoss: 3.4590 Acc@1: 35.999% [11104/30848]\n",
      "| Epoch [ 52/200] Iter[261/391]\t\tLoss: 2.6586 Acc@1: 36.023% [12034/33408]\n",
      "| Epoch [ 52/200] Iter[281/391]\t\tLoss: 3.2429 Acc@1: 36.125% [12993/35968]\n",
      "| Epoch [ 52/200] Iter[301/391]\t\tLoss: 3.0127 Acc@1: 36.095% [13906/38528]\n",
      "| Epoch [ 52/200] Iter[321/391]\t\tLoss: 3.2000 Acc@1: 35.847% [14728/41088]\n",
      "| Epoch [ 52/200] Iter[341/391]\t\tLoss: 3.0057 Acc@1: 35.802% [15626/43648]\n",
      "| Epoch [ 52/200] Iter[361/391]\t\tLoss: 3.2056 Acc@1: 35.860% [16570/46208]\n",
      "| Epoch [ 52/200] Iter[381/391]\t\tLoss: 2.2441 Acc@1: 35.997% [17555/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #52\n",
      "\n",
      "| Validation Epoch #52\t\t\tLoss: 2.1156 Acc@1: 52.81%\n",
      "\n",
      "----- Elapsed time : 0:15:22\n",
      "\n",
      "\n",
      "=> Training Epoch #53\n",
      "| Epoch [ 53/200] Iter[  1/391]\t\tLoss: 3.5172 Acc@1: 20.020% [ 25/128]\n",
      "| Epoch [ 53/200] Iter[ 21/391]\t\tLoss: 2.6054 Acc@1: 40.027% [1075/2688]\n",
      "| Epoch [ 53/200] Iter[ 41/391]\t\tLoss: 3.4280 Acc@1: 37.607% [1973/5248]\n",
      "| Epoch [ 53/200] Iter[ 61/391]\t\tLoss: 3.5152 Acc@1: 37.862% [2956/7808]\n",
      "| Epoch [ 53/200] Iter[ 81/391]\t\tLoss: 2.7092 Acc@1: 37.352% [3872/10368]\n",
      "| Epoch [ 53/200] Iter[101/391]\t\tLoss: 2.2746 Acc@1: 36.364% [4701/12928]\n",
      "| Epoch [ 53/200] Iter[121/391]\t\tLoss: 3.2452 Acc@1: 37.033% [5735/15488]\n",
      "| Epoch [ 53/200] Iter[141/391]\t\tLoss: 3.4015 Acc@1: 36.688% [6621/18048]\n",
      "| Epoch [ 53/200] Iter[161/391]\t\tLoss: 2.5992 Acc@1: 36.759% [7575/20608]\n",
      "| Epoch [ 53/200] Iter[181/391]\t\tLoss: 3.6072 Acc@1: 36.456% [8446/23168]\n",
      "| Epoch [ 53/200] Iter[201/391]\t\tLoss: 2.9163 Acc@1: 36.881% [9488/25728]\n",
      "| Epoch [ 53/200] Iter[221/391]\t\tLoss: 1.8833 Acc@1: 36.958% [10454/28288]\n",
      "| Epoch [ 53/200] Iter[241/391]\t\tLoss: 2.7818 Acc@1: 37.037% [11425/30848]\n",
      "| Epoch [ 53/200] Iter[261/391]\t\tLoss: 3.4691 Acc@1: 36.824% [12302/33408]\n",
      "| Epoch [ 53/200] Iter[281/391]\t\tLoss: 3.1603 Acc@1: 36.993% [13305/35968]\n",
      "| Epoch [ 53/200] Iter[301/391]\t\tLoss: 2.5318 Acc@1: 37.017% [14262/38528]\n",
      "| Epoch [ 53/200] Iter[321/391]\t\tLoss: 2.0986 Acc@1: 36.911% [15165/41088]\n",
      "| Epoch [ 53/200] Iter[341/391]\t\tLoss: 2.6312 Acc@1: 36.908% [16109/43648]\n",
      "| Epoch [ 53/200] Iter[361/391]\t\tLoss: 3.3806 Acc@1: 36.883% [17042/46208]\n",
      "| Epoch [ 53/200] Iter[381/391]\t\tLoss: 3.5050 Acc@1: 36.796% [17944/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #53\n",
      "\n",
      "| Validation Epoch #53\t\t\tLoss: 2.1123 Acc@1: 53.40%\n",
      "\n",
      "----- Elapsed time : 0:15:39\n",
      "\n",
      "\n",
      "=> Training Epoch #54\n",
      "| Epoch [ 54/200] Iter[  1/391]\t\tLoss: 3.1515 Acc@1: 29.324% [ 37/128]\n",
      "| Epoch [ 54/200] Iter[ 21/391]\t\tLoss: 2.8998 Acc@1: 35.368% [950/2688]\n",
      "| Epoch [ 54/200] Iter[ 41/391]\t\tLoss: 3.5273 Acc@1: 38.154% [2002/5248]\n",
      "| Epoch [ 54/200] Iter[ 61/391]\t\tLoss: 1.7330 Acc@1: 39.024% [3047/7808]\n",
      "| Epoch [ 54/200] Iter[ 81/391]\t\tLoss: 3.5244 Acc@1: 39.163% [4060/10368]\n",
      "| Epoch [ 54/200] Iter[101/391]\t\tLoss: 3.1472 Acc@1: 39.196% [5067/12928]\n",
      "| Epoch [ 54/200] Iter[121/391]\t\tLoss: 1.9662 Acc@1: 38.675% [5989/15488]\n",
      "| Epoch [ 54/200] Iter[141/391]\t\tLoss: 3.2682 Acc@1: 38.678% [6980/18048]\n",
      "| Epoch [ 54/200] Iter[161/391]\t\tLoss: 2.6289 Acc@1: 37.936% [7817/20608]\n",
      "| Epoch [ 54/200] Iter[181/391]\t\tLoss: 3.2604 Acc@1: 38.513% [8922/23168]\n",
      "| Epoch [ 54/200] Iter[201/391]\t\tLoss: 2.3605 Acc@1: 38.289% [9850/25728]\n",
      "| Epoch [ 54/200] Iter[221/391]\t\tLoss: 2.9329 Acc@1: 38.521% [10896/28288]\n",
      "| Epoch [ 54/200] Iter[241/391]\t\tLoss: 3.5071 Acc@1: 38.366% [11835/30848]\n",
      "| Epoch [ 54/200] Iter[261/391]\t\tLoss: 3.5130 Acc@1: 38.472% [12852/33408]\n",
      "| Epoch [ 54/200] Iter[281/391]\t\tLoss: 1.8284 Acc@1: 38.606% [13885/35968]\n",
      "| Epoch [ 54/200] Iter[301/391]\t\tLoss: 3.2513 Acc@1: 38.247% [14735/38528]\n",
      "| Epoch [ 54/200] Iter[321/391]\t\tLoss: 2.4590 Acc@1: 38.116% [15661/41088]\n",
      "| Epoch [ 54/200] Iter[341/391]\t\tLoss: 3.4030 Acc@1: 38.126% [16641/43648]\n",
      "| Epoch [ 54/200] Iter[361/391]\t\tLoss: 2.8606 Acc@1: 38.209% [17655/46208]\n",
      "| Epoch [ 54/200] Iter[381/391]\t\tLoss: 3.1688 Acc@1: 38.274% [18665/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #54\n",
      "\n",
      "| Validation Epoch #54\t\t\tLoss: 1.9244 Acc@1: 53.02%\n",
      "\n",
      "----- Elapsed time : 0:15:56\n",
      "\n",
      "\n",
      "=> Training Epoch #55\n",
      "| Epoch [ 55/200] Iter[  1/391]\t\tLoss: 3.0273 Acc@1: 33.675% [ 43/128]\n",
      "| Epoch [ 55/200] Iter[ 21/391]\t\tLoss: 3.0874 Acc@1: 40.301% [1083/2688]\n",
      "| Epoch [ 55/200] Iter[ 41/391]\t\tLoss: 3.5603 Acc@1: 38.278% [2008/5248]\n",
      "| Epoch [ 55/200] Iter[ 61/391]\t\tLoss: 3.4427 Acc@1: 36.672% [2863/7808]\n",
      "| Epoch [ 55/200] Iter[ 81/391]\t\tLoss: 2.2895 Acc@1: 37.729% [3911/10368]\n",
      "| Epoch [ 55/200] Iter[101/391]\t\tLoss: 3.3234 Acc@1: 36.691% [4743/12928]\n",
      "| Epoch [ 55/200] Iter[121/391]\t\tLoss: 3.3941 Acc@1: 37.508% [5809/15488]\n",
      "| Epoch [ 55/200] Iter[141/391]\t\tLoss: 2.5759 Acc@1: 37.917% [6843/18048]\n",
      "| Epoch [ 55/200] Iter[161/391]\t\tLoss: 2.5966 Acc@1: 37.493% [7726/20608]\n",
      "| Epoch [ 55/200] Iter[181/391]\t\tLoss: 1.9696 Acc@1: 37.466% [8680/23168]\n",
      "| Epoch [ 55/200] Iter[201/391]\t\tLoss: 3.0452 Acc@1: 37.442% [9633/25728]\n",
      "| Epoch [ 55/200] Iter[221/391]\t\tLoss: 2.8450 Acc@1: 37.661% [10653/28288]\n",
      "| Epoch [ 55/200] Iter[241/391]\t\tLoss: 2.0669 Acc@1: 37.923% [11698/30848]\n",
      "| Epoch [ 55/200] Iter[261/391]\t\tLoss: 2.9563 Acc@1: 37.844% [12643/33408]\n",
      "| Epoch [ 55/200] Iter[281/391]\t\tLoss: 2.9570 Acc@1: 37.765% [13583/35968]\n",
      "| Epoch [ 55/200] Iter[301/391]\t\tLoss: 2.9859 Acc@1: 37.891% [14598/38528]\n",
      "| Epoch [ 55/200] Iter[321/391]\t\tLoss: 3.3929 Acc@1: 37.624% [15458/41088]\n",
      "| Epoch [ 55/200] Iter[341/391]\t\tLoss: 2.5799 Acc@1: 37.491% [16363/43648]\n",
      "| Epoch [ 55/200] Iter[361/391]\t\tLoss: 3.1153 Acc@1: 37.328% [17248/46208]\n",
      "| Epoch [ 55/200] Iter[381/391]\t\tLoss: 3.3428 Acc@1: 37.286% [18183/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #55\n",
      "\n",
      "| Validation Epoch #55\t\t\tLoss: 1.8100 Acc@1: 53.23%\n",
      "\n",
      "----- Elapsed time : 0:16:13\n",
      "\n",
      "\n",
      "=> Training Epoch #56\n",
      "| Epoch [ 56/200] Iter[  1/391]\t\tLoss: 1.8869 Acc@1: 63.171% [ 80/128]\n",
      "| Epoch [ 56/200] Iter[ 21/391]\t\tLoss: 2.8903 Acc@1: 36.764% [988/2688]\n",
      "| Epoch [ 56/200] Iter[ 41/391]\t\tLoss: 2.7498 Acc@1: 37.370% [1961/5248]\n",
      "| Epoch [ 56/200] Iter[ 61/391]\t\tLoss: 3.3031 Acc@1: 37.350% [2916/7808]\n",
      "| Epoch [ 56/200] Iter[ 81/391]\t\tLoss: 2.9345 Acc@1: 37.700% [3908/10368]\n",
      "| Epoch [ 56/200] Iter[101/391]\t\tLoss: 2.0551 Acc@1: 37.835% [4891/12928]\n",
      "| Epoch [ 56/200] Iter[121/391]\t\tLoss: 3.2641 Acc@1: 37.767% [5849/15488]\n",
      "| Epoch [ 56/200] Iter[141/391]\t\tLoss: 2.9391 Acc@1: 37.655% [6795/18048]\n",
      "| Epoch [ 56/200] Iter[161/391]\t\tLoss: 2.1734 Acc@1: 37.672% [7763/20608]\n",
      "| Epoch [ 56/200] Iter[181/391]\t\tLoss: 2.4469 Acc@1: 38.360% [8887/23168]\n",
      "| Epoch [ 56/200] Iter[201/391]\t\tLoss: 3.5421 Acc@1: 38.213% [9831/25728]\n",
      "| Epoch [ 56/200] Iter[221/391]\t\tLoss: 3.3680 Acc@1: 38.230% [10814/28288]\n",
      "| Epoch [ 56/200] Iter[241/391]\t\tLoss: 3.3880 Acc@1: 38.247% [11798/30848]\n",
      "| Epoch [ 56/200] Iter[261/391]\t\tLoss: 3.2504 Acc@1: 38.301% [12795/33408]\n",
      "| Epoch [ 56/200] Iter[281/391]\t\tLoss: 3.4254 Acc@1: 37.904% [13633/35968]\n",
      "| Epoch [ 56/200] Iter[301/391]\t\tLoss: 2.1813 Acc@1: 38.026% [14650/38528]\n",
      "| Epoch [ 56/200] Iter[321/391]\t\tLoss: 3.6120 Acc@1: 37.802% [15531/41088]\n",
      "| Epoch [ 56/200] Iter[341/391]\t\tLoss: 3.5578 Acc@1: 37.694% [16452/43648]\n",
      "| Epoch [ 56/200] Iter[361/391]\t\tLoss: 2.9814 Acc@1: 37.707% [17423/46208]\n",
      "| Epoch [ 56/200] Iter[381/391]\t\tLoss: 3.5199 Acc@1: 37.729% [18399/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #56\n",
      "\n",
      "| Validation Epoch #56\t\t\tLoss: 1.7277 Acc@1: 54.32%\n",
      "\n",
      "----- Elapsed time : 0:16:30\n",
      "\n",
      "\n",
      "=> Training Epoch #57\n",
      "| Epoch [ 57/200] Iter[  1/391]\t\tLoss: 3.2280 Acc@1: 30.949% [ 39/128]\n",
      "| Epoch [ 57/200] Iter[ 21/391]\t\tLoss: 2.7244 Acc@1: 34.219% [919/2688]\n",
      "| Epoch [ 57/200] Iter[ 41/391]\t\tLoss: 1.6556 Acc@1: 34.299% [1799/5248]\n",
      "| Epoch [ 57/200] Iter[ 61/391]\t\tLoss: 3.4215 Acc@1: 34.388% [2685/7808]\n",
      "| Epoch [ 57/200] Iter[ 81/391]\t\tLoss: 1.9015 Acc@1: 36.583% [3792/10368]\n",
      "| Epoch [ 57/200] Iter[101/391]\t\tLoss: 2.7492 Acc@1: 36.899% [4770/12928]\n",
      "| Epoch [ 57/200] Iter[121/391]\t\tLoss: 1.8321 Acc@1: 37.024% [5734/15488]\n",
      "| Epoch [ 57/200] Iter[141/391]\t\tLoss: 2.4351 Acc@1: 38.121% [6880/18048]\n",
      "| Epoch [ 57/200] Iter[161/391]\t\tLoss: 2.8693 Acc@1: 37.977% [7826/20608]\n",
      "| Epoch [ 57/200] Iter[181/391]\t\tLoss: 3.0207 Acc@1: 38.251% [8861/23168]\n",
      "| Epoch [ 57/200] Iter[201/391]\t\tLoss: 3.5196 Acc@1: 38.200% [9828/25728]\n",
      "| Epoch [ 57/200] Iter[221/391]\t\tLoss: 3.3441 Acc@1: 38.351% [10848/28288]\n",
      "| Epoch [ 57/200] Iter[241/391]\t\tLoss: 2.7097 Acc@1: 38.345% [11828/30848]\n",
      "| Epoch [ 57/200] Iter[261/391]\t\tLoss: 2.6057 Acc@1: 38.374% [12820/33408]\n",
      "| Epoch [ 57/200] Iter[281/391]\t\tLoss: 2.6542 Acc@1: 38.482% [13841/35968]\n",
      "| Epoch [ 57/200] Iter[301/391]\t\tLoss: 1.9182 Acc@1: 38.396% [14793/38528]\n",
      "| Epoch [ 57/200] Iter[321/391]\t\tLoss: 2.4752 Acc@1: 38.321% [15745/41088]\n",
      "| Epoch [ 57/200] Iter[341/391]\t\tLoss: 2.9653 Acc@1: 38.224% [16684/43648]\n",
      "| Epoch [ 57/200] Iter[361/391]\t\tLoss: 3.4967 Acc@1: 38.267% [17682/46208]\n",
      "| Epoch [ 57/200] Iter[381/391]\t\tLoss: 2.8511 Acc@1: 38.011% [18537/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #57\n",
      "\n",
      "| Validation Epoch #57\t\t\tLoss: 1.7402 Acc@1: 52.85%\n",
      "\n",
      "----- Elapsed time : 0:16:48\n",
      "\n",
      "\n",
      "=> Training Epoch #58\n",
      "| Epoch [ 58/200] Iter[  1/391]\t\tLoss: 3.3106 Acc@1: 22.500% [ 28/128]\n",
      "| Epoch [ 58/200] Iter[ 21/391]\t\tLoss: 3.3523 Acc@1: 37.533% [1008/2688]\n",
      "| Epoch [ 58/200] Iter[ 41/391]\t\tLoss: 2.8004 Acc@1: 36.637% [1922/5248]\n",
      "| Epoch [ 58/200] Iter[ 61/391]\t\tLoss: 2.8386 Acc@1: 35.996% [2810/7808]\n",
      "| Epoch [ 58/200] Iter[ 81/391]\t\tLoss: 2.5344 Acc@1: 37.250% [3862/10368]\n",
      "| Epoch [ 58/200] Iter[101/391]\t\tLoss: 3.5595 Acc@1: 38.217% [4940/12928]\n",
      "| Epoch [ 58/200] Iter[121/391]\t\tLoss: 3.2456 Acc@1: 36.992% [5729/15488]\n",
      "| Epoch [ 58/200] Iter[141/391]\t\tLoss: 3.3300 Acc@1: 37.985% [6855/18048]\n",
      "| Epoch [ 58/200] Iter[161/391]\t\tLoss: 2.3750 Acc@1: 38.346% [7902/20608]\n",
      "| Epoch [ 58/200] Iter[181/391]\t\tLoss: 2.2458 Acc@1: 38.413% [8899/23168]\n",
      "| Epoch [ 58/200] Iter[201/391]\t\tLoss: 2.5013 Acc@1: 38.151% [9815/25728]\n",
      "| Epoch [ 58/200] Iter[221/391]\t\tLoss: 3.0686 Acc@1: 37.777% [10686/28288]\n",
      "| Epoch [ 58/200] Iter[241/391]\t\tLoss: 3.3053 Acc@1: 37.501% [11568/30848]\n",
      "| Epoch [ 58/200] Iter[261/391]\t\tLoss: 3.4902 Acc@1: 37.444% [12509/33408]\n",
      "| Epoch [ 58/200] Iter[281/391]\t\tLoss: 3.4460 Acc@1: 37.357% [13436/35968]\n",
      "| Epoch [ 58/200] Iter[301/391]\t\tLoss: 3.1833 Acc@1: 37.544% [14464/38528]\n",
      "| Epoch [ 58/200] Iter[321/391]\t\tLoss: 1.7074 Acc@1: 37.341% [15342/41088]\n",
      "| Epoch [ 58/200] Iter[341/391]\t\tLoss: 2.3477 Acc@1: 37.629% [16424/43648]\n",
      "| Epoch [ 58/200] Iter[361/391]\t\tLoss: 3.1632 Acc@1: 37.621% [17384/46208]\n",
      "| Epoch [ 58/200] Iter[381/391]\t\tLoss: 2.8694 Acc@1: 37.784% [18426/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #58\n",
      "\n",
      "| Validation Epoch #58\t\t\tLoss: 1.6395 Acc@1: 54.74%\n",
      "\n",
      "----- Elapsed time : 0:17:05\n",
      "\n",
      "\n",
      "=> Training Epoch #59\n",
      "| Epoch [ 59/200] Iter[  1/391]\t\tLoss: 2.2960 Acc@1: 49.616% [ 63/128]\n",
      "| Epoch [ 59/200] Iter[ 21/391]\t\tLoss: 3.3305 Acc@1: 40.553% [1090/2688]\n",
      "| Epoch [ 59/200] Iter[ 41/391]\t\tLoss: 3.4422 Acc@1: 40.097% [2104/5248]\n",
      "| Epoch [ 59/200] Iter[ 61/391]\t\tLoss: 2.2447 Acc@1: 38.269% [2988/7808]\n",
      "| Epoch [ 59/200] Iter[ 81/391]\t\tLoss: 3.3819 Acc@1: 37.668% [3905/10368]\n",
      "| Epoch [ 59/200] Iter[101/391]\t\tLoss: 1.8137 Acc@1: 38.367% [4960/12928]\n",
      "| Epoch [ 59/200] Iter[121/391]\t\tLoss: 2.1104 Acc@1: 38.770% [6004/15488]\n",
      "| Epoch [ 59/200] Iter[141/391]\t\tLoss: 3.1333 Acc@1: 38.369% [6924/18048]\n",
      "| Epoch [ 59/200] Iter[161/391]\t\tLoss: 3.1643 Acc@1: 38.246% [7881/20608]\n",
      "| Epoch [ 59/200] Iter[181/391]\t\tLoss: 2.3309 Acc@1: 38.831% [8996/23168]\n",
      "| Epoch [ 59/200] Iter[201/391]\t\tLoss: 2.4578 Acc@1: 38.367% [9871/25728]\n",
      "| Epoch [ 59/200] Iter[221/391]\t\tLoss: 3.1723 Acc@1: 38.415% [10866/28288]\n",
      "| Epoch [ 59/200] Iter[241/391]\t\tLoss: 3.3586 Acc@1: 38.323% [11821/30848]\n",
      "| Epoch [ 59/200] Iter[261/391]\t\tLoss: 2.3522 Acc@1: 38.247% [12777/33408]\n",
      "| Epoch [ 59/200] Iter[281/391]\t\tLoss: 2.8976 Acc@1: 37.635% [13536/35968]\n",
      "| Epoch [ 59/200] Iter[301/391]\t\tLoss: 2.8766 Acc@1: 37.422% [14417/38528]\n",
      "| Epoch [ 59/200] Iter[321/391]\t\tLoss: 3.0119 Acc@1: 37.633% [15462/41088]\n",
      "| Epoch [ 59/200] Iter[341/391]\t\tLoss: 3.5423 Acc@1: 37.366% [16309/43648]\n",
      "| Epoch [ 59/200] Iter[361/391]\t\tLoss: 3.1671 Acc@1: 37.277% [17224/46208]\n",
      "| Epoch [ 59/200] Iter[381/391]\t\tLoss: 2.3000 Acc@1: 37.360% [18219/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #59\n",
      "\n",
      "| Validation Epoch #59\t\t\tLoss: 1.8183 Acc@1: 55.76%\n",
      "\n",
      "----- Elapsed time : 0:17:23\n",
      "\n",
      "\n",
      "=> Training Epoch #60\n",
      "| Epoch [ 60/200] Iter[  1/391]\t\tLoss: 2.4885 Acc@1: 53.883% [ 68/128]\n",
      "| Epoch [ 60/200] Iter[ 21/391]\t\tLoss: 3.1696 Acc@1: 39.525% [1062/2688]\n",
      "| Epoch [ 60/200] Iter[ 41/391]\t\tLoss: 3.0723 Acc@1: 41.119% [2157/5248]\n",
      "| Epoch [ 60/200] Iter[ 61/391]\t\tLoss: 3.4759 Acc@1: 39.394% [3075/7808]\n",
      "| Epoch [ 60/200] Iter[ 81/391]\t\tLoss: 1.8677 Acc@1: 38.572% [3999/10368]\n",
      "| Epoch [ 60/200] Iter[101/391]\t\tLoss: 3.4150 Acc@1: 38.701% [5003/12928]\n",
      "| Epoch [ 60/200] Iter[121/391]\t\tLoss: 3.0863 Acc@1: 39.130% [6060/15488]\n",
      "| Epoch [ 60/200] Iter[141/391]\t\tLoss: 3.2309 Acc@1: 39.613% [7149/18048]\n",
      "| Epoch [ 60/200] Iter[161/391]\t\tLoss: 2.3364 Acc@1: 39.414% [8122/20608]\n",
      "| Epoch [ 60/200] Iter[181/391]\t\tLoss: 2.8764 Acc@1: 38.713% [8969/23168]\n",
      "| Epoch [ 60/200] Iter[201/391]\t\tLoss: 3.0489 Acc@1: 38.459% [9894/25728]\n",
      "| Epoch [ 60/200] Iter[221/391]\t\tLoss: 3.3846 Acc@1: 38.334% [10843/28288]\n",
      "| Epoch [ 60/200] Iter[241/391]\t\tLoss: 2.0005 Acc@1: 38.335% [11825/30848]\n",
      "| Epoch [ 60/200] Iter[261/391]\t\tLoss: 2.0767 Acc@1: 38.188% [12757/33408]\n",
      "| Epoch [ 60/200] Iter[281/391]\t\tLoss: 3.0542 Acc@1: 38.161% [13725/35968]\n",
      "| Epoch [ 60/200] Iter[301/391]\t\tLoss: 2.2282 Acc@1: 38.360% [14779/38528]\n",
      "| Epoch [ 60/200] Iter[321/391]\t\tLoss: 1.6923 Acc@1: 38.291% [15733/41088]\n",
      "| Epoch [ 60/200] Iter[341/391]\t\tLoss: 1.7302 Acc@1: 38.481% [16796/43648]\n",
      "| Epoch [ 60/200] Iter[361/391]\t\tLoss: 1.8046 Acc@1: 38.519% [17798/46208]\n",
      "| Epoch [ 60/200] Iter[381/391]\t\tLoss: 1.8973 Acc@1: 38.492% [18771/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #60\n",
      "\n",
      "| Validation Epoch #60\t\t\tLoss: 1.9402 Acc@1: 55.34%\n",
      "\n",
      "----- Elapsed time : 0:17:40\n",
      "\n",
      "\n",
      "=> Training Epoch #61\n",
      "| Epoch [ 61/200] Iter[  1/391]\t\tLoss: 2.0929 Acc@1: 54.898% [ 70/128]\n",
      "| Epoch [ 61/200] Iter[ 21/391]\t\tLoss: 2.1067 Acc@1: 38.653% [1039/2688]\n",
      "| Epoch [ 61/200] Iter[ 41/391]\t\tLoss: 3.2595 Acc@1: 37.055% [1944/5248]\n",
      "| Epoch [ 61/200] Iter[ 61/391]\t\tLoss: 2.7445 Acc@1: 38.050% [2970/7808]\n",
      "| Epoch [ 61/200] Iter[ 81/391]\t\tLoss: 3.2728 Acc@1: 37.115% [3848/10368]\n",
      "| Epoch [ 61/200] Iter[101/391]\t\tLoss: 2.8939 Acc@1: 38.388% [4962/12928]\n",
      "| Epoch [ 61/200] Iter[121/391]\t\tLoss: 1.6949 Acc@1: 39.160% [6065/15488]\n",
      "| Epoch [ 61/200] Iter[141/391]\t\tLoss: 3.4552 Acc@1: 39.209% [7076/18048]\n",
      "| Epoch [ 61/200] Iter[161/391]\t\tLoss: 3.0535 Acc@1: 38.752% [7985/20608]\n",
      "| Epoch [ 61/200] Iter[181/391]\t\tLoss: 3.4010 Acc@1: 38.549% [8930/23168]\n",
      "| Epoch [ 61/200] Iter[201/391]\t\tLoss: 3.3308 Acc@1: 38.422% [9885/25728]\n",
      "| Epoch [ 61/200] Iter[221/391]\t\tLoss: 1.9658 Acc@1: 38.783% [10970/28288]\n",
      "| Epoch [ 61/200] Iter[241/391]\t\tLoss: 3.1997 Acc@1: 38.449% [11860/30848]\n",
      "| Epoch [ 61/200] Iter[261/391]\t\tLoss: 3.3807 Acc@1: 38.252% [12779/33408]\n",
      "| Epoch [ 61/200] Iter[281/391]\t\tLoss: 3.2871 Acc@1: 38.201% [13740/35968]\n",
      "| Epoch [ 61/200] Iter[301/391]\t\tLoss: 3.1937 Acc@1: 38.048% [14659/38528]\n",
      "| Epoch [ 61/200] Iter[321/391]\t\tLoss: 3.2686 Acc@1: 37.849% [15551/41088]\n",
      "| Epoch [ 61/200] Iter[341/391]\t\tLoss: 2.3452 Acc@1: 37.736% [16471/43648]\n",
      "| Epoch [ 61/200] Iter[361/391]\t\tLoss: 3.2030 Acc@1: 37.972% [17546/46208]\n",
      "| Epoch [ 61/200] Iter[381/391]\t\tLoss: 2.9299 Acc@1: 37.910% [18488/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #61\n",
      "\n",
      "| Validation Epoch #61\t\t\tLoss: 1.7385 Acc@1: 55.02%\n",
      "\n",
      "----- Elapsed time : 0:17:57\n",
      "\n",
      "\n",
      "=> Training Epoch #62\n",
      "| Epoch [ 62/200] Iter[  1/391]\t\tLoss: 3.3799 Acc@1: 23.359% [ 29/128]\n",
      "| Epoch [ 62/200] Iter[ 21/391]\t\tLoss: 3.4905 Acc@1: 40.764% [1095/2688]\n",
      "| Epoch [ 62/200] Iter[ 41/391]\t\tLoss: 3.0847 Acc@1: 38.890% [2040/5248]\n",
      "| Epoch [ 62/200] Iter[ 61/391]\t\tLoss: 2.6707 Acc@1: 37.759% [2948/7808]\n",
      "| Epoch [ 62/200] Iter[ 81/391]\t\tLoss: 3.3940 Acc@1: 38.002% [3940/10368]\n",
      "| Epoch [ 62/200] Iter[101/391]\t\tLoss: 3.4359 Acc@1: 37.633% [4865/12928]\n",
      "| Epoch [ 62/200] Iter[121/391]\t\tLoss: 3.1915 Acc@1: 36.962% [5724/15488]\n",
      "| Epoch [ 62/200] Iter[141/391]\t\tLoss: 2.4757 Acc@1: 37.101% [6696/18048]\n",
      "| Epoch [ 62/200] Iter[161/391]\t\tLoss: 3.4052 Acc@1: 37.257% [7678/20608]\n",
      "| Epoch [ 62/200] Iter[181/391]\t\tLoss: 2.4723 Acc@1: 37.902% [8781/23168]\n",
      "| Epoch [ 62/200] Iter[201/391]\t\tLoss: 2.3508 Acc@1: 38.207% [9829/25728]\n",
      "| Epoch [ 62/200] Iter[221/391]\t\tLoss: 3.2352 Acc@1: 38.212% [10809/28288]\n",
      "| Epoch [ 62/200] Iter[241/391]\t\tLoss: 2.2279 Acc@1: 38.154% [11769/30848]\n",
      "| Epoch [ 62/200] Iter[261/391]\t\tLoss: 1.9820 Acc@1: 38.093% [12726/33408]\n",
      "| Epoch [ 62/200] Iter[281/391]\t\tLoss: 2.8523 Acc@1: 38.492% [13844/35968]\n",
      "| Epoch [ 62/200] Iter[301/391]\t\tLoss: 3.1342 Acc@1: 38.393% [14792/38528]\n",
      "| Epoch [ 62/200] Iter[321/391]\t\tLoss: 1.8205 Acc@1: 38.312% [15741/41088]\n",
      "| Epoch [ 62/200] Iter[341/391]\t\tLoss: 2.6864 Acc@1: 38.219% [16681/43648]\n",
      "| Epoch [ 62/200] Iter[361/391]\t\tLoss: 3.2085 Acc@1: 37.967% [17543/46208]\n",
      "| Epoch [ 62/200] Iter[381/391]\t\tLoss: 2.5556 Acc@1: 38.037% [18549/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #62\n",
      "\n",
      "| Validation Epoch #62\t\t\tLoss: 1.8159 Acc@1: 54.70%\n",
      "\n",
      "----- Elapsed time : 0:18:15\n",
      "\n",
      "\n",
      "=> Training Epoch #63\n",
      "| Epoch [ 63/200] Iter[  1/391]\t\tLoss: 2.3026 Acc@1: 52.455% [ 67/128]\n",
      "| Epoch [ 63/200] Iter[ 21/391]\t\tLoss: 3.4630 Acc@1: 42.634% [1145/2688]\n",
      "| Epoch [ 63/200] Iter[ 41/391]\t\tLoss: 2.7598 Acc@1: 41.754% [2191/5248]\n",
      "| Epoch [ 63/200] Iter[ 61/391]\t\tLoss: 2.3949 Acc@1: 41.379% [3230/7808]\n",
      "| Epoch [ 63/200] Iter[ 81/391]\t\tLoss: 2.2929 Acc@1: 41.177% [4269/10368]\n",
      "| Epoch [ 63/200] Iter[101/391]\t\tLoss: 3.3644 Acc@1: 39.328% [5084/12928]\n",
      "| Epoch [ 63/200] Iter[121/391]\t\tLoss: 3.4611 Acc@1: 38.965% [6034/15488]\n",
      "| Epoch [ 63/200] Iter[141/391]\t\tLoss: 2.2794 Acc@1: 39.354% [7102/18048]\n",
      "| Epoch [ 63/200] Iter[161/391]\t\tLoss: 1.7618 Acc@1: 39.193% [8076/20608]\n",
      "| Epoch [ 63/200] Iter[181/391]\t\tLoss: 1.8736 Acc@1: 38.792% [8987/23168]\n",
      "| Epoch [ 63/200] Iter[201/391]\t\tLoss: 3.4940 Acc@1: 38.551% [9918/25728]\n",
      "| Epoch [ 63/200] Iter[221/391]\t\tLoss: 1.6343 Acc@1: 38.953% [11019/28288]\n",
      "| Epoch [ 63/200] Iter[241/391]\t\tLoss: 3.4137 Acc@1: 39.238% [12104/30848]\n",
      "| Epoch [ 63/200] Iter[261/391]\t\tLoss: 1.9786 Acc@1: 39.153% [13080/33408]\n",
      "| Epoch [ 63/200] Iter[281/391]\t\tLoss: 2.8374 Acc@1: 39.250% [14117/35968]\n",
      "| Epoch [ 63/200] Iter[301/391]\t\tLoss: 1.5720 Acc@1: 39.395% [15178/38528]\n",
      "| Epoch [ 63/200] Iter[321/391]\t\tLoss: 3.3280 Acc@1: 39.612% [16275/41088]\n",
      "| Epoch [ 63/200] Iter[341/391]\t\tLoss: 3.4828 Acc@1: 39.539% [17258/43648]\n",
      "| Epoch [ 63/200] Iter[361/391]\t\tLoss: 3.5078 Acc@1: 39.533% [18267/46208]\n",
      "| Epoch [ 63/200] Iter[381/391]\t\tLoss: 2.5306 Acc@1: 39.601% [19312/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #63\n",
      "\n",
      "| Validation Epoch #63\t\t\tLoss: 1.8313 Acc@1: 54.23%\n",
      "\n",
      "----- Elapsed time : 0:18:32\n",
      "\n",
      "\n",
      "=> Training Epoch #64\n",
      "| Epoch [ 64/200] Iter[  1/391]\t\tLoss: 3.4859 Acc@1: 20.832% [ 26/128]\n",
      "| Epoch [ 64/200] Iter[ 21/391]\t\tLoss: 2.5410 Acc@1: 39.957% [1074/2688]\n",
      "| Epoch [ 64/200] Iter[ 41/391]\t\tLoss: 1.7840 Acc@1: 38.472% [2019/5248]\n",
      "| Epoch [ 64/200] Iter[ 61/391]\t\tLoss: 2.7249 Acc@1: 37.342% [2915/7808]\n",
      "| Epoch [ 64/200] Iter[ 81/391]\t\tLoss: 1.5777 Acc@1: 38.012% [3941/10368]\n",
      "| Epoch [ 64/200] Iter[101/391]\t\tLoss: 2.4092 Acc@1: 38.947% [5035/12928]\n",
      "| Epoch [ 64/200] Iter[121/391]\t\tLoss: 2.9572 Acc@1: 38.668% [5988/15488]\n",
      "| Epoch [ 64/200] Iter[141/391]\t\tLoss: 3.1205 Acc@1: 39.696% [7164/18048]\n",
      "| Epoch [ 64/200] Iter[161/391]\t\tLoss: 2.0359 Acc@1: 39.636% [8168/20608]\n",
      "| Epoch [ 64/200] Iter[181/391]\t\tLoss: 2.2656 Acc@1: 39.705% [9198/23168]\n",
      "| Epoch [ 64/200] Iter[201/391]\t\tLoss: 2.7118 Acc@1: 39.462% [10152/25728]\n",
      "| Epoch [ 64/200] Iter[221/391]\t\tLoss: 2.4376 Acc@1: 39.627% [11209/28288]\n",
      "| Epoch [ 64/200] Iter[241/391]\t\tLoss: 3.3912 Acc@1: 39.447% [12168/30848]\n",
      "| Epoch [ 64/200] Iter[261/391]\t\tLoss: 1.7938 Acc@1: 39.551% [13213/33408]\n",
      "| Epoch [ 64/200] Iter[281/391]\t\tLoss: 3.0691 Acc@1: 39.603% [14244/35968]\n",
      "| Epoch [ 64/200] Iter[301/391]\t\tLoss: 3.3367 Acc@1: 39.644% [15274/38528]\n",
      "| Epoch [ 64/200] Iter[321/391]\t\tLoss: 2.6017 Acc@1: 39.463% [16214/41088]\n",
      "| Epoch [ 64/200] Iter[341/391]\t\tLoss: 1.4761 Acc@1: 39.526% [17252/43648]\n",
      "| Epoch [ 64/200] Iter[361/391]\t\tLoss: 2.6090 Acc@1: 39.575% [18286/46208]\n",
      "| Epoch [ 64/200] Iter[381/391]\t\tLoss: 2.3422 Acc@1: 39.505% [19265/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #64\n",
      "\n",
      "| Validation Epoch #64\t\t\tLoss: 1.7484 Acc@1: 54.49%\n",
      "\n",
      "----- Elapsed time : 0:18:49\n",
      "\n",
      "\n",
      "=> Training Epoch #65\n",
      "| Epoch [ 65/200] Iter[  1/391]\t\tLoss: 3.0842 Acc@1: 34.188% [ 43/128]\n",
      "| Epoch [ 65/200] Iter[ 21/391]\t\tLoss: 3.3842 Acc@1: 42.979% [1155/2688]\n",
      "| Epoch [ 65/200] Iter[ 41/391]\t\tLoss: 2.8874 Acc@1: 42.183% [2213/5248]\n",
      "| Epoch [ 65/200] Iter[ 61/391]\t\tLoss: 2.1547 Acc@1: 41.594% [3247/7808]\n",
      "| Epoch [ 65/200] Iter[ 81/391]\t\tLoss: 2.3637 Acc@1: 41.577% [4310/10368]\n",
      "| Epoch [ 65/200] Iter[101/391]\t\tLoss: 1.8203 Acc@1: 40.902% [5287/12928]\n",
      "| Epoch [ 65/200] Iter[121/391]\t\tLoss: 3.3881 Acc@1: 40.690% [6302/15488]\n",
      "| Epoch [ 65/200] Iter[141/391]\t\tLoss: 3.1402 Acc@1: 40.451% [7300/18048]\n",
      "| Epoch [ 65/200] Iter[161/391]\t\tLoss: 2.3695 Acc@1: 40.576% [8361/20608]\n",
      "| Epoch [ 65/200] Iter[181/391]\t\tLoss: 2.9216 Acc@1: 40.085% [9286/23168]\n",
      "| Epoch [ 65/200] Iter[201/391]\t\tLoss: 3.2907 Acc@1: 40.338% [10378/25728]\n",
      "| Epoch [ 65/200] Iter[221/391]\t\tLoss: 3.3607 Acc@1: 39.910% [11289/28288]\n",
      "| Epoch [ 65/200] Iter[241/391]\t\tLoss: 3.0036 Acc@1: 40.180% [12394/30848]\n",
      "| Epoch [ 65/200] Iter[261/391]\t\tLoss: 1.9716 Acc@1: 40.495% [13528/33408]\n",
      "| Epoch [ 65/200] Iter[281/391]\t\tLoss: 2.9224 Acc@1: 40.129% [14433/35968]\n",
      "| Epoch [ 65/200] Iter[301/391]\t\tLoss: 2.9914 Acc@1: 40.088% [15445/38528]\n",
      "| Epoch [ 65/200] Iter[321/391]\t\tLoss: 2.8104 Acc@1: 40.012% [16440/41088]\n",
      "| Epoch [ 65/200] Iter[341/391]\t\tLoss: 2.0507 Acc@1: 40.189% [17541/43648]\n",
      "| Epoch [ 65/200] Iter[361/391]\t\tLoss: 3.3068 Acc@1: 40.088% [18523/46208]\n",
      "| Epoch [ 65/200] Iter[381/391]\t\tLoss: 3.4317 Acc@1: 39.879% [19448/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #65\n",
      "\n",
      "| Validation Epoch #65\t\t\tLoss: 1.9617 Acc@1: 55.82%\n",
      "\n",
      "----- Elapsed time : 0:19:07\n",
      "\n",
      "\n",
      "=> Training Epoch #66\n",
      "| Epoch [ 66/200] Iter[  1/391]\t\tLoss: 3.2264 Acc@1: 28.826% [ 36/128]\n",
      "| Epoch [ 66/200] Iter[ 21/391]\t\tLoss: 2.7747 Acc@1: 40.253% [1081/2688]\n",
      "| Epoch [ 66/200] Iter[ 41/391]\t\tLoss: 2.2635 Acc@1: 40.992% [2151/5248]\n",
      "| Epoch [ 66/200] Iter[ 61/391]\t\tLoss: 2.1582 Acc@1: 40.667% [3175/7808]\n",
      "| Epoch [ 66/200] Iter[ 81/391]\t\tLoss: 3.3492 Acc@1: 40.545% [4203/10368]\n",
      "| Epoch [ 66/200] Iter[101/391]\t\tLoss: 3.3934 Acc@1: 39.389% [5092/12928]\n",
      "| Epoch [ 66/200] Iter[121/391]\t\tLoss: 1.8529 Acc@1: 40.028% [6199/15488]\n",
      "| Epoch [ 66/200] Iter[141/391]\t\tLoss: 1.8880 Acc@1: 39.588% [7144/18048]\n",
      "| Epoch [ 66/200] Iter[161/391]\t\tLoss: 3.4299 Acc@1: 39.647% [8170/20608]\n",
      "| Epoch [ 66/200] Iter[181/391]\t\tLoss: 2.7972 Acc@1: 39.731% [9204/23168]\n",
      "| Epoch [ 66/200] Iter[201/391]\t\tLoss: 3.0366 Acc@1: 39.633% [10196/25728]\n",
      "| Epoch [ 66/200] Iter[221/391]\t\tLoss: 3.5017 Acc@1: 39.200% [11089/28288]\n",
      "| Epoch [ 66/200] Iter[241/391]\t\tLoss: 3.2222 Acc@1: 38.750% [11953/30848]\n",
      "| Epoch [ 66/200] Iter[261/391]\t\tLoss: 2.1782 Acc@1: 39.015% [13034/33408]\n",
      "| Epoch [ 66/200] Iter[281/391]\t\tLoss: 2.6506 Acc@1: 39.354% [14154/35968]\n",
      "| Epoch [ 66/200] Iter[301/391]\t\tLoss: 2.9851 Acc@1: 39.574% [15246/38528]\n",
      "| Epoch [ 66/200] Iter[321/391]\t\tLoss: 3.4044 Acc@1: 39.691% [16308/41088]\n",
      "| Epoch [ 66/200] Iter[341/391]\t\tLoss: 3.3062 Acc@1: 39.403% [17198/43648]\n",
      "| Epoch [ 66/200] Iter[361/391]\t\tLoss: 3.5304 Acc@1: 39.505% [18254/46208]\n",
      "| Epoch [ 66/200] Iter[381/391]\t\tLoss: 3.2517 Acc@1: 39.371% [19200/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #66\n",
      "\n",
      "| Validation Epoch #66\t\t\tLoss: 1.8078 Acc@1: 54.62%\n",
      "\n",
      "----- Elapsed time : 0:19:24\n",
      "\n",
      "\n",
      "=> Training Epoch #67\n",
      "| Epoch [ 67/200] Iter[  1/391]\t\tLoss: 2.8304 Acc@1: 43.226% [ 55/128]\n",
      "| Epoch [ 67/200] Iter[ 21/391]\t\tLoss: 1.8241 Acc@1: 41.200% [1107/2688]\n",
      "| Epoch [ 67/200] Iter[ 41/391]\t\tLoss: 1.8446 Acc@1: 41.477% [2176/5248]\n",
      "| Epoch [ 67/200] Iter[ 61/391]\t\tLoss: 2.5333 Acc@1: 39.480% [3082/7808]\n",
      "| Epoch [ 67/200] Iter[ 81/391]\t\tLoss: 2.8809 Acc@1: 39.531% [4098/10368]\n",
      "| Epoch [ 67/200] Iter[101/391]\t\tLoss: 3.4172 Acc@1: 40.281% [5207/12928]\n",
      "| Epoch [ 67/200] Iter[121/391]\t\tLoss: 2.1749 Acc@1: 40.927% [6338/15488]\n",
      "| Epoch [ 67/200] Iter[141/391]\t\tLoss: 2.6426 Acc@1: 41.000% [7399/18048]\n",
      "| Epoch [ 67/200] Iter[161/391]\t\tLoss: 2.0464 Acc@1: 41.349% [8521/20608]\n",
      "| Epoch [ 67/200] Iter[181/391]\t\tLoss: 3.1838 Acc@1: 41.042% [9508/23168]\n",
      "| Epoch [ 67/200] Iter[201/391]\t\tLoss: 1.9727 Acc@1: 40.761% [10486/25728]\n",
      "| Epoch [ 67/200] Iter[221/391]\t\tLoss: 2.4265 Acc@1: 40.291% [11397/28288]\n",
      "| Epoch [ 67/200] Iter[241/391]\t\tLoss: 2.7001 Acc@1: 40.487% [12489/30848]\n",
      "| Epoch [ 67/200] Iter[261/391]\t\tLoss: 2.4109 Acc@1: 40.500% [13530/33408]\n",
      "| Epoch [ 67/200] Iter[281/391]\t\tLoss: 3.1189 Acc@1: 40.714% [14644/35968]\n",
      "| Epoch [ 67/200] Iter[301/391]\t\tLoss: 3.4187 Acc@1: 40.228% [15498/38528]\n",
      "| Epoch [ 67/200] Iter[321/391]\t\tLoss: 3.1481 Acc@1: 40.084% [16469/41088]\n",
      "| Epoch [ 67/200] Iter[341/391]\t\tLoss: 1.8395 Acc@1: 40.212% [17551/43648]\n",
      "| Epoch [ 67/200] Iter[361/391]\t\tLoss: 2.0355 Acc@1: 40.382% [18659/46208]\n",
      "| Epoch [ 67/200] Iter[381/391]\t\tLoss: 1.8192 Acc@1: 40.524% [19762/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #67\n",
      "\n",
      "| Validation Epoch #67\t\t\tLoss: 2.1045 Acc@1: 54.63%\n",
      "\n",
      "----- Elapsed time : 0:19:41\n",
      "\n",
      "\n",
      "=> Training Epoch #68\n",
      "| Epoch [ 68/200] Iter[  1/391]\t\tLoss: 3.2266 Acc@1: 28.112% [ 35/128]\n",
      "| Epoch [ 68/200] Iter[ 21/391]\t\tLoss: 2.0185 Acc@1: 38.080% [1023/2688]\n",
      "| Epoch [ 68/200] Iter[ 41/391]\t\tLoss: 2.6324 Acc@1: 40.583% [2129/5248]\n",
      "| Epoch [ 68/200] Iter[ 61/391]\t\tLoss: 2.9803 Acc@1: 39.698% [3099/7808]\n",
      "| Epoch [ 68/200] Iter[ 81/391]\t\tLoss: 2.3226 Acc@1: 39.454% [4090/10368]\n",
      "| Epoch [ 68/200] Iter[101/391]\t\tLoss: 2.6741 Acc@1: 41.183% [5324/12928]\n",
      "| Epoch [ 68/200] Iter[121/391]\t\tLoss: 2.4720 Acc@1: 40.877% [6331/15488]\n",
      "| Epoch [ 68/200] Iter[141/391]\t\tLoss: 3.2419 Acc@1: 40.625% [7332/18048]\n",
      "| Epoch [ 68/200] Iter[161/391]\t\tLoss: 3.0629 Acc@1: 40.310% [8306/20608]\n",
      "| Epoch [ 68/200] Iter[181/391]\t\tLoss: 2.0859 Acc@1: 40.031% [9274/23168]\n",
      "| Epoch [ 68/200] Iter[201/391]\t\tLoss: 3.3673 Acc@1: 40.556% [10434/25728]\n",
      "| Epoch [ 68/200] Iter[221/391]\t\tLoss: 3.0076 Acc@1: 40.695% [11511/28288]\n",
      "| Epoch [ 68/200] Iter[241/391]\t\tLoss: 3.3139 Acc@1: 40.674% [12547/30848]\n",
      "| Epoch [ 68/200] Iter[261/391]\t\tLoss: 2.8347 Acc@1: 40.601% [13563/33408]\n",
      "| Epoch [ 68/200] Iter[281/391]\t\tLoss: 1.5999 Acc@1: 40.271% [14484/35968]\n",
      "| Epoch [ 68/200] Iter[301/391]\t\tLoss: 2.1708 Acc@1: 40.178% [15479/38528]\n",
      "| Epoch [ 68/200] Iter[321/391]\t\tLoss: 3.3859 Acc@1: 40.084% [16469/41088]\n",
      "| Epoch [ 68/200] Iter[341/391]\t\tLoss: 2.3975 Acc@1: 39.751% [17350/43648]\n",
      "| Epoch [ 68/200] Iter[361/391]\t\tLoss: 3.3792 Acc@1: 39.536% [18268/46208]\n",
      "| Epoch [ 68/200] Iter[381/391]\t\tLoss: 2.8675 Acc@1: 39.546% [19285/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #68\n",
      "\n",
      "| Validation Epoch #68\t\t\tLoss: 1.8382 Acc@1: 55.43%\n",
      "\n",
      "----- Elapsed time : 0:19:59\n",
      "\n",
      "\n",
      "=> Training Epoch #69\n",
      "| Epoch [ 69/200] Iter[  1/391]\t\tLoss: 3.1331 Acc@1: 33.893% [ 43/128]\n",
      "| Epoch [ 69/200] Iter[ 21/391]\t\tLoss: 3.4661 Acc@1: 39.387% [1058/2688]\n",
      "| Epoch [ 69/200] Iter[ 41/391]\t\tLoss: 2.7782 Acc@1: 37.486% [1967/5248]\n",
      "| Epoch [ 69/200] Iter[ 61/391]\t\tLoss: 1.9490 Acc@1: 38.398% [2998/7808]\n",
      "| Epoch [ 69/200] Iter[ 81/391]\t\tLoss: 3.3347 Acc@1: 38.290% [3969/10368]\n",
      "| Epoch [ 69/200] Iter[101/391]\t\tLoss: 2.5455 Acc@1: 39.375% [5090/12928]\n",
      "| Epoch [ 69/200] Iter[121/391]\t\tLoss: 3.5605 Acc@1: 39.796% [6163/15488]\n",
      "| Epoch [ 69/200] Iter[141/391]\t\tLoss: 3.1113 Acc@1: 40.368% [7285/18048]\n",
      "| Epoch [ 69/200] Iter[161/391]\t\tLoss: 1.5023 Acc@1: 40.992% [8447/20608]\n",
      "| Epoch [ 69/200] Iter[181/391]\t\tLoss: 2.7208 Acc@1: 41.083% [9518/23168]\n",
      "| Epoch [ 69/200] Iter[201/391]\t\tLoss: 3.4845 Acc@1: 41.075% [10567/25728]\n",
      "| Epoch [ 69/200] Iter[221/391]\t\tLoss: 3.4441 Acc@1: 40.701% [11513/28288]\n",
      "| Epoch [ 69/200] Iter[241/391]\t\tLoss: 3.5512 Acc@1: 40.511% [12496/30848]\n",
      "| Epoch [ 69/200] Iter[261/391]\t\tLoss: 2.9179 Acc@1: 40.406% [13498/33408]\n",
      "| Epoch [ 69/200] Iter[281/391]\t\tLoss: 3.0301 Acc@1: 40.092% [14420/35968]\n",
      "| Epoch [ 69/200] Iter[301/391]\t\tLoss: 1.7552 Acc@1: 39.896% [15371/38528]\n",
      "| Epoch [ 69/200] Iter[321/391]\t\tLoss: 2.4703 Acc@1: 40.070% [16463/41088]\n",
      "| Epoch [ 69/200] Iter[341/391]\t\tLoss: 3.1494 Acc@1: 39.790% [17367/43648]\n",
      "| Epoch [ 69/200] Iter[361/391]\t\tLoss: 1.8924 Acc@1: 39.659% [18325/46208]\n",
      "| Epoch [ 69/200] Iter[381/391]\t\tLoss: 3.1842 Acc@1: 39.213% [19123/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #69\n",
      "\n",
      "| Validation Epoch #69\t\t\tLoss: 1.6199 Acc@1: 56.19%\n",
      "\n",
      "----- Elapsed time : 0:20:16\n",
      "\n",
      "\n",
      "=> Training Epoch #70\n",
      "| Epoch [ 70/200] Iter[  1/391]\t\tLoss: 2.9629 Acc@1: 39.260% [ 50/128]\n",
      "| Epoch [ 70/200] Iter[ 21/391]\t\tLoss: 1.8445 Acc@1: 41.564% [1117/2688]\n",
      "| Epoch [ 70/200] Iter[ 41/391]\t\tLoss: 2.7718 Acc@1: 40.807% [2141/5248]\n",
      "| Epoch [ 70/200] Iter[ 61/391]\t\tLoss: 2.9562 Acc@1: 39.788% [3106/7808]\n",
      "| Epoch [ 70/200] Iter[ 81/391]\t\tLoss: 2.9190 Acc@1: 39.859% [4132/10368]\n",
      "| Epoch [ 70/200] Iter[101/391]\t\tLoss: 3.4194 Acc@1: 40.109% [5185/12928]\n",
      "| Epoch [ 70/200] Iter[121/391]\t\tLoss: 3.3380 Acc@1: 40.594% [6287/15488]\n",
      "| Epoch [ 70/200] Iter[141/391]\t\tLoss: 2.1599 Acc@1: 40.871% [7376/18048]\n",
      "| Epoch [ 70/200] Iter[161/391]\t\tLoss: 1.7164 Acc@1: 40.767% [8401/20608]\n",
      "| Epoch [ 70/200] Iter[181/391]\t\tLoss: 3.2281 Acc@1: 40.635% [9414/23168]\n",
      "| Epoch [ 70/200] Iter[201/391]\t\tLoss: 1.8039 Acc@1: 41.045% [10560/25728]\n",
      "| Epoch [ 70/200] Iter[221/391]\t\tLoss: 2.0121 Acc@1: 41.148% [11640/28288]\n",
      "| Epoch [ 70/200] Iter[241/391]\t\tLoss: 1.8017 Acc@1: 41.398% [12770/30848]\n",
      "| Epoch [ 70/200] Iter[261/391]\t\tLoss: 2.7802 Acc@1: 41.177% [13756/33408]\n",
      "| Epoch [ 70/200] Iter[281/391]\t\tLoss: 1.9145 Acc@1: 41.430% [14901/35968]\n",
      "| Epoch [ 70/200] Iter[301/391]\t\tLoss: 3.2056 Acc@1: 41.639% [16042/38528]\n",
      "| Epoch [ 70/200] Iter[321/391]\t\tLoss: 3.0498 Acc@1: 41.929% [17227/41088]\n",
      "| Epoch [ 70/200] Iter[341/391]\t\tLoss: 2.1261 Acc@1: 41.643% [18176/43648]\n",
      "| Epoch [ 70/200] Iter[361/391]\t\tLoss: 3.3631 Acc@1: 41.108% [18995/46208]\n",
      "| Epoch [ 70/200] Iter[381/391]\t\tLoss: 1.6347 Acc@1: 41.154% [20070/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #70\n",
      "\n",
      "| Validation Epoch #70\t\t\tLoss: 1.9430 Acc@1: 56.57%\n",
      "\n",
      "----- Elapsed time : 0:20:33\n",
      "\n",
      "\n",
      "=> Training Epoch #71\n",
      "| Epoch [ 71/200] Iter[  1/391]\t\tLoss: 2.7849 Acc@1: 44.567% [ 57/128]\n",
      "| Epoch [ 71/200] Iter[ 21/391]\t\tLoss: 1.7846 Acc@1: 42.351% [1138/2688]\n",
      "| Epoch [ 71/200] Iter[ 41/391]\t\tLoss: 3.3110 Acc@1: 42.799% [2246/5248]\n",
      "| Epoch [ 71/200] Iter[ 61/391]\t\tLoss: 3.1140 Acc@1: 40.951% [3197/7808]\n",
      "| Epoch [ 71/200] Iter[ 81/391]\t\tLoss: 2.1795 Acc@1: 41.355% [4287/10368]\n",
      "| Epoch [ 71/200] Iter[101/391]\t\tLoss: 2.4628 Acc@1: 41.312% [5340/12928]\n",
      "| Epoch [ 71/200] Iter[121/391]\t\tLoss: 2.6098 Acc@1: 41.994% [6504/15488]\n",
      "| Epoch [ 71/200] Iter[141/391]\t\tLoss: 1.6196 Acc@1: 41.805% [7545/18048]\n",
      "| Epoch [ 71/200] Iter[161/391]\t\tLoss: 2.0748 Acc@1: 42.123% [8680/20608]\n",
      "| Epoch [ 71/200] Iter[181/391]\t\tLoss: 2.3655 Acc@1: 42.156% [9766/23168]\n",
      "| Epoch [ 71/200] Iter[201/391]\t\tLoss: 2.5417 Acc@1: 42.151% [10844/25728]\n",
      "| Epoch [ 71/200] Iter[221/391]\t\tLoss: 3.4248 Acc@1: 41.570% [11759/28288]\n",
      "| Epoch [ 71/200] Iter[241/391]\t\tLoss: 3.3603 Acc@1: 41.071% [12669/30848]\n",
      "| Epoch [ 71/200] Iter[261/391]\t\tLoss: 2.3752 Acc@1: 41.010% [13700/33408]\n",
      "| Epoch [ 71/200] Iter[281/391]\t\tLoss: 2.3975 Acc@1: 41.246% [14835/35968]\n",
      "| Epoch [ 71/200] Iter[301/391]\t\tLoss: 3.0207 Acc@1: 40.818% [15726/38528]\n",
      "| Epoch [ 71/200] Iter[321/391]\t\tLoss: 2.9568 Acc@1: 40.697% [16721/41088]\n",
      "| Epoch [ 71/200] Iter[341/391]\t\tLoss: 3.1519 Acc@1: 40.584% [17713/43648]\n",
      "| Epoch [ 71/200] Iter[361/391]\t\tLoss: 3.1927 Acc@1: 40.329% [18635/46208]\n",
      "| Epoch [ 71/200] Iter[381/391]\t\tLoss: 2.6284 Acc@1: 39.858% [19438/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #71\n",
      "\n",
      "| Validation Epoch #71\t\t\tLoss: 1.8150 Acc@1: 57.29%\n",
      "\n",
      "----- Elapsed time : 0:20:51\n",
      "\n",
      "\n",
      "=> Training Epoch #72\n",
      "| Epoch [ 72/200] Iter[  1/391]\t\tLoss: 2.3559 Acc@1: 54.269% [ 69/128]\n",
      "| Epoch [ 72/200] Iter[ 21/391]\t\tLoss: 1.9421 Acc@1: 48.788% [1311/2688]\n",
      "| Epoch [ 72/200] Iter[ 41/391]\t\tLoss: 3.4209 Acc@1: 40.209% [2110/5248]\n",
      "| Epoch [ 72/200] Iter[ 61/391]\t\tLoss: 1.9820 Acc@1: 40.414% [3155/7808]\n",
      "| Epoch [ 72/200] Iter[ 81/391]\t\tLoss: 2.4213 Acc@1: 42.201% [4375/10368]\n",
      "| Epoch [ 72/200] Iter[101/391]\t\tLoss: 2.9131 Acc@1: 41.876% [5413/12928]\n",
      "| Epoch [ 72/200] Iter[121/391]\t\tLoss: 2.5681 Acc@1: 42.180% [6532/15488]\n",
      "| Epoch [ 72/200] Iter[141/391]\t\tLoss: 3.3493 Acc@1: 42.374% [7647/18048]\n",
      "| Epoch [ 72/200] Iter[161/391]\t\tLoss: 1.9215 Acc@1: 42.539% [8766/20608]\n",
      "| Epoch [ 72/200] Iter[181/391]\t\tLoss: 2.6195 Acc@1: 42.291% [9797/23168]\n",
      "| Epoch [ 72/200] Iter[201/391]\t\tLoss: 3.2667 Acc@1: 41.248% [10612/25728]\n",
      "| Epoch [ 72/200] Iter[221/391]\t\tLoss: 2.6083 Acc@1: 40.938% [11580/28288]\n",
      "| Epoch [ 72/200] Iter[241/391]\t\tLoss: 1.9453 Acc@1: 41.042% [12660/30848]\n",
      "| Epoch [ 72/200] Iter[261/391]\t\tLoss: 1.9622 Acc@1: 40.524% [13538/33408]\n",
      "| Epoch [ 72/200] Iter[281/391]\t\tLoss: 2.2422 Acc@1: 40.036% [14399/35968]\n",
      "| Epoch [ 72/200] Iter[301/391]\t\tLoss: 2.6400 Acc@1: 40.108% [15452/38528]\n",
      "| Epoch [ 72/200] Iter[321/391]\t\tLoss: 1.7694 Acc@1: 40.081% [16468/41088]\n",
      "| Epoch [ 72/200] Iter[341/391]\t\tLoss: 3.0051 Acc@1: 40.135% [17518/43648]\n",
      "| Epoch [ 72/200] Iter[361/391]\t\tLoss: 2.8664 Acc@1: 40.054% [18507/46208]\n",
      "| Epoch [ 72/200] Iter[381/391]\t\tLoss: 2.3030 Acc@1: 39.999% [19506/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #72\n",
      "\n",
      "| Validation Epoch #72\t\t\tLoss: 1.7643 Acc@1: 56.37%\n",
      "\n",
      "----- Elapsed time : 0:21:08\n",
      "\n",
      "\n",
      "=> Training Epoch #73\n",
      "| Epoch [ 73/200] Iter[  1/391]\t\tLoss: 2.3121 Acc@1: 52.075% [ 66/128]\n",
      "| Epoch [ 73/200] Iter[ 21/391]\t\tLoss: 3.4386 Acc@1: 46.685% [1254/2688]\n",
      "| Epoch [ 73/200] Iter[ 41/391]\t\tLoss: 1.7252 Acc@1: 43.679% [2292/5248]\n",
      "| Epoch [ 73/200] Iter[ 61/391]\t\tLoss: 1.6129 Acc@1: 42.296% [3302/7808]\n",
      "| Epoch [ 73/200] Iter[ 81/391]\t\tLoss: 3.2794 Acc@1: 39.761% [4122/10368]\n",
      "| Epoch [ 73/200] Iter[101/391]\t\tLoss: 2.3585 Acc@1: 39.917% [5160/12928]\n",
      "| Epoch [ 73/200] Iter[121/391]\t\tLoss: 2.9032 Acc@1: 40.272% [6237/15488]\n",
      "| Epoch [ 73/200] Iter[141/391]\t\tLoss: 2.5586 Acc@1: 40.020% [7222/18048]\n",
      "| Epoch [ 73/200] Iter[161/391]\t\tLoss: 3.4332 Acc@1: 40.854% [8419/20608]\n",
      "| Epoch [ 73/200] Iter[181/391]\t\tLoss: 1.5702 Acc@1: 40.571% [9399/23168]\n",
      "| Epoch [ 73/200] Iter[201/391]\t\tLoss: 1.9699 Acc@1: 40.780% [10491/25728]\n",
      "| Epoch [ 73/200] Iter[221/391]\t\tLoss: 3.0256 Acc@1: 40.670% [11504/28288]\n",
      "| Epoch [ 73/200] Iter[241/391]\t\tLoss: 3.0450 Acc@1: 40.684% [12550/30848]\n",
      "| Epoch [ 73/200] Iter[261/391]\t\tLoss: 2.5515 Acc@1: 40.695% [13595/33408]\n",
      "| Epoch [ 73/200] Iter[281/391]\t\tLoss: 2.8446 Acc@1: 40.691% [14635/35968]\n",
      "| Epoch [ 73/200] Iter[301/391]\t\tLoss: 2.2563 Acc@1: 40.827% [15729/38528]\n",
      "| Epoch [ 73/200] Iter[321/391]\t\tLoss: 2.1141 Acc@1: 40.899% [16804/41088]\n",
      "| Epoch [ 73/200] Iter[341/391]\t\tLoss: 3.2037 Acc@1: 40.439% [17650/43648]\n",
      "| Epoch [ 73/200] Iter[361/391]\t\tLoss: 2.6557 Acc@1: 40.094% [18526/46208]\n",
      "| Epoch [ 73/200] Iter[381/391]\t\tLoss: 1.9077 Acc@1: 40.072% [19542/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #73\n",
      "\n",
      "| Validation Epoch #73\t\t\tLoss: 1.9057 Acc@1: 55.84%\n",
      "\n",
      "----- Elapsed time : 0:21:25\n",
      "\n",
      "\n",
      "=> Training Epoch #74\n",
      "| Epoch [ 74/200] Iter[  1/391]\t\tLoss: 2.0799 Acc@1: 58.194% [ 74/128]\n",
      "| Epoch [ 74/200] Iter[ 21/391]\t\tLoss: 2.5954 Acc@1: 39.838% [1070/2688]\n",
      "| Epoch [ 74/200] Iter[ 41/391]\t\tLoss: 3.4813 Acc@1: 40.255% [2112/5248]\n",
      "| Epoch [ 74/200] Iter[ 61/391]\t\tLoss: 3.3336 Acc@1: 39.852% [3111/7808]\n",
      "| Epoch [ 74/200] Iter[ 81/391]\t\tLoss: 3.4598 Acc@1: 40.828% [4233/10368]\n",
      "| Epoch [ 74/200] Iter[101/391]\t\tLoss: 2.5435 Acc@1: 41.344% [5344/12928]\n",
      "| Epoch [ 74/200] Iter[121/391]\t\tLoss: 3.2930 Acc@1: 40.901% [6334/15488]\n",
      "| Epoch [ 74/200] Iter[141/391]\t\tLoss: 1.9210 Acc@1: 40.642% [7335/18048]\n",
      "| Epoch [ 74/200] Iter[161/391]\t\tLoss: 3.6422 Acc@1: 40.558% [8358/20608]\n",
      "| Epoch [ 74/200] Iter[181/391]\t\tLoss: 2.8762 Acc@1: 40.891% [9473/23168]\n",
      "| Epoch [ 74/200] Iter[201/391]\t\tLoss: 2.7839 Acc@1: 40.562% [10435/25728]\n",
      "| Epoch [ 74/200] Iter[221/391]\t\tLoss: 3.3630 Acc@1: 40.531% [11465/28288]\n",
      "| Epoch [ 74/200] Iter[241/391]\t\tLoss: 3.0408 Acc@1: 41.045% [12661/30848]\n",
      "| Epoch [ 74/200] Iter[261/391]\t\tLoss: 3.2812 Acc@1: 41.343% [13812/33408]\n",
      "| Epoch [ 74/200] Iter[281/391]\t\tLoss: 3.4161 Acc@1: 41.133% [14794/35968]\n",
      "| Epoch [ 74/200] Iter[301/391]\t\tLoss: 3.3064 Acc@1: 41.136% [15849/38528]\n",
      "| Epoch [ 74/200] Iter[321/391]\t\tLoss: 2.5314 Acc@1: 41.193% [16925/41088]\n",
      "| Epoch [ 74/200] Iter[341/391]\t\tLoss: 3.4011 Acc@1: 41.165% [17967/43648]\n",
      "| Epoch [ 74/200] Iter[361/391]\t\tLoss: 3.1933 Acc@1: 41.297% [19082/46208]\n",
      "| Epoch [ 74/200] Iter[381/391]\t\tLoss: 2.8652 Acc@1: 41.528% [20252/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #74\n",
      "\n",
      "| Validation Epoch #74\t\t\tLoss: 1.7522 Acc@1: 57.32%\n",
      "\n",
      "----- Elapsed time : 0:21:43\n",
      "\n",
      "\n",
      "=> Training Epoch #75\n",
      "| Epoch [ 75/200] Iter[  1/391]\t\tLoss: 2.4962 Acc@1: 50.365% [ 64/128]\n",
      "| Epoch [ 75/200] Iter[ 21/391]\t\tLoss: 2.8192 Acc@1: 38.241% [1027/2688]\n",
      "| Epoch [ 75/200] Iter[ 41/391]\t\tLoss: 3.4667 Acc@1: 38.460% [2018/5248]\n",
      "| Epoch [ 75/200] Iter[ 61/391]\t\tLoss: 2.5140 Acc@1: 38.979% [3043/7808]\n",
      "| Epoch [ 75/200] Iter[ 81/391]\t\tLoss: 2.5952 Acc@1: 39.250% [4069/10368]\n",
      "| Epoch [ 75/200] Iter[101/391]\t\tLoss: 2.6602 Acc@1: 39.326% [5084/12928]\n",
      "| Epoch [ 75/200] Iter[121/391]\t\tLoss: 2.9980 Acc@1: 39.581% [6130/15488]\n",
      "| Epoch [ 75/200] Iter[141/391]\t\tLoss: 1.8896 Acc@1: 39.706% [7166/18048]\n",
      "| Epoch [ 75/200] Iter[161/391]\t\tLoss: 3.3033 Acc@1: 39.819% [8205/20608]\n",
      "| Epoch [ 75/200] Iter[181/391]\t\tLoss: 1.9334 Acc@1: 39.742% [9207/23168]\n",
      "| Epoch [ 75/200] Iter[201/391]\t\tLoss: 3.1831 Acc@1: 40.129% [10324/25728]\n",
      "| Epoch [ 75/200] Iter[221/391]\t\tLoss: 2.7378 Acc@1: 40.340% [11411/28288]\n",
      "| Epoch [ 75/200] Iter[241/391]\t\tLoss: 2.9820 Acc@1: 40.261% [12419/30848]\n",
      "| Epoch [ 75/200] Iter[261/391]\t\tLoss: 2.6305 Acc@1: 40.459% [13516/33408]\n",
      "| Epoch [ 75/200] Iter[281/391]\t\tLoss: 3.1332 Acc@1: 40.303% [14496/35968]\n",
      "| Epoch [ 75/200] Iter[301/391]\t\tLoss: 3.3821 Acc@1: 40.579% [15634/38528]\n",
      "| Epoch [ 75/200] Iter[321/391]\t\tLoss: 2.8613 Acc@1: 41.008% [16849/41088]\n",
      "| Epoch [ 75/200] Iter[341/391]\t\tLoss: 1.9070 Acc@1: 41.237% [17999/43648]\n",
      "| Epoch [ 75/200] Iter[361/391]\t\tLoss: 2.2162 Acc@1: 40.936% [18915/46208]\n",
      "| Epoch [ 75/200] Iter[381/391]\t\tLoss: 3.2960 Acc@1: 40.871% [19932/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #75\n",
      "\n",
      "| Validation Epoch #75\t\t\tLoss: 1.8965 Acc@1: 56.64%\n",
      "\n",
      "----- Elapsed time : 0:22:00\n",
      "\n",
      "\n",
      "=> Training Epoch #76\n",
      "| Epoch [ 76/200] Iter[  1/391]\t\tLoss: 2.4414 Acc@1: 47.831% [ 61/128]\n",
      "| Epoch [ 76/200] Iter[ 21/391]\t\tLoss: 2.2925 Acc@1: 42.685% [1147/2688]\n",
      "| Epoch [ 76/200] Iter[ 41/391]\t\tLoss: 3.1899 Acc@1: 39.378% [2066/5248]\n",
      "| Epoch [ 76/200] Iter[ 61/391]\t\tLoss: 3.2276 Acc@1: 39.842% [3110/7808]\n",
      "| Epoch [ 76/200] Iter[ 81/391]\t\tLoss: 1.9008 Acc@1: 41.114% [4262/10368]\n",
      "| Epoch [ 76/200] Iter[101/391]\t\tLoss: 2.6579 Acc@1: 41.944% [5422/12928]\n",
      "| Epoch [ 76/200] Iter[121/391]\t\tLoss: 3.3254 Acc@1: 41.589% [6441/15488]\n",
      "| Epoch [ 76/200] Iter[141/391]\t\tLoss: 3.2809 Acc@1: 41.696% [7525/18048]\n",
      "| Epoch [ 76/200] Iter[161/391]\t\tLoss: 3.4059 Acc@1: 41.666% [8586/20608]\n",
      "| Epoch [ 76/200] Iter[181/391]\t\tLoss: 3.2478 Acc@1: 41.644% [9648/23168]\n",
      "| Epoch [ 76/200] Iter[201/391]\t\tLoss: 2.5136 Acc@1: 41.015% [10552/25728]\n",
      "| Epoch [ 76/200] Iter[221/391]\t\tLoss: 3.1424 Acc@1: 41.119% [11631/28288]\n",
      "| Epoch [ 76/200] Iter[241/391]\t\tLoss: 1.7760 Acc@1: 41.418% [12776/30848]\n",
      "| Epoch [ 76/200] Iter[261/391]\t\tLoss: 1.9986 Acc@1: 41.298% [13796/33408]\n",
      "| Epoch [ 76/200] Iter[281/391]\t\tLoss: 2.5225 Acc@1: 41.104% [14784/35968]\n",
      "| Epoch [ 76/200] Iter[301/391]\t\tLoss: 3.1055 Acc@1: 40.968% [15783/38528]\n",
      "| Epoch [ 76/200] Iter[321/391]\t\tLoss: 2.3236 Acc@1: 40.911% [16809/41088]\n",
      "| Epoch [ 76/200] Iter[341/391]\t\tLoss: 3.0038 Acc@1: 40.845% [17827/43648]\n",
      "| Epoch [ 76/200] Iter[361/391]\t\tLoss: 2.1344 Acc@1: 41.229% [19051/46208]\n",
      "| Epoch [ 76/200] Iter[381/391]\t\tLoss: 3.3710 Acc@1: 40.901% [19946/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #76\n",
      "\n",
      "| Validation Epoch #76\t\t\tLoss: 1.6720 Acc@1: 56.70%\n",
      "\n",
      "----- Elapsed time : 0:22:18\n",
      "\n",
      "\n",
      "=> Training Epoch #77\n",
      "| Epoch [ 77/200] Iter[  1/391]\t\tLoss: 3.3605 Acc@1: 25.507% [ 32/128]\n",
      "| Epoch [ 77/200] Iter[ 21/391]\t\tLoss: 2.0620 Acc@1: 40.004% [1075/2688]\n",
      "| Epoch [ 77/200] Iter[ 41/391]\t\tLoss: 2.4754 Acc@1: 42.155% [2212/5248]\n",
      "| Epoch [ 77/200] Iter[ 61/391]\t\tLoss: 1.4960 Acc@1: 41.620% [3249/7808]\n",
      "| Epoch [ 77/200] Iter[ 81/391]\t\tLoss: 2.5584 Acc@1: 41.810% [4334/10368]\n",
      "| Epoch [ 77/200] Iter[101/391]\t\tLoss: 3.3180 Acc@1: 40.956% [5294/12928]\n",
      "| Epoch [ 77/200] Iter[121/391]\t\tLoss: 2.0639 Acc@1: 41.243% [6387/15488]\n",
      "| Epoch [ 77/200] Iter[141/391]\t\tLoss: 2.5844 Acc@1: 41.299% [7453/18048]\n",
      "| Epoch [ 77/200] Iter[161/391]\t\tLoss: 2.6118 Acc@1: 41.397% [8531/20608]\n",
      "| Epoch [ 77/200] Iter[181/391]\t\tLoss: 1.8966 Acc@1: 41.283% [9564/23168]\n",
      "| Epoch [ 77/200] Iter[201/391]\t\tLoss: 3.3437 Acc@1: 41.049% [10560/25728]\n",
      "| Epoch [ 77/200] Iter[221/391]\t\tLoss: 2.5372 Acc@1: 40.667% [11503/28288]\n",
      "| Epoch [ 77/200] Iter[241/391]\t\tLoss: 2.6087 Acc@1: 40.485% [12488/30848]\n",
      "| Epoch [ 77/200] Iter[261/391]\t\tLoss: 1.4372 Acc@1: 40.879% [13656/33408]\n",
      "| Epoch [ 77/200] Iter[281/391]\t\tLoss: 3.2102 Acc@1: 40.977% [14738/35968]\n",
      "| Epoch [ 77/200] Iter[301/391]\t\tLoss: 3.1033 Acc@1: 40.922% [15766/38528]\n",
      "| Epoch [ 77/200] Iter[321/391]\t\tLoss: 2.3762 Acc@1: 41.102% [16888/41088]\n",
      "| Epoch [ 77/200] Iter[341/391]\t\tLoss: 1.6321 Acc@1: 41.231% [17996/43648]\n",
      "| Epoch [ 77/200] Iter[361/391]\t\tLoss: 3.1869 Acc@1: 41.054% [18970/46208]\n",
      "| Epoch [ 77/200] Iter[381/391]\t\tLoss: 2.7696 Acc@1: 40.693% [19845/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #77\n",
      "\n",
      "| Validation Epoch #77\t\t\tLoss: 1.6119 Acc@1: 56.46%\n",
      "\n",
      "----- Elapsed time : 0:22:35\n",
      "\n",
      "\n",
      "=> Training Epoch #78\n",
      "| Epoch [ 78/200] Iter[  1/391]\t\tLoss: 2.8109 Acc@1: 42.716% [ 54/128]\n",
      "| Epoch [ 78/200] Iter[ 21/391]\t\tLoss: 3.5115 Acc@1: 40.410% [1086/2688]\n",
      "| Epoch [ 78/200] Iter[ 41/391]\t\tLoss: 1.8573 Acc@1: 42.175% [2213/5248]\n",
      "| Epoch [ 78/200] Iter[ 61/391]\t\tLoss: 2.4976 Acc@1: 44.906% [3506/7808]\n",
      "| Epoch [ 78/200] Iter[ 81/391]\t\tLoss: 1.7241 Acc@1: 44.089% [4571/10368]\n",
      "| Epoch [ 78/200] Iter[101/391]\t\tLoss: 3.0508 Acc@1: 43.541% [5628/12928]\n",
      "| Epoch [ 78/200] Iter[121/391]\t\tLoss: 2.6747 Acc@1: 43.467% [6732/15488]\n",
      "| Epoch [ 78/200] Iter[141/391]\t\tLoss: 2.8626 Acc@1: 43.155% [7788/18048]\n",
      "| Epoch [ 78/200] Iter[161/391]\t\tLoss: 3.1147 Acc@1: 42.588% [8776/20608]\n",
      "| Epoch [ 78/200] Iter[181/391]\t\tLoss: 2.7494 Acc@1: 42.434% [9831/23168]\n",
      "| Epoch [ 78/200] Iter[201/391]\t\tLoss: 3.0804 Acc@1: 42.145% [10843/25728]\n",
      "| Epoch [ 78/200] Iter[221/391]\t\tLoss: 1.6252 Acc@1: 42.295% [11964/28288]\n",
      "| Epoch [ 78/200] Iter[241/391]\t\tLoss: 2.6208 Acc@1: 41.760% [12882/30848]\n",
      "| Epoch [ 78/200] Iter[261/391]\t\tLoss: 3.3387 Acc@1: 41.528% [13873/33408]\n",
      "| Epoch [ 78/200] Iter[281/391]\t\tLoss: 2.2719 Acc@1: 41.869% [15059/35968]\n",
      "| Epoch [ 78/200] Iter[301/391]\t\tLoss: 1.4743 Acc@1: 41.875% [16133/38528]\n",
      "| Epoch [ 78/200] Iter[321/391]\t\tLoss: 2.6545 Acc@1: 41.994% [17254/41088]\n",
      "| Epoch [ 78/200] Iter[341/391]\t\tLoss: 2.8340 Acc@1: 41.828% [18256/43648]\n",
      "| Epoch [ 78/200] Iter[361/391]\t\tLoss: 1.7618 Acc@1: 41.969% [19392/46208]\n",
      "| Epoch [ 78/200] Iter[381/391]\t\tLoss: 2.6988 Acc@1: 41.911% [20439/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #78\n",
      "\n",
      "| Validation Epoch #78\t\t\tLoss: 1.9109 Acc@1: 57.25%\n",
      "\n",
      "----- Elapsed time : 0:22:53\n",
      "\n",
      "\n",
      "=> Training Epoch #79\n",
      "| Epoch [ 79/200] Iter[  1/391]\t\tLoss: 3.2529 Acc@1: 26.164% [ 33/128]\n",
      "| Epoch [ 79/200] Iter[ 21/391]\t\tLoss: 2.7798 Acc@1: 38.534% [1035/2688]\n",
      "| Epoch [ 79/200] Iter[ 41/391]\t\tLoss: 3.2337 Acc@1: 40.917% [2147/5248]\n",
      "| Epoch [ 79/200] Iter[ 61/391]\t\tLoss: 2.2288 Acc@1: 40.507% [3162/7808]\n",
      "| Epoch [ 79/200] Iter[ 81/391]\t\tLoss: 1.6313 Acc@1: 41.762% [4329/10368]\n",
      "| Epoch [ 79/200] Iter[101/391]\t\tLoss: 3.3204 Acc@1: 41.691% [5389/12928]\n",
      "| Epoch [ 79/200] Iter[121/391]\t\tLoss: 3.2111 Acc@1: 41.375% [6408/15488]\n",
      "| Epoch [ 79/200] Iter[141/391]\t\tLoss: 3.1945 Acc@1: 41.041% [7407/18048]\n",
      "| Epoch [ 79/200] Iter[161/391]\t\tLoss: 3.1292 Acc@1: 41.368% [8525/20608]\n",
      "| Epoch [ 79/200] Iter[181/391]\t\tLoss: 2.0866 Acc@1: 41.325% [9574/23168]\n",
      "| Epoch [ 79/200] Iter[201/391]\t\tLoss: 3.2534 Acc@1: 41.035% [10557/25728]\n",
      "| Epoch [ 79/200] Iter[221/391]\t\tLoss: 2.5968 Acc@1: 40.648% [11498/28288]\n",
      "| Epoch [ 79/200] Iter[241/391]\t\tLoss: 3.4860 Acc@1: 40.703% [12556/30848]\n",
      "| Epoch [ 79/200] Iter[261/391]\t\tLoss: 2.4323 Acc@1: 40.431% [13507/33408]\n",
      "| Epoch [ 79/200] Iter[281/391]\t\tLoss: 3.3762 Acc@1: 40.381% [14524/35968]\n",
      "| Epoch [ 79/200] Iter[301/391]\t\tLoss: 2.5471 Acc@1: 40.456% [15586/38528]\n",
      "| Epoch [ 79/200] Iter[321/391]\t\tLoss: 1.3822 Acc@1: 40.371% [16587/41088]\n",
      "| Epoch [ 79/200] Iter[341/391]\t\tLoss: 2.7721 Acc@1: 40.394% [17631/43648]\n",
      "| Epoch [ 79/200] Iter[361/391]\t\tLoss: 2.9008 Acc@1: 40.485% [18707/46208]\n",
      "| Epoch [ 79/200] Iter[381/391]\t\tLoss: 3.3156 Acc@1: 40.475% [19738/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #79\n",
      "\n",
      "| Validation Epoch #79\t\t\tLoss: 2.0008 Acc@1: 55.78%\n",
      "\n",
      "----- Elapsed time : 0:23:10\n",
      "\n",
      "\n",
      "=> Training Epoch #80\n",
      "| Epoch [ 80/200] Iter[  1/391]\t\tLoss: 2.3628 Acc@1: 47.620% [ 60/128]\n",
      "| Epoch [ 80/200] Iter[ 21/391]\t\tLoss: 3.0505 Acc@1: 43.157% [1160/2688]\n",
      "| Epoch [ 80/200] Iter[ 41/391]\t\tLoss: 2.8744 Acc@1: 43.511% [2283/5248]\n",
      "| Epoch [ 80/200] Iter[ 61/391]\t\tLoss: 3.4794 Acc@1: 45.970% [3589/7808]\n",
      "| Epoch [ 80/200] Iter[ 81/391]\t\tLoss: 1.5262 Acc@1: 45.421% [4709/10368]\n",
      "| Epoch [ 80/200] Iter[101/391]\t\tLoss: 3.5164 Acc@1: 44.262% [5722/12928]\n",
      "| Epoch [ 80/200] Iter[121/391]\t\tLoss: 2.7066 Acc@1: 42.522% [6585/15488]\n",
      "| Epoch [ 80/200] Iter[141/391]\t\tLoss: 3.3482 Acc@1: 41.522% [7493/18048]\n",
      "| Epoch [ 80/200] Iter[161/391]\t\tLoss: 2.6508 Acc@1: 41.233% [8497/20608]\n",
      "| Epoch [ 80/200] Iter[181/391]\t\tLoss: 2.1215 Acc@1: 40.791% [9450/23168]\n",
      "| Epoch [ 80/200] Iter[201/391]\t\tLoss: 3.4045 Acc@1: 41.354% [10639/25728]\n",
      "| Epoch [ 80/200] Iter[221/391]\t\tLoss: 3.2309 Acc@1: 40.834% [11551/28288]\n",
      "| Epoch [ 80/200] Iter[241/391]\t\tLoss: 3.2732 Acc@1: 41.128% [12687/30848]\n",
      "| Epoch [ 80/200] Iter[261/391]\t\tLoss: 2.6919 Acc@1: 41.562% [13885/33408]\n",
      "| Epoch [ 80/200] Iter[281/391]\t\tLoss: 2.0984 Acc@1: 41.560% [14948/35968]\n",
      "| Epoch [ 80/200] Iter[301/391]\t\tLoss: 2.6704 Acc@1: 41.155% [15856/38528]\n",
      "| Epoch [ 80/200] Iter[321/391]\t\tLoss: 3.2806 Acc@1: 40.938% [16820/41088]\n",
      "| Epoch [ 80/200] Iter[341/391]\t\tLoss: 2.8622 Acc@1: 40.862% [17835/43648]\n",
      "| Epoch [ 80/200] Iter[361/391]\t\tLoss: 3.1763 Acc@1: 40.485% [18707/46208]\n",
      "| Epoch [ 80/200] Iter[381/391]\t\tLoss: 3.3280 Acc@1: 40.542% [19771/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #80\n",
      "\n",
      "| Validation Epoch #80\t\t\tLoss: 1.8026 Acc@1: 57.75%\n",
      "\n",
      "----- Elapsed time : 0:23:27\n",
      "\n",
      "\n",
      "=> Training Epoch #81\n",
      "| Epoch [ 81/200] Iter[  1/391]\t\tLoss: 2.9563 Acc@1: 36.567% [ 46/128]\n",
      "| Epoch [ 81/200] Iter[ 21/391]\t\tLoss: 1.9258 Acc@1: 41.278% [1109/2688]\n",
      "| Epoch [ 81/200] Iter[ 41/391]\t\tLoss: 2.0538 Acc@1: 42.999% [2256/5248]\n",
      "| Epoch [ 81/200] Iter[ 61/391]\t\tLoss: 2.5447 Acc@1: 42.424% [3312/7808]\n",
      "| Epoch [ 81/200] Iter[ 81/391]\t\tLoss: 2.7188 Acc@1: 43.156% [4474/10368]\n",
      "| Epoch [ 81/200] Iter[101/391]\t\tLoss: 2.8947 Acc@1: 41.560% [5372/12928]\n",
      "| Epoch [ 81/200] Iter[121/391]\t\tLoss: 2.3585 Acc@1: 41.234% [6386/15488]\n",
      "| Epoch [ 81/200] Iter[141/391]\t\tLoss: 3.2962 Acc@1: 41.757% [7536/18048]\n",
      "| Epoch [ 81/200] Iter[161/391]\t\tLoss: 3.2201 Acc@1: 41.509% [8554/20608]\n",
      "| Epoch [ 81/200] Iter[181/391]\t\tLoss: 1.4845 Acc@1: 41.763% [9675/23168]\n",
      "| Epoch [ 81/200] Iter[201/391]\t\tLoss: 1.8494 Acc@1: 42.130% [10839/25728]\n",
      "| Epoch [ 81/200] Iter[221/391]\t\tLoss: 2.5491 Acc@1: 42.067% [11899/28288]\n",
      "| Epoch [ 81/200] Iter[241/391]\t\tLoss: 3.2161 Acc@1: 41.706% [12865/30848]\n",
      "| Epoch [ 81/200] Iter[261/391]\t\tLoss: 3.1324 Acc@1: 41.798% [13963/33408]\n",
      "| Epoch [ 81/200] Iter[281/391]\t\tLoss: 2.3394 Acc@1: 42.055% [15126/35968]\n",
      "| Epoch [ 81/200] Iter[301/391]\t\tLoss: 2.2340 Acc@1: 41.785% [16098/38528]\n",
      "| Epoch [ 81/200] Iter[321/391]\t\tLoss: 3.4686 Acc@1: 41.596% [17091/41088]\n",
      "| Epoch [ 81/200] Iter[341/391]\t\tLoss: 2.4529 Acc@1: 41.542% [18132/43648]\n",
      "| Epoch [ 81/200] Iter[361/391]\t\tLoss: 2.2322 Acc@1: 41.619% [19231/46208]\n",
      "| Epoch [ 81/200] Iter[381/391]\t\tLoss: 2.4615 Acc@1: 41.582% [20278/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #81\n",
      "\n",
      "| Validation Epoch #81\t\t\tLoss: 1.8355 Acc@1: 58.29%\n",
      "\n",
      "----- Elapsed time : 0:23:44\n",
      "\n",
      "\n",
      "=> Training Epoch #82\n",
      "| Epoch [ 82/200] Iter[  1/391]\t\tLoss: 3.4867 Acc@1: 21.586% [ 27/128]\n",
      "| Epoch [ 82/200] Iter[ 21/391]\t\tLoss: 2.2646 Acc@1: 44.756% [1203/2688]\n",
      "| Epoch [ 82/200] Iter[ 41/391]\t\tLoss: 2.9650 Acc@1: 42.224% [2215/5248]\n",
      "| Epoch [ 82/200] Iter[ 61/391]\t\tLoss: 2.0496 Acc@1: 43.236% [3375/7808]\n",
      "| Epoch [ 82/200] Iter[ 81/391]\t\tLoss: 3.3578 Acc@1: 43.159% [4474/10368]\n",
      "| Epoch [ 82/200] Iter[101/391]\t\tLoss: 3.3142 Acc@1: 42.630% [5511/12928]\n",
      "| Epoch [ 82/200] Iter[121/391]\t\tLoss: 1.9602 Acc@1: 42.622% [6601/15488]\n",
      "| Epoch [ 82/200] Iter[141/391]\t\tLoss: 3.2821 Acc@1: 43.112% [7780/18048]\n",
      "| Epoch [ 82/200] Iter[161/391]\t\tLoss: 1.9557 Acc@1: 42.816% [8823/20608]\n",
      "| Epoch [ 82/200] Iter[181/391]\t\tLoss: 3.0778 Acc@1: 42.528% [9852/23168]\n",
      "| Epoch [ 82/200] Iter[201/391]\t\tLoss: 3.1700 Acc@1: 42.366% [10899/25728]\n",
      "| Epoch [ 82/200] Iter[221/391]\t\tLoss: 3.4267 Acc@1: 42.570% [12042/28288]\n",
      "| Epoch [ 82/200] Iter[241/391]\t\tLoss: 3.1814 Acc@1: 42.070% [12977/30848]\n",
      "| Epoch [ 82/200] Iter[261/391]\t\tLoss: 1.9493 Acc@1: 42.289% [14127/33408]\n",
      "| Epoch [ 82/200] Iter[281/391]\t\tLoss: 2.6678 Acc@1: 42.104% [15144/35968]\n",
      "| Epoch [ 82/200] Iter[301/391]\t\tLoss: 3.1104 Acc@1: 42.322% [16305/38528]\n",
      "| Epoch [ 82/200] Iter[321/391]\t\tLoss: 3.4958 Acc@1: 42.396% [17419/41088]\n",
      "| Epoch [ 82/200] Iter[341/391]\t\tLoss: 1.4228 Acc@1: 42.448% [18527/43648]\n",
      "| Epoch [ 82/200] Iter[361/391]\t\tLoss: 1.6867 Acc@1: 42.292% [19542/46208]\n",
      "| Epoch [ 82/200] Iter[381/391]\t\tLoss: 2.4480 Acc@1: 42.185% [20572/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #82\n",
      "\n",
      "| Validation Epoch #82\t\t\tLoss: 1.8385 Acc@1: 58.67%\n",
      "\n",
      "----- Elapsed time : 0:24:01\n",
      "\n",
      "\n",
      "=> Training Epoch #83\n",
      "| Epoch [ 83/200] Iter[  1/391]\t\tLoss: 3.4499 Acc@1: 22.002% [ 28/128]\n",
      "| Epoch [ 83/200] Iter[ 21/391]\t\tLoss: 3.3042 Acc@1: 42.484% [1141/2688]\n",
      "| Epoch [ 83/200] Iter[ 41/391]\t\tLoss: 3.1838 Acc@1: 39.088% [2051/5248]\n",
      "| Epoch [ 83/200] Iter[ 61/391]\t\tLoss: 1.6257 Acc@1: 40.318% [3147/7808]\n",
      "| Epoch [ 83/200] Iter[ 81/391]\t\tLoss: 2.8362 Acc@1: 42.298% [4385/10368]\n",
      "| Epoch [ 83/200] Iter[101/391]\t\tLoss: 3.4518 Acc@1: 41.852% [5410/12928]\n",
      "| Epoch [ 83/200] Iter[121/391]\t\tLoss: 2.3275 Acc@1: 41.967% [6499/15488]\n",
      "| Epoch [ 83/200] Iter[141/391]\t\tLoss: 3.4940 Acc@1: 42.593% [7687/18048]\n",
      "| Epoch [ 83/200] Iter[161/391]\t\tLoss: 2.1977 Acc@1: 42.318% [8720/20608]\n",
      "| Epoch [ 83/200] Iter[181/391]\t\tLoss: 2.6481 Acc@1: 42.509% [9848/23168]\n",
      "| Epoch [ 83/200] Iter[201/391]\t\tLoss: 2.1413 Acc@1: 42.832% [11019/25728]\n",
      "| Epoch [ 83/200] Iter[221/391]\t\tLoss: 3.3496 Acc@1: 42.955% [12151/28288]\n",
      "| Epoch [ 83/200] Iter[241/391]\t\tLoss: 3.3294 Acc@1: 42.629% [13150/30848]\n",
      "| Epoch [ 83/200] Iter[261/391]\t\tLoss: 1.8670 Acc@1: 43.027% [14374/33408]\n",
      "| Epoch [ 83/200] Iter[281/391]\t\tLoss: 1.3756 Acc@1: 43.453% [15629/35968]\n",
      "| Epoch [ 83/200] Iter[301/391]\t\tLoss: 2.0143 Acc@1: 43.231% [16656/38528]\n",
      "| Epoch [ 83/200] Iter[321/391]\t\tLoss: 2.7094 Acc@1: 43.044% [17686/41088]\n",
      "| Epoch [ 83/200] Iter[341/391]\t\tLoss: 2.2252 Acc@1: 42.973% [18756/43648]\n",
      "| Epoch [ 83/200] Iter[361/391]\t\tLoss: 3.2724 Acc@1: 42.696% [19729/46208]\n",
      "| Epoch [ 83/200] Iter[381/391]\t\tLoss: 3.4964 Acc@1: 42.695% [20821/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #83\n",
      "\n",
      "| Validation Epoch #83\t\t\tLoss: 1.6510 Acc@1: 58.23%\n",
      "\n",
      "----- Elapsed time : 0:24:19\n",
      "\n",
      "\n",
      "=> Training Epoch #84\n",
      "| Epoch [ 84/200] Iter[  1/391]\t\tLoss: 1.7201 Acc@1: 60.211% [ 77/128]\n",
      "| Epoch [ 84/200] Iter[ 21/391]\t\tLoss: 1.8714 Acc@1: 44.513% [1196/2688]\n",
      "| Epoch [ 84/200] Iter[ 41/391]\t\tLoss: 2.4212 Acc@1: 43.762% [2296/5248]\n",
      "| Epoch [ 84/200] Iter[ 61/391]\t\tLoss: 3.2410 Acc@1: 42.016% [3280/7808]\n",
      "| Epoch [ 84/200] Iter[ 81/391]\t\tLoss: 2.3520 Acc@1: 41.956% [4350/10368]\n",
      "| Epoch [ 84/200] Iter[101/391]\t\tLoss: 2.0192 Acc@1: 42.574% [5503/12928]\n",
      "| Epoch [ 84/200] Iter[121/391]\t\tLoss: 1.7045 Acc@1: 42.531% [6587/15488]\n",
      "| Epoch [ 84/200] Iter[141/391]\t\tLoss: 2.1716 Acc@1: 43.035% [7766/18048]\n",
      "| Epoch [ 84/200] Iter[161/391]\t\tLoss: 1.6403 Acc@1: 43.286% [8920/20608]\n",
      "| Epoch [ 84/200] Iter[181/391]\t\tLoss: 1.7357 Acc@1: 43.045% [9972/23168]\n",
      "| Epoch [ 84/200] Iter[201/391]\t\tLoss: 2.7375 Acc@1: 42.501% [10934/25728]\n",
      "| Epoch [ 84/200] Iter[221/391]\t\tLoss: 2.0347 Acc@1: 42.453% [12008/28288]\n",
      "| Epoch [ 84/200] Iter[241/391]\t\tLoss: 3.2417 Acc@1: 42.525% [13118/30848]\n",
      "| Epoch [ 84/200] Iter[261/391]\t\tLoss: 2.9245 Acc@1: 42.194% [14096/33408]\n",
      "| Epoch [ 84/200] Iter[281/391]\t\tLoss: 3.3512 Acc@1: 41.913% [15075/35968]\n",
      "| Epoch [ 84/200] Iter[301/391]\t\tLoss: 2.4431 Acc@1: 41.741% [16081/38528]\n",
      "| Epoch [ 84/200] Iter[321/391]\t\tLoss: 2.4114 Acc@1: 42.001% [17257/41088]\n",
      "| Epoch [ 84/200] Iter[341/391]\t\tLoss: 2.8650 Acc@1: 42.030% [18345/43648]\n",
      "| Epoch [ 84/200] Iter[361/391]\t\tLoss: 2.8116 Acc@1: 41.986% [19400/46208]\n",
      "| Epoch [ 84/200] Iter[381/391]\t\tLoss: 1.6014 Acc@1: 42.201% [20580/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #84\n",
      "\n",
      "| Validation Epoch #84\t\t\tLoss: 1.6128 Acc@1: 58.33%\n",
      "\n",
      "----- Elapsed time : 0:24:36\n",
      "\n",
      "\n",
      "=> Training Epoch #85\n",
      "| Epoch [ 85/200] Iter[  1/391]\t\tLoss: 3.2419 Acc@1: 26.836% [ 34/128]\n",
      "| Epoch [ 85/200] Iter[ 21/391]\t\tLoss: 1.8901 Acc@1: 44.889% [1206/2688]\n",
      "| Epoch [ 85/200] Iter[ 41/391]\t\tLoss: 2.8041 Acc@1: 45.260% [2375/5248]\n",
      "| Epoch [ 85/200] Iter[ 61/391]\t\tLoss: 1.5895 Acc@1: 44.143% [3446/7808]\n",
      "| Epoch [ 85/200] Iter[ 81/391]\t\tLoss: 1.6333 Acc@1: 43.698% [4530/10368]\n",
      "| Epoch [ 85/200] Iter[101/391]\t\tLoss: 2.7503 Acc@1: 43.639% [5641/12928]\n",
      "| Epoch [ 85/200] Iter[121/391]\t\tLoss: 3.2580 Acc@1: 43.072% [6670/15488]\n",
      "| Epoch [ 85/200] Iter[141/391]\t\tLoss: 3.0738 Acc@1: 42.311% [7636/18048]\n",
      "| Epoch [ 85/200] Iter[161/391]\t\tLoss: 1.7178 Acc@1: 42.533% [8765/20608]\n",
      "| Epoch [ 85/200] Iter[181/391]\t\tLoss: 3.3331 Acc@1: 42.649% [9880/23168]\n",
      "| Epoch [ 85/200] Iter[201/391]\t\tLoss: 3.3877 Acc@1: 42.472% [10927/25728]\n",
      "| Epoch [ 85/200] Iter[221/391]\t\tLoss: 2.9473 Acc@1: 42.859% [12123/28288]\n",
      "| Epoch [ 85/200] Iter[241/391]\t\tLoss: 3.5171 Acc@1: 42.676% [13164/30848]\n",
      "| Epoch [ 85/200] Iter[261/391]\t\tLoss: 3.3492 Acc@1: 42.300% [14131/33408]\n",
      "| Epoch [ 85/200] Iter[281/391]\t\tLoss: 3.1507 Acc@1: 42.580% [15315/35968]\n",
      "| Epoch [ 85/200] Iter[301/391]\t\tLoss: 1.5669 Acc@1: 42.354% [16318/38528]\n",
      "| Epoch [ 85/200] Iter[321/391]\t\tLoss: 2.0958 Acc@1: 42.347% [17399/41088]\n",
      "| Epoch [ 85/200] Iter[341/391]\t\tLoss: 2.3257 Acc@1: 42.435% [18522/43648]\n",
      "| Epoch [ 85/200] Iter[361/391]\t\tLoss: 2.6918 Acc@1: 42.563% [19667/46208]\n",
      "| Epoch [ 85/200] Iter[381/391]\t\tLoss: 1.5117 Acc@1: 42.518% [20735/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #85\n",
      "\n",
      "| Validation Epoch #85\t\t\tLoss: 1.6658 Acc@1: 57.09%\n",
      "\n",
      "----- Elapsed time : 0:24:53\n",
      "\n",
      "\n",
      "=> Training Epoch #86\n",
      "| Epoch [ 86/200] Iter[  1/391]\t\tLoss: 1.9252 Acc@1: 61.824% [ 79/128]\n",
      "| Epoch [ 86/200] Iter[ 21/391]\t\tLoss: 3.2980 Acc@1: 38.833% [1043/2688]\n",
      "| Epoch [ 86/200] Iter[ 41/391]\t\tLoss: 2.5983 Acc@1: 41.069% [2155/5248]\n",
      "| Epoch [ 86/200] Iter[ 61/391]\t\tLoss: 3.0733 Acc@1: 40.208% [3139/7808]\n",
      "| Epoch [ 86/200] Iter[ 81/391]\t\tLoss: 2.9343 Acc@1: 39.881% [4134/10368]\n",
      "| Epoch [ 86/200] Iter[101/391]\t\tLoss: 3.2265 Acc@1: 40.783% [5272/12928]\n",
      "| Epoch [ 86/200] Iter[121/391]\t\tLoss: 3.2597 Acc@1: 40.631% [6292/15488]\n",
      "| Epoch [ 86/200] Iter[141/391]\t\tLoss: 1.6739 Acc@1: 40.670% [7340/18048]\n",
      "| Epoch [ 86/200] Iter[161/391]\t\tLoss: 2.8226 Acc@1: 41.103% [8470/20608]\n",
      "| Epoch [ 86/200] Iter[181/391]\t\tLoss: 2.8655 Acc@1: 40.696% [9428/23168]\n",
      "| Epoch [ 86/200] Iter[201/391]\t\tLoss: 3.1922 Acc@1: 40.926% [10529/25728]\n",
      "| Epoch [ 86/200] Iter[221/391]\t\tLoss: 1.8188 Acc@1: 41.249% [11668/28288]\n",
      "| Epoch [ 86/200] Iter[241/391]\t\tLoss: 1.7641 Acc@1: 41.788% [12890/30848]\n",
      "| Epoch [ 86/200] Iter[261/391]\t\tLoss: 1.6838 Acc@1: 41.956% [14016/33408]\n",
      "| Epoch [ 86/200] Iter[281/391]\t\tLoss: 3.3147 Acc@1: 41.567% [14950/35968]\n",
      "| Epoch [ 86/200] Iter[301/391]\t\tLoss: 2.6068 Acc@1: 41.791% [16101/38528]\n",
      "| Epoch [ 86/200] Iter[321/391]\t\tLoss: 3.3494 Acc@1: 41.676% [17123/41088]\n",
      "| Epoch [ 86/200] Iter[341/391]\t\tLoss: 3.2431 Acc@1: 41.421% [18079/43648]\n",
      "| Epoch [ 86/200] Iter[361/391]\t\tLoss: 3.2113 Acc@1: 41.248% [19059/46208]\n",
      "| Epoch [ 86/200] Iter[381/391]\t\tLoss: 2.8397 Acc@1: 41.028% [20008/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #86\n",
      "\n",
      "| Validation Epoch #86\t\t\tLoss: 1.8523 Acc@1: 56.91%\n",
      "\n",
      "----- Elapsed time : 0:25:10\n",
      "\n",
      "\n",
      "=> Training Epoch #87\n",
      "| Epoch [ 87/200] Iter[  1/391]\t\tLoss: 1.6537 Acc@1: 63.429% [ 81/128]\n",
      "| Epoch [ 87/200] Iter[ 21/391]\t\tLoss: 2.5995 Acc@1: 45.567% [1224/2688]\n",
      "| Epoch [ 87/200] Iter[ 41/391]\t\tLoss: 3.4938 Acc@1: 43.356% [2275/5248]\n",
      "| Epoch [ 87/200] Iter[ 61/391]\t\tLoss: 3.4548 Acc@1: 43.187% [3372/7808]\n",
      "| Epoch [ 87/200] Iter[ 81/391]\t\tLoss: 3.3684 Acc@1: 42.130% [4367/10368]\n",
      "| Epoch [ 87/200] Iter[101/391]\t\tLoss: 3.3908 Acc@1: 41.565% [5373/12928]\n",
      "| Epoch [ 87/200] Iter[121/391]\t\tLoss: 3.2285 Acc@1: 40.888% [6332/15488]\n",
      "| Epoch [ 87/200] Iter[141/391]\t\tLoss: 3.0192 Acc@1: 41.145% [7425/18048]\n",
      "| Epoch [ 87/200] Iter[161/391]\t\tLoss: 3.2136 Acc@1: 40.154% [8275/20608]\n",
      "| Epoch [ 87/200] Iter[181/391]\t\tLoss: 2.5069 Acc@1: 40.055% [9280/23168]\n",
      "| Epoch [ 87/200] Iter[201/391]\t\tLoss: 3.3522 Acc@1: 39.880% [10260/25728]\n",
      "| Epoch [ 87/200] Iter[221/391]\t\tLoss: 3.2443 Acc@1: 39.830% [11267/28288]\n",
      "| Epoch [ 87/200] Iter[241/391]\t\tLoss: 3.3756 Acc@1: 40.231% [12410/30848]\n",
      "| Epoch [ 87/200] Iter[261/391]\t\tLoss: 1.5888 Acc@1: 40.466% [13518/33408]\n",
      "| Epoch [ 87/200] Iter[281/391]\t\tLoss: 3.1793 Acc@1: 40.500% [14567/35968]\n",
      "| Epoch [ 87/200] Iter[301/391]\t\tLoss: 2.7762 Acc@1: 40.524% [15613/38528]\n",
      "| Epoch [ 87/200] Iter[321/391]\t\tLoss: 1.5349 Acc@1: 40.433% [16613/41088]\n",
      "| Epoch [ 87/200] Iter[341/391]\t\tLoss: 2.4491 Acc@1: 40.684% [17757/43648]\n",
      "| Epoch [ 87/200] Iter[361/391]\t\tLoss: 2.1966 Acc@1: 40.604% [18762/46208]\n",
      "| Epoch [ 87/200] Iter[381/391]\t\tLoss: 1.7870 Acc@1: 40.646% [19822/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #87\n",
      "\n",
      "| Validation Epoch #87\t\t\tLoss: 1.6442 Acc@1: 58.78%\n",
      "\n",
      "----- Elapsed time : 0:25:28\n",
      "\n",
      "\n",
      "=> Training Epoch #88\n",
      "| Epoch [ 88/200] Iter[  1/391]\t\tLoss: 2.8688 Acc@1: 42.493% [ 54/128]\n",
      "| Epoch [ 88/200] Iter[ 21/391]\t\tLoss: 3.2979 Acc@1: 38.695% [1040/2688]\n",
      "| Epoch [ 88/200] Iter[ 41/391]\t\tLoss: 2.4179 Acc@1: 38.747% [2033/5248]\n",
      "| Epoch [ 88/200] Iter[ 61/391]\t\tLoss: 1.3766 Acc@1: 40.542% [3165/7808]\n",
      "| Epoch [ 88/200] Iter[ 81/391]\t\tLoss: 2.9406 Acc@1: 40.868% [4237/10368]\n",
      "| Epoch [ 88/200] Iter[101/391]\t\tLoss: 2.0588 Acc@1: 41.206% [5327/12928]\n",
      "| Epoch [ 88/200] Iter[121/391]\t\tLoss: 2.8977 Acc@1: 41.499% [6427/15488]\n",
      "| Epoch [ 88/200] Iter[141/391]\t\tLoss: 3.0546 Acc@1: 41.962% [7573/18048]\n",
      "| Epoch [ 88/200] Iter[161/391]\t\tLoss: 3.2975 Acc@1: 42.110% [8677/20608]\n",
      "| Epoch [ 88/200] Iter[181/391]\t\tLoss: 2.4208 Acc@1: 42.512% [9849/23168]\n",
      "| Epoch [ 88/200] Iter[201/391]\t\tLoss: 2.9384 Acc@1: 42.269% [10874/25728]\n",
      "| Epoch [ 88/200] Iter[221/391]\t\tLoss: 1.3329 Acc@1: 42.227% [11945/28288]\n",
      "| Epoch [ 88/200] Iter[241/391]\t\tLoss: 3.3689 Acc@1: 41.927% [12933/30848]\n",
      "| Epoch [ 88/200] Iter[261/391]\t\tLoss: 3.2914 Acc@1: 41.900% [13997/33408]\n",
      "| Epoch [ 88/200] Iter[281/391]\t\tLoss: 2.8911 Acc@1: 41.848% [15052/35968]\n",
      "| Epoch [ 88/200] Iter[301/391]\t\tLoss: 2.7799 Acc@1: 41.997% [16180/38528]\n",
      "| Epoch [ 88/200] Iter[321/391]\t\tLoss: 1.7233 Acc@1: 42.322% [17389/41088]\n",
      "| Epoch [ 88/200] Iter[341/391]\t\tLoss: 2.7016 Acc@1: 42.368% [18492/43648]\n",
      "| Epoch [ 88/200] Iter[361/391]\t\tLoss: 3.3108 Acc@1: 42.208% [19503/46208]\n",
      "| Epoch [ 88/200] Iter[381/391]\t\tLoss: 1.6269 Acc@1: 42.247% [20603/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #88\n",
      "\n",
      "| Validation Epoch #88\t\t\tLoss: 1.5195 Acc@1: 56.11%\n",
      "\n",
      "----- Elapsed time : 0:25:45\n",
      "\n",
      "\n",
      "=> Training Epoch #89\n",
      "| Epoch [ 89/200] Iter[  1/391]\t\tLoss: 3.1032 Acc@1: 31.334% [ 40/128]\n",
      "| Epoch [ 89/200] Iter[ 21/391]\t\tLoss: 3.3030 Acc@1: 45.348% [1218/2688]\n",
      "| Epoch [ 89/200] Iter[ 41/391]\t\tLoss: 1.6151 Acc@1: 44.734% [2347/5248]\n",
      "| Epoch [ 89/200] Iter[ 61/391]\t\tLoss: 3.4105 Acc@1: 43.888% [3426/7808]\n",
      "| Epoch [ 89/200] Iter[ 81/391]\t\tLoss: 1.5730 Acc@1: 43.792% [4540/10368]\n",
      "| Epoch [ 89/200] Iter[101/391]\t\tLoss: 2.1977 Acc@1: 43.853% [5669/12928]\n",
      "| Epoch [ 89/200] Iter[121/391]\t\tLoss: 3.2478 Acc@1: 44.077% [6826/15488]\n",
      "| Epoch [ 89/200] Iter[141/391]\t\tLoss: 2.1434 Acc@1: 44.338% [8002/18048]\n",
      "| Epoch [ 89/200] Iter[161/391]\t\tLoss: 3.2114 Acc@1: 43.670% [8999/20608]\n",
      "| Epoch [ 89/200] Iter[181/391]\t\tLoss: 1.6958 Acc@1: 43.646% [10111/23168]\n",
      "| Epoch [ 89/200] Iter[201/391]\t\tLoss: 1.8067 Acc@1: 43.414% [11169/25728]\n",
      "| Epoch [ 89/200] Iter[221/391]\t\tLoss: 3.2065 Acc@1: 43.532% [12314/28288]\n",
      "| Epoch [ 89/200] Iter[241/391]\t\tLoss: 3.2013 Acc@1: 42.958% [13251/30848]\n",
      "| Epoch [ 89/200] Iter[261/391]\t\tLoss: 2.7093 Acc@1: 42.571% [14222/33408]\n",
      "| Epoch [ 89/200] Iter[281/391]\t\tLoss: 3.3188 Acc@1: 42.436% [15263/35968]\n",
      "| Epoch [ 89/200] Iter[301/391]\t\tLoss: 3.4791 Acc@1: 42.802% [16490/38528]\n",
      "| Epoch [ 89/200] Iter[321/391]\t\tLoss: 3.3781 Acc@1: 42.535% [17476/41088]\n",
      "| Epoch [ 89/200] Iter[341/391]\t\tLoss: 3.4370 Acc@1: 42.431% [18520/43648]\n",
      "| Epoch [ 89/200] Iter[361/391]\t\tLoss: 1.7006 Acc@1: 42.320% [19555/46208]\n",
      "| Epoch [ 89/200] Iter[381/391]\t\tLoss: 1.8657 Acc@1: 42.466% [20709/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #89\n",
      "\n",
      "| Validation Epoch #89\t\t\tLoss: 1.3978 Acc@1: 58.68%\n",
      "\n",
      "----- Elapsed time : 0:26:02\n",
      "\n",
      "\n",
      "=> Training Epoch #90\n",
      "| Epoch [ 90/200] Iter[  1/391]\t\tLoss: 2.9200 Acc@1: 34.859% [ 44/128]\n",
      "| Epoch [ 90/200] Iter[ 21/391]\t\tLoss: 2.9908 Acc@1: 39.714% [1067/2688]\n",
      "| Epoch [ 90/200] Iter[ 41/391]\t\tLoss: 1.5424 Acc@1: 40.808% [2141/5248]\n",
      "| Epoch [ 90/200] Iter[ 61/391]\t\tLoss: 3.3758 Acc@1: 41.001% [3201/7808]\n",
      "| Epoch [ 90/200] Iter[ 81/391]\t\tLoss: 3.3810 Acc@1: 42.217% [4377/10368]\n",
      "| Epoch [ 90/200] Iter[101/391]\t\tLoss: 2.2606 Acc@1: 42.781% [5530/12928]\n",
      "| Epoch [ 90/200] Iter[121/391]\t\tLoss: 3.2751 Acc@1: 42.471% [6577/15488]\n",
      "| Epoch [ 90/200] Iter[141/391]\t\tLoss: 2.9732 Acc@1: 42.349% [7643/18048]\n",
      "| Epoch [ 90/200] Iter[161/391]\t\tLoss: 3.2843 Acc@1: 41.701% [8593/20608]\n",
      "| Epoch [ 90/200] Iter[181/391]\t\tLoss: 2.7263 Acc@1: 41.859% [9697/23168]\n",
      "| Epoch [ 90/200] Iter[201/391]\t\tLoss: 1.4553 Acc@1: 41.409% [10653/25728]\n",
      "| Epoch [ 90/200] Iter[221/391]\t\tLoss: 2.5782 Acc@1: 41.596% [11766/28288]\n",
      "| Epoch [ 90/200] Iter[241/391]\t\tLoss: 3.0546 Acc@1: 41.355% [12757/30848]\n",
      "| Epoch [ 90/200] Iter[261/391]\t\tLoss: 1.8102 Acc@1: 41.706% [13933/33408]\n",
      "| Epoch [ 90/200] Iter[281/391]\t\tLoss: 2.6138 Acc@1: 41.648% [14979/35968]\n",
      "| Epoch [ 90/200] Iter[301/391]\t\tLoss: 1.6913 Acc@1: 41.581% [16020/38528]\n",
      "| Epoch [ 90/200] Iter[321/391]\t\tLoss: 2.1446 Acc@1: 41.680% [17125/41088]\n",
      "| Epoch [ 90/200] Iter[341/391]\t\tLoss: 3.0966 Acc@1: 41.936% [18304/43648]\n",
      "| Epoch [ 90/200] Iter[361/391]\t\tLoss: 3.0232 Acc@1: 42.151% [19477/46208]\n",
      "| Epoch [ 90/200] Iter[381/391]\t\tLoss: 3.1161 Acc@1: 42.138% [20550/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #90\n",
      "\n",
      "| Validation Epoch #90\t\t\tLoss: 1.8372 Acc@1: 57.63%\n",
      "\n",
      "----- Elapsed time : 0:26:19\n",
      "\n",
      "\n",
      "=> Training Epoch #91\n",
      "| Epoch [ 91/200] Iter[  1/391]\t\tLoss: 3.0819 Acc@1: 28.597% [ 36/128]\n",
      "| Epoch [ 91/200] Iter[ 21/391]\t\tLoss: 3.3440 Acc@1: 44.231% [1188/2688]\n",
      "| Epoch [ 91/200] Iter[ 41/391]\t\tLoss: 3.2011 Acc@1: 42.969% [2255/5248]\n",
      "| Epoch [ 91/200] Iter[ 61/391]\t\tLoss: 2.4296 Acc@1: 43.393% [3388/7808]\n",
      "| Epoch [ 91/200] Iter[ 81/391]\t\tLoss: 1.5681 Acc@1: 43.614% [4521/10368]\n",
      "| Epoch [ 91/200] Iter[101/391]\t\tLoss: 2.9228 Acc@1: 43.673% [5646/12928]\n",
      "| Epoch [ 91/200] Iter[121/391]\t\tLoss: 2.1633 Acc@1: 43.930% [6803/15488]\n",
      "| Epoch [ 91/200] Iter[141/391]\t\tLoss: 1.9030 Acc@1: 43.608% [7870/18048]\n",
      "| Epoch [ 91/200] Iter[161/391]\t\tLoss: 1.6277 Acc@1: 43.808% [9027/20608]\n",
      "| Epoch [ 91/200] Iter[181/391]\t\tLoss: 3.1095 Acc@1: 43.917% [10174/23168]\n",
      "| Epoch [ 91/200] Iter[201/391]\t\tLoss: 3.1982 Acc@1: 44.073% [11339/25728]\n",
      "| Epoch [ 91/200] Iter[221/391]\t\tLoss: 3.3461 Acc@1: 43.791% [12387/28288]\n",
      "| Epoch [ 91/200] Iter[241/391]\t\tLoss: 2.7414 Acc@1: 44.336% [13676/30848]\n",
      "| Epoch [ 91/200] Iter[261/391]\t\tLoss: 3.3508 Acc@1: 44.278% [14792/33408]\n",
      "| Epoch [ 91/200] Iter[281/391]\t\tLoss: 2.8705 Acc@1: 44.126% [15871/35968]\n",
      "| Epoch [ 91/200] Iter[301/391]\t\tLoss: 1.6423 Acc@1: 44.006% [16954/38528]\n",
      "| Epoch [ 91/200] Iter[321/391]\t\tLoss: 2.6918 Acc@1: 44.014% [18084/41088]\n",
      "| Epoch [ 91/200] Iter[341/391]\t\tLoss: 3.1564 Acc@1: 43.957% [19186/43648]\n",
      "| Epoch [ 91/200] Iter[361/391]\t\tLoss: 2.9998 Acc@1: 43.700% [20192/46208]\n",
      "| Epoch [ 91/200] Iter[381/391]\t\tLoss: 1.7825 Acc@1: 43.438% [21183/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #91\n",
      "\n",
      "| Validation Epoch #91\t\t\tLoss: 1.5599 Acc@1: 58.48%\n",
      "\n",
      "----- Elapsed time : 0:26:37\n",
      "\n",
      "\n",
      "=> Training Epoch #92\n",
      "| Epoch [ 92/200] Iter[  1/391]\t\tLoss: 3.4552 Acc@1: 21.202% [ 27/128]\n",
      "| Epoch [ 92/200] Iter[ 21/391]\t\tLoss: 3.2922 Acc@1: 43.658% [1173/2688]\n",
      "| Epoch [ 92/200] Iter[ 41/391]\t\tLoss: 3.2515 Acc@1: 42.960% [2254/5248]\n",
      "| Epoch [ 92/200] Iter[ 61/391]\t\tLoss: 2.3121 Acc@1: 43.030% [3359/7808]\n",
      "| Epoch [ 92/200] Iter[ 81/391]\t\tLoss: 2.1542 Acc@1: 42.142% [4369/10368]\n",
      "| Epoch [ 92/200] Iter[101/391]\t\tLoss: 3.2747 Acc@1: 41.182% [5324/12928]\n",
      "| Epoch [ 92/200] Iter[121/391]\t\tLoss: 2.8402 Acc@1: 39.903% [6180/15488]\n",
      "| Epoch [ 92/200] Iter[141/391]\t\tLoss: 1.3522 Acc@1: 40.286% [7270/18048]\n",
      "| Epoch [ 92/200] Iter[161/391]\t\tLoss: 1.5236 Acc@1: 40.219% [8288/20608]\n",
      "| Epoch [ 92/200] Iter[181/391]\t\tLoss: 3.1636 Acc@1: 40.851% [9464/23168]\n",
      "| Epoch [ 92/200] Iter[201/391]\t\tLoss: 2.1221 Acc@1: 40.988% [10545/25728]\n",
      "| Epoch [ 92/200] Iter[221/391]\t\tLoss: 3.3780 Acc@1: 41.397% [11710/28288]\n",
      "| Epoch [ 92/200] Iter[241/391]\t\tLoss: 2.1539 Acc@1: 41.621% [12839/30848]\n",
      "| Epoch [ 92/200] Iter[261/391]\t\tLoss: 1.4368 Acc@1: 42.040% [14044/33408]\n",
      "| Epoch [ 92/200] Iter[281/391]\t\tLoss: 2.4676 Acc@1: 42.190% [15175/35968]\n",
      "| Epoch [ 92/200] Iter[301/391]\t\tLoss: 3.3579 Acc@1: 41.866% [16130/38528]\n",
      "| Epoch [ 92/200] Iter[321/391]\t\tLoss: 2.7065 Acc@1: 41.736% [17148/41088]\n",
      "| Epoch [ 92/200] Iter[341/391]\t\tLoss: 3.3034 Acc@1: 41.582% [18149/43648]\n",
      "| Epoch [ 92/200] Iter[361/391]\t\tLoss: 2.4844 Acc@1: 41.618% [19230/46208]\n",
      "| Epoch [ 92/200] Iter[381/391]\t\tLoss: 2.6838 Acc@1: 41.631% [20302/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #92\n",
      "\n",
      "| Validation Epoch #92\t\t\tLoss: 1.4724 Acc@1: 57.84%\n",
      "\n",
      "----- Elapsed time : 0:26:54\n",
      "\n",
      "\n",
      "=> Training Epoch #93\n",
      "| Epoch [ 93/200] Iter[  1/391]\t\tLoss: 2.6550 Acc@1: 47.264% [ 60/128]\n",
      "| Epoch [ 93/200] Iter[ 21/391]\t\tLoss: 3.3270 Acc@1: 43.626% [1172/2688]\n",
      "| Epoch [ 93/200] Iter[ 41/391]\t\tLoss: 3.2403 Acc@1: 42.055% [2207/5248]\n",
      "| Epoch [ 93/200] Iter[ 61/391]\t\tLoss: 3.3285 Acc@1: 43.361% [3385/7808]\n",
      "| Epoch [ 93/200] Iter[ 81/391]\t\tLoss: 2.8888 Acc@1: 43.999% [4561/10368]\n",
      "| Epoch [ 93/200] Iter[101/391]\t\tLoss: 2.8942 Acc@1: 43.437% [5615/12928]\n",
      "| Epoch [ 93/200] Iter[121/391]\t\tLoss: 3.2761 Acc@1: 43.217% [6693/15488]\n",
      "| Epoch [ 93/200] Iter[141/391]\t\tLoss: 3.3387 Acc@1: 43.695% [7886/18048]\n",
      "| Epoch [ 93/200] Iter[161/391]\t\tLoss: 2.2650 Acc@1: 43.822% [9030/20608]\n",
      "| Epoch [ 93/200] Iter[181/391]\t\tLoss: 3.2864 Acc@1: 43.772% [10141/23168]\n",
      "| Epoch [ 93/200] Iter[201/391]\t\tLoss: 2.4331 Acc@1: 44.053% [11333/25728]\n",
      "| Epoch [ 93/200] Iter[221/391]\t\tLoss: 2.2732 Acc@1: 44.281% [12526/28288]\n",
      "| Epoch [ 93/200] Iter[241/391]\t\tLoss: 2.6629 Acc@1: 43.850% [13526/30848]\n",
      "| Epoch [ 93/200] Iter[261/391]\t\tLoss: 1.3559 Acc@1: 43.661% [14586/33408]\n",
      "| Epoch [ 93/200] Iter[281/391]\t\tLoss: 1.6031 Acc@1: 43.677% [15709/35968]\n",
      "| Epoch [ 93/200] Iter[301/391]\t\tLoss: 2.7057 Acc@1: 43.420% [16728/38528]\n",
      "| Epoch [ 93/200] Iter[321/391]\t\tLoss: 3.1872 Acc@1: 43.332% [17804/41088]\n",
      "| Epoch [ 93/200] Iter[341/391]\t\tLoss: 3.1802 Acc@1: 43.204% [18857/43648]\n",
      "| Epoch [ 93/200] Iter[361/391]\t\tLoss: 2.6517 Acc@1: 43.220% [19971/46208]\n",
      "| Epoch [ 93/200] Iter[381/391]\t\tLoss: 3.2304 Acc@1: 43.483% [21205/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #93\n",
      "\n",
      "| Validation Epoch #93\t\t\tLoss: 1.6951 Acc@1: 59.13%\n",
      "\n",
      "----- Elapsed time : 0:27:12\n",
      "\n",
      "\n",
      "=> Training Epoch #94\n",
      "| Epoch [ 94/200] Iter[  1/391]\t\tLoss: 3.1780 Acc@1: 33.244% [ 42/128]\n",
      "| Epoch [ 94/200] Iter[ 21/391]\t\tLoss: 3.1034 Acc@1: 45.146% [1213/2688]\n",
      "| Epoch [ 94/200] Iter[ 41/391]\t\tLoss: 1.5944 Acc@1: 46.361% [2433/5248]\n",
      "| Epoch [ 94/200] Iter[ 61/391]\t\tLoss: 3.4620 Acc@1: 44.629% [3484/7808]\n",
      "| Epoch [ 94/200] Iter[ 81/391]\t\tLoss: 1.9006 Acc@1: 43.694% [4530/10368]\n",
      "| Epoch [ 94/200] Iter[101/391]\t\tLoss: 1.6654 Acc@1: 44.244% [5719/12928]\n",
      "| Epoch [ 94/200] Iter[121/391]\t\tLoss: 2.4602 Acc@1: 44.603% [6908/15488]\n",
      "| Epoch [ 94/200] Iter[141/391]\t\tLoss: 1.7646 Acc@1: 44.731% [8072/18048]\n",
      "| Epoch [ 94/200] Iter[161/391]\t\tLoss: 3.3552 Acc@1: 44.604% [9192/20608]\n",
      "| Epoch [ 94/200] Iter[181/391]\t\tLoss: 2.4527 Acc@1: 44.654% [10345/23168]\n",
      "| Epoch [ 94/200] Iter[201/391]\t\tLoss: 3.2775 Acc@1: 44.544% [11460/25728]\n",
      "| Epoch [ 94/200] Iter[221/391]\t\tLoss: 3.3997 Acc@1: 44.528% [12596/28288]\n",
      "| Epoch [ 94/200] Iter[241/391]\t\tLoss: 2.1656 Acc@1: 44.489% [13723/30848]\n",
      "| Epoch [ 94/200] Iter[261/391]\t\tLoss: 3.1463 Acc@1: 44.326% [14808/33408]\n",
      "| Epoch [ 94/200] Iter[281/391]\t\tLoss: 2.9688 Acc@1: 44.049% [15843/35968]\n",
      "| Epoch [ 94/200] Iter[301/391]\t\tLoss: 2.8772 Acc@1: 44.144% [17007/38528]\n",
      "| Epoch [ 94/200] Iter[321/391]\t\tLoss: 1.8085 Acc@1: 44.191% [18157/41088]\n",
      "| Epoch [ 94/200] Iter[341/391]\t\tLoss: 2.9907 Acc@1: 43.964% [19189/43648]\n",
      "| Epoch [ 94/200] Iter[361/391]\t\tLoss: 3.3743 Acc@1: 44.007% [20334/46208]\n",
      "| Epoch [ 94/200] Iter[381/391]\t\tLoss: 1.4359 Acc@1: 43.943% [21429/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #94\n",
      "\n",
      "| Validation Epoch #94\t\t\tLoss: 1.3689 Acc@1: 58.22%\n",
      "\n",
      "----- Elapsed time : 0:27:30\n",
      "\n",
      "\n",
      "=> Training Epoch #95\n",
      "| Epoch [ 95/200] Iter[  1/391]\t\tLoss: 2.0869 Acc@1: 55.319% [ 70/128]\n",
      "| Epoch [ 95/200] Iter[ 21/391]\t\tLoss: 1.6872 Acc@1: 45.580% [1225/2688]\n",
      "| Epoch [ 95/200] Iter[ 41/391]\t\tLoss: 3.3437 Acc@1: 45.867% [2407/5248]\n",
      "| Epoch [ 95/200] Iter[ 61/391]\t\tLoss: 3.3004 Acc@1: 45.109% [3522/7808]\n",
      "| Epoch [ 95/200] Iter[ 81/391]\t\tLoss: 2.2432 Acc@1: 45.236% [4690/10368]\n",
      "| Epoch [ 95/200] Iter[101/391]\t\tLoss: 2.5271 Acc@1: 45.102% [5830/12928]\n",
      "| Epoch [ 95/200] Iter[121/391]\t\tLoss: 3.1125 Acc@1: 44.312% [6863/15488]\n",
      "| Epoch [ 95/200] Iter[141/391]\t\tLoss: 2.2939 Acc@1: 43.838% [7911/18048]\n",
      "| Epoch [ 95/200] Iter[161/391]\t\tLoss: 2.9559 Acc@1: 43.232% [8909/20608]\n",
      "| Epoch [ 95/200] Iter[181/391]\t\tLoss: 3.3083 Acc@1: 43.801% [10147/23168]\n",
      "| Epoch [ 95/200] Iter[201/391]\t\tLoss: 2.2476 Acc@1: 43.618% [11221/25728]\n",
      "| Epoch [ 95/200] Iter[221/391]\t\tLoss: 2.9816 Acc@1: 43.396% [12275/28288]\n",
      "| Epoch [ 95/200] Iter[241/391]\t\tLoss: 2.8655 Acc@1: 43.512% [13422/30848]\n",
      "| Epoch [ 95/200] Iter[261/391]\t\tLoss: 3.2369 Acc@1: 43.648% [14581/33408]\n",
      "| Epoch [ 95/200] Iter[281/391]\t\tLoss: 2.8840 Acc@1: 43.621% [15689/35968]\n",
      "| Epoch [ 95/200] Iter[301/391]\t\tLoss: 3.2399 Acc@1: 43.088% [16600/38528]\n",
      "| Epoch [ 95/200] Iter[321/391]\t\tLoss: 1.9565 Acc@1: 43.033% [17681/41088]\n",
      "| Epoch [ 95/200] Iter[341/391]\t\tLoss: 2.7629 Acc@1: 43.045% [18788/43648]\n",
      "| Epoch [ 95/200] Iter[361/391]\t\tLoss: 3.2449 Acc@1: 43.131% [19929/46208]\n",
      "| Epoch [ 95/200] Iter[381/391]\t\tLoss: 2.7658 Acc@1: 43.328% [21130/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #95\n",
      "\n",
      "| Validation Epoch #95\t\t\tLoss: 1.5542 Acc@1: 58.57%\n",
      "\n",
      "----- Elapsed time : 0:27:48\n",
      "\n",
      "\n",
      "=> Training Epoch #96\n",
      "| Epoch [ 96/200] Iter[  1/391]\t\tLoss: 2.4847 Acc@1: 49.414% [ 63/128]\n",
      "| Epoch [ 96/200] Iter[ 21/391]\t\tLoss: 1.9103 Acc@1: 45.717% [1228/2688]\n",
      "| Epoch [ 96/200] Iter[ 41/391]\t\tLoss: 3.1218 Acc@1: 44.506% [2335/5248]\n",
      "| Epoch [ 96/200] Iter[ 61/391]\t\tLoss: 3.1782 Acc@1: 44.380% [3465/7808]\n",
      "| Epoch [ 96/200] Iter[ 81/391]\t\tLoss: 3.0439 Acc@1: 44.091% [4571/10368]\n",
      "| Epoch [ 96/200] Iter[101/391]\t\tLoss: 3.3357 Acc@1: 43.359% [5605/12928]\n",
      "| Epoch [ 96/200] Iter[121/391]\t\tLoss: 2.9099 Acc@1: 42.365% [6561/15488]\n",
      "| Epoch [ 96/200] Iter[141/391]\t\tLoss: 2.4892 Acc@1: 43.138% [7785/18048]\n",
      "| Epoch [ 96/200] Iter[161/391]\t\tLoss: 3.0914 Acc@1: 43.298% [8922/20608]\n",
      "| Epoch [ 96/200] Iter[181/391]\t\tLoss: 1.5546 Acc@1: 43.151% [9997/23168]\n",
      "| Epoch [ 96/200] Iter[201/391]\t\tLoss: 2.8378 Acc@1: 42.911% [11040/25728]\n",
      "| Epoch [ 96/200] Iter[221/391]\t\tLoss: 2.8477 Acc@1: 42.887% [12131/28288]\n",
      "| Epoch [ 96/200] Iter[241/391]\t\tLoss: 3.2618 Acc@1: 42.754% [13188/30848]\n",
      "| Epoch [ 96/200] Iter[261/391]\t\tLoss: 2.2889 Acc@1: 42.927% [14341/33408]\n",
      "| Epoch [ 96/200] Iter[281/391]\t\tLoss: 3.2983 Acc@1: 42.933% [15442/35968]\n",
      "| Epoch [ 96/200] Iter[301/391]\t\tLoss: 3.1247 Acc@1: 43.027% [16577/38528]\n",
      "| Epoch [ 96/200] Iter[321/391]\t\tLoss: 3.4344 Acc@1: 42.824% [17595/41088]\n",
      "| Epoch [ 96/200] Iter[341/391]\t\tLoss: 3.1284 Acc@1: 42.626% [18605/43648]\n",
      "| Epoch [ 96/200] Iter[361/391]\t\tLoss: 2.9921 Acc@1: 42.814% [19783/46208]\n",
      "| Epoch [ 96/200] Iter[381/391]\t\tLoss: 3.1734 Acc@1: 42.910% [20926/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #96\n",
      "\n",
      "| Validation Epoch #96\t\t\tLoss: 1.5496 Acc@1: 58.17%\n",
      "\n",
      "----- Elapsed time : 0:28:05\n",
      "\n",
      "\n",
      "=> Training Epoch #97\n",
      "| Epoch [ 97/200] Iter[  1/391]\t\tLoss: 3.2351 Acc@1: 25.230% [ 32/128]\n",
      "| Epoch [ 97/200] Iter[ 21/391]\t\tLoss: 2.7682 Acc@1: 45.460% [1221/2688]\n",
      "| Epoch [ 97/200] Iter[ 41/391]\t\tLoss: 3.4395 Acc@1: 43.671% [2291/5248]\n",
      "| Epoch [ 97/200] Iter[ 61/391]\t\tLoss: 3.2029 Acc@1: 43.439% [3391/7808]\n",
      "| Epoch [ 97/200] Iter[ 81/391]\t\tLoss: 3.2562 Acc@1: 42.950% [4453/10368]\n",
      "| Epoch [ 97/200] Iter[101/391]\t\tLoss: 1.7833 Acc@1: 43.990% [5687/12928]\n",
      "| Epoch [ 97/200] Iter[121/391]\t\tLoss: 2.8507 Acc@1: 44.486% [6890/15488]\n",
      "| Epoch [ 97/200] Iter[141/391]\t\tLoss: 2.6250 Acc@1: 43.917% [7926/18048]\n",
      "| Epoch [ 97/200] Iter[161/391]\t\tLoss: 1.9902 Acc@1: 43.688% [9003/20608]\n",
      "| Epoch [ 97/200] Iter[181/391]\t\tLoss: 3.4407 Acc@1: 43.414% [10058/23168]\n",
      "| Epoch [ 97/200] Iter[201/391]\t\tLoss: 2.1615 Acc@1: 43.405% [11167/25728]\n",
      "| Epoch [ 97/200] Iter[221/391]\t\tLoss: 3.4931 Acc@1: 43.006% [12165/28288]\n",
      "| Epoch [ 97/200] Iter[241/391]\t\tLoss: 1.5488 Acc@1: 42.965% [13253/30848]\n",
      "| Epoch [ 97/200] Iter[261/391]\t\tLoss: 1.9799 Acc@1: 43.000% [14365/33408]\n",
      "| Epoch [ 97/200] Iter[281/391]\t\tLoss: 3.3896 Acc@1: 42.800% [15394/35968]\n",
      "| Epoch [ 97/200] Iter[301/391]\t\tLoss: 3.2312 Acc@1: 42.951% [16548/38528]\n",
      "| Epoch [ 97/200] Iter[321/391]\t\tLoss: 1.5047 Acc@1: 43.061% [17692/41088]\n",
      "| Epoch [ 97/200] Iter[341/391]\t\tLoss: 2.7723 Acc@1: 43.289% [18894/43648]\n",
      "| Epoch [ 97/200] Iter[361/391]\t\tLoss: 3.0562 Acc@1: 43.207% [19965/46208]\n",
      "| Epoch [ 97/200] Iter[381/391]\t\tLoss: 2.6144 Acc@1: 42.946% [20943/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #97\n",
      "\n",
      "| Validation Epoch #97\t\t\tLoss: 1.2974 Acc@1: 59.12%\n",
      "\n",
      "----- Elapsed time : 0:28:23\n",
      "\n",
      "\n",
      "=> Training Epoch #98\n",
      "| Epoch [ 98/200] Iter[  1/391]\t\tLoss: 3.3280 Acc@1: 22.512% [ 28/128]\n",
      "| Epoch [ 98/200] Iter[ 21/391]\t\tLoss: 2.8193 Acc@1: 38.609% [1037/2688]\n",
      "| Epoch [ 98/200] Iter[ 41/391]\t\tLoss: 1.8018 Acc@1: 40.765% [2139/5248]\n",
      "| Epoch [ 98/200] Iter[ 61/391]\t\tLoss: 2.1462 Acc@1: 40.614% [3171/7808]\n",
      "| Epoch [ 98/200] Iter[ 81/391]\t\tLoss: 2.8349 Acc@1: 41.713% [4324/10368]\n",
      "| Epoch [ 98/200] Iter[101/391]\t\tLoss: 1.5815 Acc@1: 43.043% [5564/12928]\n",
      "| Epoch [ 98/200] Iter[121/391]\t\tLoss: 2.2544 Acc@1: 43.182% [6687/15488]\n",
      "| Epoch [ 98/200] Iter[141/391]\t\tLoss: 1.9844 Acc@1: 42.401% [7652/18048]\n",
      "| Epoch [ 98/200] Iter[161/391]\t\tLoss: 3.1457 Acc@1: 42.462% [8750/20608]\n",
      "| Epoch [ 98/200] Iter[181/391]\t\tLoss: 3.3123 Acc@1: 43.027% [9968/23168]\n",
      "| Epoch [ 98/200] Iter[201/391]\t\tLoss: 3.4210 Acc@1: 43.055% [11077/25728]\n",
      "| Epoch [ 98/200] Iter[221/391]\t\tLoss: 3.3190 Acc@1: 42.794% [12105/28288]\n",
      "| Epoch [ 98/200] Iter[241/391]\t\tLoss: 2.8003 Acc@1: 42.231% [13027/30848]\n",
      "| Epoch [ 98/200] Iter[261/391]\t\tLoss: 3.2372 Acc@1: 42.561% [14218/33408]\n",
      "| Epoch [ 98/200] Iter[281/391]\t\tLoss: 2.0252 Acc@1: 42.692% [15355/35968]\n",
      "| Epoch [ 98/200] Iter[301/391]\t\tLoss: 2.3098 Acc@1: 43.222% [16652/38528]\n",
      "| Epoch [ 98/200] Iter[321/391]\t\tLoss: 2.6668 Acc@1: 43.208% [17753/41088]\n",
      "| Epoch [ 98/200] Iter[341/391]\t\tLoss: 3.5578 Acc@1: 42.893% [18722/43648]\n",
      "| Epoch [ 98/200] Iter[361/391]\t\tLoss: 1.7739 Acc@1: 43.187% [19955/46208]\n",
      "| Epoch [ 98/200] Iter[381/391]\t\tLoss: 2.6530 Acc@1: 43.156% [21046/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #98\n",
      "\n",
      "| Validation Epoch #98\t\t\tLoss: 1.4239 Acc@1: 59.51%\n",
      "\n",
      "----- Elapsed time : 0:28:40\n",
      "\n",
      "\n",
      "=> Training Epoch #99\n",
      "| Epoch [ 99/200] Iter[  1/391]\t\tLoss: 3.0018 Acc@1: 35.571% [ 45/128]\n",
      "| Epoch [ 99/200] Iter[ 21/391]\t\tLoss: 2.9200 Acc@1: 45.558% [1224/2688]\n",
      "| Epoch [ 99/200] Iter[ 41/391]\t\tLoss: 1.7732 Acc@1: 46.356% [2432/5248]\n",
      "| Epoch [ 99/200] Iter[ 61/391]\t\tLoss: 3.2017 Acc@1: 44.579% [3480/7808]\n",
      "| Epoch [ 99/200] Iter[ 81/391]\t\tLoss: 1.4088 Acc@1: 43.295% [4488/10368]\n",
      "| Epoch [ 99/200] Iter[101/391]\t\tLoss: 1.8642 Acc@1: 44.971% [5813/12928]\n",
      "| Epoch [ 99/200] Iter[121/391]\t\tLoss: 3.1634 Acc@1: 44.208% [6846/15488]\n",
      "| Epoch [ 99/200] Iter[141/391]\t\tLoss: 2.8910 Acc@1: 43.726% [7891/18048]\n",
      "| Epoch [ 99/200] Iter[161/391]\t\tLoss: 2.6404 Acc@1: 44.074% [9082/20608]\n",
      "| Epoch [ 99/200] Iter[181/391]\t\tLoss: 3.0788 Acc@1: 44.026% [10199/23168]\n",
      "| Epoch [ 99/200] Iter[201/391]\t\tLoss: 2.4936 Acc@1: 44.267% [11388/25728]\n",
      "| Epoch [ 99/200] Iter[221/391]\t\tLoss: 2.7483 Acc@1: 44.329% [12539/28288]\n",
      "| Epoch [ 99/200] Iter[241/391]\t\tLoss: 2.1608 Acc@1: 44.372% [13687/30848]\n",
      "| Epoch [ 99/200] Iter[261/391]\t\tLoss: 2.3954 Acc@1: 44.057% [14718/33408]\n",
      "| Epoch [ 99/200] Iter[281/391]\t\tLoss: 2.2042 Acc@1: 44.398% [15969/35968]\n",
      "| Epoch [ 99/200] Iter[301/391]\t\tLoss: 2.6322 Acc@1: 44.471% [17133/38528]\n",
      "| Epoch [ 99/200] Iter[321/391]\t\tLoss: 3.2222 Acc@1: 44.319% [18209/41088]\n",
      "| Epoch [ 99/200] Iter[341/391]\t\tLoss: 3.1052 Acc@1: 44.192% [19288/43648]\n",
      "| Epoch [ 99/200] Iter[361/391]\t\tLoss: 1.7100 Acc@1: 44.104% [20379/46208]\n",
      "| Epoch [ 99/200] Iter[381/391]\t\tLoss: 2.3347 Acc@1: 44.262% [21585/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #99\n",
      "\n",
      "| Validation Epoch #99\t\t\tLoss: 1.5420 Acc@1: 58.91%\n",
      "\n",
      "----- Elapsed time : 0:28:57\n",
      "\n",
      "\n",
      "=> Training Epoch #100\n",
      "| Epoch [100/200] Iter[  1/391]\t\tLoss: 1.4363 Acc@1: 63.226% [ 80/128]\n",
      "| Epoch [100/200] Iter[ 21/391]\t\tLoss: 3.2305 Acc@1: 43.502% [1169/2688]\n",
      "| Epoch [100/200] Iter[ 41/391]\t\tLoss: 2.0007 Acc@1: 45.000% [2361/5248]\n",
      "| Epoch [100/200] Iter[ 61/391]\t\tLoss: 2.8489 Acc@1: 45.319% [3538/7808]\n",
      "| Epoch [100/200] Iter[ 81/391]\t\tLoss: 2.3525 Acc@1: 45.550% [4722/10368]\n",
      "| Epoch [100/200] Iter[101/391]\t\tLoss: 3.3532 Acc@1: 45.253% [5850/12928]\n",
      "| Epoch [100/200] Iter[121/391]\t\tLoss: 2.0604 Acc@1: 44.715% [6925/15488]\n",
      "| Epoch [100/200] Iter[141/391]\t\tLoss: 1.9489 Acc@1: 44.592% [8047/18048]\n",
      "| Epoch [100/200] Iter[161/391]\t\tLoss: 3.3526 Acc@1: 44.449% [9159/20608]\n",
      "| Epoch [100/200] Iter[181/391]\t\tLoss: 1.6790 Acc@1: 44.594% [10331/23168]\n",
      "| Epoch [100/200] Iter[201/391]\t\tLoss: 2.0223 Acc@1: 44.422% [11428/25728]\n",
      "| Epoch [100/200] Iter[221/391]\t\tLoss: 2.6288 Acc@1: 44.050% [12460/28288]\n",
      "| Epoch [100/200] Iter[241/391]\t\tLoss: 2.5627 Acc@1: 43.880% [13535/30848]\n",
      "| Epoch [100/200] Iter[261/391]\t\tLoss: 2.7921 Acc@1: 43.696% [14598/33408]\n",
      "| Epoch [100/200] Iter[281/391]\t\tLoss: 3.2931 Acc@1: 43.437% [15623/35968]\n",
      "| Epoch [100/200] Iter[301/391]\t\tLoss: 1.5701 Acc@1: 43.458% [16743/38528]\n",
      "| Epoch [100/200] Iter[321/391]\t\tLoss: 1.6923 Acc@1: 43.720% [17963/41088]\n",
      "| Epoch [100/200] Iter[341/391]\t\tLoss: 1.9653 Acc@1: 43.839% [19134/43648]\n",
      "| Epoch [100/200] Iter[361/391]\t\tLoss: 3.0646 Acc@1: 43.656% [20172/46208]\n",
      "| Epoch [100/200] Iter[381/391]\t\tLoss: 1.6707 Acc@1: 43.701% [21311/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #100\n",
      "\n",
      "| Validation Epoch #100\t\t\tLoss: 1.4717 Acc@1: 59.36%\n",
      "\n",
      "----- Elapsed time : 0:29:15\n",
      "\n",
      "\n",
      "=> Training Epoch #101\n",
      "| Epoch [101/200] Iter[  1/391]\t\tLoss: 1.3205 Acc@1: 67.737% [ 86/128]\n",
      "| Epoch [101/200] Iter[ 21/391]\t\tLoss: 2.3276 Acc@1: 50.598% [1360/2688]\n",
      "| Epoch [101/200] Iter[ 41/391]\t\tLoss: 2.8854 Acc@1: 46.843% [2458/5248]\n",
      "| Epoch [101/200] Iter[ 61/391]\t\tLoss: 1.7347 Acc@1: 43.619% [3405/7808]\n",
      "| Epoch [101/200] Iter[ 81/391]\t\tLoss: 2.2336 Acc@1: 43.031% [4461/10368]\n",
      "| Epoch [101/200] Iter[101/391]\t\tLoss: 1.6952 Acc@1: 43.029% [5562/12928]\n",
      "| Epoch [101/200] Iter[121/391]\t\tLoss: 2.6376 Acc@1: 43.199% [6690/15488]\n",
      "| Epoch [101/200] Iter[141/391]\t\tLoss: 2.8666 Acc@1: 44.005% [7942/18048]\n",
      "| Epoch [101/200] Iter[161/391]\t\tLoss: 2.8401 Acc@1: 44.573% [9185/20608]\n",
      "| Epoch [101/200] Iter[181/391]\t\tLoss: 3.2887 Acc@1: 44.147% [10228/23168]\n",
      "| Epoch [101/200] Iter[201/391]\t\tLoss: 2.6041 Acc@1: 44.330% [11405/25728]\n",
      "| Epoch [101/200] Iter[221/391]\t\tLoss: 1.7211 Acc@1: 44.167% [12493/28288]\n",
      "| Epoch [101/200] Iter[241/391]\t\tLoss: 2.9451 Acc@1: 43.995% [13571/30848]\n",
      "| Epoch [101/200] Iter[261/391]\t\tLoss: 2.4081 Acc@1: 43.899% [14665/33408]\n",
      "| Epoch [101/200] Iter[281/391]\t\tLoss: 2.9648 Acc@1: 43.807% [15756/35968]\n",
      "| Epoch [101/200] Iter[301/391]\t\tLoss: 3.4738 Acc@1: 43.633% [16810/38528]\n",
      "| Epoch [101/200] Iter[321/391]\t\tLoss: 1.7018 Acc@1: 43.791% [17992/41088]\n",
      "| Epoch [101/200] Iter[341/391]\t\tLoss: 2.6175 Acc@1: 43.960% [19187/43648]\n",
      "| Epoch [101/200] Iter[361/391]\t\tLoss: 2.2621 Acc@1: 44.060% [20359/46208]\n",
      "| Epoch [101/200] Iter[381/391]\t\tLoss: 2.4443 Acc@1: 44.293% [21600/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #101\n",
      "\n",
      "| Validation Epoch #101\t\t\tLoss: 1.4455 Acc@1: 59.88%\n",
      "\n",
      "----- Elapsed time : 0:29:32\n",
      "\n",
      "\n",
      "=> Training Epoch #102\n",
      "| Epoch [102/200] Iter[  1/391]\t\tLoss: 3.3263 Acc@1: 23.648% [ 30/128]\n",
      "| Epoch [102/200] Iter[ 21/391]\t\tLoss: 2.1276 Acc@1: 46.047% [1237/2688]\n",
      "| Epoch [102/200] Iter[ 41/391]\t\tLoss: 3.0578 Acc@1: 46.706% [2451/5248]\n",
      "| Epoch [102/200] Iter[ 61/391]\t\tLoss: 3.2141 Acc@1: 45.767% [3573/7808]\n",
      "| Epoch [102/200] Iter[ 81/391]\t\tLoss: 3.0595 Acc@1: 44.855% [4650/10368]\n",
      "| Epoch [102/200] Iter[101/391]\t\tLoss: 2.9926 Acc@1: 45.152% [5837/12928]\n",
      "| Epoch [102/200] Iter[121/391]\t\tLoss: 1.9600 Acc@1: 44.656% [6916/15488]\n",
      "| Epoch [102/200] Iter[141/391]\t\tLoss: 2.1927 Acc@1: 44.484% [8028/18048]\n",
      "| Epoch [102/200] Iter[161/391]\t\tLoss: 3.1056 Acc@1: 44.619% [9195/20608]\n",
      "| Epoch [102/200] Iter[181/391]\t\tLoss: 2.1708 Acc@1: 44.739% [10365/23168]\n",
      "| Epoch [102/200] Iter[201/391]\t\tLoss: 3.0892 Acc@1: 44.225% [11378/25728]\n",
      "| Epoch [102/200] Iter[221/391]\t\tLoss: 1.5998 Acc@1: 44.218% [12508/28288]\n",
      "| Epoch [102/200] Iter[241/391]\t\tLoss: 2.0361 Acc@1: 44.255% [13651/30848]\n",
      "| Epoch [102/200] Iter[261/391]\t\tLoss: 2.4619 Acc@1: 44.598% [14899/33408]\n",
      "| Epoch [102/200] Iter[281/391]\t\tLoss: 2.9243 Acc@1: 44.354% [15953/35968]\n",
      "| Epoch [102/200] Iter[301/391]\t\tLoss: 2.6099 Acc@1: 44.623% [17192/38528]\n",
      "| Epoch [102/200] Iter[321/391]\t\tLoss: 2.9361 Acc@1: 44.665% [18352/41088]\n",
      "| Epoch [102/200] Iter[341/391]\t\tLoss: 1.2304 Acc@1: 44.988% [19636/43648]\n",
      "| Epoch [102/200] Iter[361/391]\t\tLoss: 3.1989 Acc@1: 44.695% [20652/46208]\n",
      "| Epoch [102/200] Iter[381/391]\t\tLoss: 3.1936 Acc@1: 44.396% [21650/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #102\n",
      "\n",
      "| Validation Epoch #102\t\t\tLoss: 1.3852 Acc@1: 59.23%\n",
      "\n",
      "----- Elapsed time : 0:29:50\n",
      "\n",
      "\n",
      "=> Training Epoch #103\n",
      "| Epoch [103/200] Iter[  1/391]\t\tLoss: 1.8149 Acc@1: 65.756% [ 84/128]\n",
      "| Epoch [103/200] Iter[ 21/391]\t\tLoss: 3.4691 Acc@1: 47.538% [1277/2688]\n",
      "| Epoch [103/200] Iter[ 41/391]\t\tLoss: 2.0775 Acc@1: 47.900% [2513/5248]\n",
      "| Epoch [103/200] Iter[ 61/391]\t\tLoss: 2.4821 Acc@1: 47.352% [3697/7808]\n",
      "| Epoch [103/200] Iter[ 81/391]\t\tLoss: 3.3659 Acc@1: 47.156% [4889/10368]\n",
      "| Epoch [103/200] Iter[101/391]\t\tLoss: 3.3076 Acc@1: 45.762% [5916/12928]\n",
      "| Epoch [103/200] Iter[121/391]\t\tLoss: 1.7605 Acc@1: 45.742% [7084/15488]\n",
      "| Epoch [103/200] Iter[141/391]\t\tLoss: 3.1273 Acc@1: 45.960% [8294/18048]\n",
      "| Epoch [103/200] Iter[161/391]\t\tLoss: 1.8211 Acc@1: 46.399% [9561/20608]\n",
      "| Epoch [103/200] Iter[181/391]\t\tLoss: 2.1758 Acc@1: 46.349% [10738/23168]\n",
      "| Epoch [103/200] Iter[201/391]\t\tLoss: 1.7199 Acc@1: 46.397% [11936/25728]\n",
      "| Epoch [103/200] Iter[221/391]\t\tLoss: 3.1311 Acc@1: 46.395% [13124/28288]\n",
      "| Epoch [103/200] Iter[241/391]\t\tLoss: 2.8062 Acc@1: 46.272% [14273/30848]\n",
      "| Epoch [103/200] Iter[261/391]\t\tLoss: 3.2847 Acc@1: 45.759% [15287/33408]\n",
      "| Epoch [103/200] Iter[281/391]\t\tLoss: 1.4195 Acc@1: 45.663% [16423/35968]\n",
      "| Epoch [103/200] Iter[301/391]\t\tLoss: 1.4609 Acc@1: 45.727% [17617/38528]\n",
      "| Epoch [103/200] Iter[321/391]\t\tLoss: 3.1182 Acc@1: 45.553% [18716/41088]\n",
      "| Epoch [103/200] Iter[341/391]\t\tLoss: 3.1025 Acc@1: 45.593% [19900/43648]\n",
      "| Epoch [103/200] Iter[361/391]\t\tLoss: 2.9241 Acc@1: 45.684% [21109/46208]\n",
      "| Epoch [103/200] Iter[381/391]\t\tLoss: 2.8194 Acc@1: 45.727% [22300/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #103\n",
      "\n",
      "| Validation Epoch #103\t\t\tLoss: 1.5071 Acc@1: 59.56%\n",
      "\n",
      "----- Elapsed time : 0:30:08\n",
      "\n",
      "\n",
      "=> Training Epoch #104\n",
      "| Epoch [104/200] Iter[  1/391]\t\tLoss: 3.3888 Acc@1: 27.049% [ 34/128]\n",
      "| Epoch [104/200] Iter[ 21/391]\t\tLoss: 2.7420 Acc@1: 44.844% [1205/2688]\n",
      "| Epoch [104/200] Iter[ 41/391]\t\tLoss: 2.5322 Acc@1: 41.101% [2157/5248]\n",
      "| Epoch [104/200] Iter[ 61/391]\t\tLoss: 1.7873 Acc@1: 42.544% [3321/7808]\n",
      "| Epoch [104/200] Iter[ 81/391]\t\tLoss: 2.2170 Acc@1: 42.653% [4422/10368]\n",
      "| Epoch [104/200] Iter[101/391]\t\tLoss: 3.3046 Acc@1: 43.920% [5678/12928]\n",
      "| Epoch [104/200] Iter[121/391]\t\tLoss: 3.3845 Acc@1: 43.679% [6764/15488]\n",
      "| Epoch [104/200] Iter[141/391]\t\tLoss: 3.2543 Acc@1: 43.798% [7904/18048]\n",
      "| Epoch [104/200] Iter[161/391]\t\tLoss: 3.3587 Acc@1: 43.798% [9025/20608]\n",
      "| Epoch [104/200] Iter[181/391]\t\tLoss: 2.7395 Acc@1: 43.509% [10080/23168]\n",
      "| Epoch [104/200] Iter[201/391]\t\tLoss: 1.7303 Acc@1: 43.882% [11290/25728]\n",
      "| Epoch [104/200] Iter[221/391]\t\tLoss: 2.3507 Acc@1: 44.166% [12493/28288]\n",
      "| Epoch [104/200] Iter[241/391]\t\tLoss: 1.8065 Acc@1: 44.162% [13623/30848]\n",
      "| Epoch [104/200] Iter[261/391]\t\tLoss: 2.2901 Acc@1: 44.507% [14868/33408]\n",
      "| Epoch [104/200] Iter[281/391]\t\tLoss: 2.9071 Acc@1: 44.761% [16099/35968]\n",
      "| Epoch [104/200] Iter[301/391]\t\tLoss: 1.9057 Acc@1: 44.714% [17227/38528]\n",
      "| Epoch [104/200] Iter[321/391]\t\tLoss: 2.1362 Acc@1: 44.685% [18360/41088]\n",
      "| Epoch [104/200] Iter[341/391]\t\tLoss: 1.2174 Acc@1: 44.729% [19523/43648]\n",
      "| Epoch [104/200] Iter[361/391]\t\tLoss: 3.3303 Acc@1: 44.418% [20524/46208]\n",
      "| Epoch [104/200] Iter[381/391]\t\tLoss: 2.6740 Acc@1: 44.469% [21686/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #104\n",
      "\n",
      "| Validation Epoch #104\t\t\tLoss: 1.8270 Acc@1: 60.23%\n",
      "\n",
      "----- Elapsed time : 0:30:27\n",
      "\n",
      "\n",
      "=> Training Epoch #105\n",
      "| Epoch [105/200] Iter[  1/391]\t\tLoss: 3.3824 Acc@1: 22.761% [ 29/128]\n",
      "| Epoch [105/200] Iter[ 21/391]\t\tLoss: 2.1183 Acc@1: 39.405% [1059/2688]\n",
      "| Epoch [105/200] Iter[ 41/391]\t\tLoss: 3.2479 Acc@1: 43.995% [2308/5248]\n",
      "| Epoch [105/200] Iter[ 61/391]\t\tLoss: 3.2347 Acc@1: 46.445% [3626/7808]\n",
      "| Epoch [105/200] Iter[ 81/391]\t\tLoss: 2.8980 Acc@1: 44.795% [4644/10368]\n",
      "| Epoch [105/200] Iter[101/391]\t\tLoss: 2.0773 Acc@1: 45.440% [5874/12928]\n",
      "| Epoch [105/200] Iter[121/391]\t\tLoss: 1.9517 Acc@1: 44.559% [6901/15488]\n",
      "| Epoch [105/200] Iter[141/391]\t\tLoss: 1.5775 Acc@1: 43.971% [7935/18048]\n",
      "| Epoch [105/200] Iter[161/391]\t\tLoss: 3.3028 Acc@1: 43.681% [9001/20608]\n",
      "| Epoch [105/200] Iter[181/391]\t\tLoss: 2.0263 Acc@1: 43.622% [10106/23168]\n",
      "| Epoch [105/200] Iter[201/391]\t\tLoss: 2.1629 Acc@1: 43.627% [11224/25728]\n",
      "| Epoch [105/200] Iter[221/391]\t\tLoss: 3.2738 Acc@1: 43.610% [12336/28288]\n",
      "| Epoch [105/200] Iter[241/391]\t\tLoss: 2.9841 Acc@1: 43.295% [13355/30848]\n",
      "| Epoch [105/200] Iter[261/391]\t\tLoss: 2.8281 Acc@1: 43.356% [14484/33408]\n",
      "| Epoch [105/200] Iter[281/391]\t\tLoss: 1.3156 Acc@1: 43.461% [15632/35968]\n",
      "| Epoch [105/200] Iter[301/391]\t\tLoss: 3.1826 Acc@1: 43.274% [16672/38528]\n",
      "| Epoch [105/200] Iter[321/391]\t\tLoss: 2.6434 Acc@1: 43.071% [17697/41088]\n",
      "| Epoch [105/200] Iter[341/391]\t\tLoss: 2.5556 Acc@1: 43.344% [18918/43648]\n",
      "| Epoch [105/200] Iter[361/391]\t\tLoss: 2.9789 Acc@1: 43.268% [19993/46208]\n",
      "| Epoch [105/200] Iter[381/391]\t\tLoss: 2.8169 Acc@1: 43.404% [21167/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #105\n",
      "\n",
      "| Validation Epoch #105\t\t\tLoss: 1.3721 Acc@1: 60.39%\n",
      "\n",
      "----- Elapsed time : 0:30:45\n",
      "\n",
      "\n",
      "=> Training Epoch #106\n",
      "| Epoch [106/200] Iter[  1/391]\t\tLoss: 2.8469 Acc@1: 40.589% [ 51/128]\n",
      "| Epoch [106/200] Iter[ 21/391]\t\tLoss: 2.6563 Acc@1: 41.371% [1112/2688]\n",
      "| Epoch [106/200] Iter[ 41/391]\t\tLoss: 1.9440 Acc@1: 44.620% [2341/5248]\n",
      "| Epoch [106/200] Iter[ 61/391]\t\tLoss: 2.6371 Acc@1: 43.596% [3403/7808]\n",
      "| Epoch [106/200] Iter[ 81/391]\t\tLoss: 1.4880 Acc@1: 44.879% [4653/10368]\n",
      "| Epoch [106/200] Iter[101/391]\t\tLoss: 2.7231 Acc@1: 44.889% [5803/12928]\n",
      "| Epoch [106/200] Iter[121/391]\t\tLoss: 3.5776 Acc@1: 45.182% [6997/15488]\n",
      "| Epoch [106/200] Iter[141/391]\t\tLoss: 1.9679 Acc@1: 46.094% [8319/18048]\n",
      "| Epoch [106/200] Iter[161/391]\t\tLoss: 3.1898 Acc@1: 45.491% [9374/20608]\n",
      "| Epoch [106/200] Iter[181/391]\t\tLoss: 3.2166 Acc@1: 45.241% [10481/23168]\n",
      "| Epoch [106/200] Iter[201/391]\t\tLoss: 1.3905 Acc@1: 45.537% [11715/25728]\n",
      "| Epoch [106/200] Iter[221/391]\t\tLoss: 2.7706 Acc@1: 45.133% [12767/28288]\n",
      "| Epoch [106/200] Iter[241/391]\t\tLoss: 2.7574 Acc@1: 45.549% [14051/30848]\n",
      "| Epoch [106/200] Iter[261/391]\t\tLoss: 1.5207 Acc@1: 45.343% [15148/33408]\n",
      "| Epoch [106/200] Iter[281/391]\t\tLoss: 3.6417 Acc@1: 45.212% [16261/35968]\n",
      "| Epoch [106/200] Iter[301/391]\t\tLoss: 1.9257 Acc@1: 45.286% [17447/38528]\n",
      "| Epoch [106/200] Iter[321/391]\t\tLoss: 3.3181 Acc@1: 44.882% [18441/41088]\n",
      "| Epoch [106/200] Iter[341/391]\t\tLoss: 1.8851 Acc@1: 44.867% [19583/43648]\n",
      "| Epoch [106/200] Iter[361/391]\t\tLoss: 1.4849 Acc@1: 44.809% [20705/46208]\n",
      "| Epoch [106/200] Iter[381/391]\t\tLoss: 3.0881 Acc@1: 44.638% [21769/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #106\n",
      "\n",
      "| Validation Epoch #106\t\t\tLoss: 1.4473 Acc@1: 60.62%\n",
      "\n",
      "----- Elapsed time : 0:31:04\n",
      "\n",
      "\n",
      "=> Training Epoch #107\n",
      "| Epoch [107/200] Iter[  1/391]\t\tLoss: 1.4586 Acc@1: 68.697% [ 87/128]\n",
      "| Epoch [107/200] Iter[ 21/391]\t\tLoss: 1.9970 Acc@1: 50.566% [1359/2688]\n",
      "| Epoch [107/200] Iter[ 41/391]\t\tLoss: 1.8650 Acc@1: 46.524% [2441/5248]\n",
      "| Epoch [107/200] Iter[ 61/391]\t\tLoss: 2.8160 Acc@1: 43.826% [3421/7808]\n",
      "| Epoch [107/200] Iter[ 81/391]\t\tLoss: 2.8156 Acc@1: 43.796% [4540/10368]\n",
      "| Epoch [107/200] Iter[101/391]\t\tLoss: 3.3235 Acc@1: 43.698% [5649/12928]\n",
      "| Epoch [107/200] Iter[121/391]\t\tLoss: 2.7795 Acc@1: 43.757% [6777/15488]\n",
      "| Epoch [107/200] Iter[141/391]\t\tLoss: 2.7485 Acc@1: 43.082% [7775/18048]\n",
      "| Epoch [107/200] Iter[161/391]\t\tLoss: 1.7603 Acc@1: 43.657% [8996/20608]\n",
      "| Epoch [107/200] Iter[181/391]\t\tLoss: 1.8500 Acc@1: 43.620% [10105/23168]\n",
      "| Epoch [107/200] Iter[201/391]\t\tLoss: 1.5509 Acc@1: 43.843% [11279/25728]\n",
      "| Epoch [107/200] Iter[221/391]\t\tLoss: 3.3309 Acc@1: 43.599% [12333/28288]\n",
      "| Epoch [107/200] Iter[241/391]\t\tLoss: 3.0868 Acc@1: 43.687% [13476/30848]\n",
      "| Epoch [107/200] Iter[261/391]\t\tLoss: 3.2564 Acc@1: 43.844% [14647/33408]\n",
      "| Epoch [107/200] Iter[281/391]\t\tLoss: 2.2015 Acc@1: 43.967% [15814/35968]\n",
      "| Epoch [107/200] Iter[301/391]\t\tLoss: 2.3626 Acc@1: 43.703% [16837/38528]\n",
      "| Epoch [107/200] Iter[321/391]\t\tLoss: 2.9873 Acc@1: 43.614% [17920/41088]\n",
      "| Epoch [107/200] Iter[341/391]\t\tLoss: 1.7689 Acc@1: 43.743% [19092/43648]\n",
      "| Epoch [107/200] Iter[361/391]\t\tLoss: 2.7162 Acc@1: 43.860% [20266/46208]\n",
      "| Epoch [107/200] Iter[381/391]\t\tLoss: 1.7545 Acc@1: 43.988% [21452/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #107\n",
      "\n",
      "| Validation Epoch #107\t\t\tLoss: 1.4636 Acc@1: 59.44%\n",
      "\n",
      "----- Elapsed time : 0:31:23\n",
      "\n",
      "\n",
      "=> Training Epoch #108\n",
      "| Epoch [108/200] Iter[  1/391]\t\tLoss: 1.8546 Acc@1: 60.763% [ 77/128]\n",
      "| Epoch [108/200] Iter[ 21/391]\t\tLoss: 2.2587 Acc@1: 44.062% [1184/2688]\n",
      "| Epoch [108/200] Iter[ 41/391]\t\tLoss: 1.8658 Acc@1: 43.531% [2284/5248]\n",
      "| Epoch [108/200] Iter[ 61/391]\t\tLoss: 2.2943 Acc@1: 43.173% [3370/7808]\n",
      "| Epoch [108/200] Iter[ 81/391]\t\tLoss: 1.9499 Acc@1: 43.932% [4554/10368]\n",
      "| Epoch [108/200] Iter[101/391]\t\tLoss: 3.1451 Acc@1: 43.071% [5568/12928]\n",
      "| Epoch [108/200] Iter[121/391]\t\tLoss: 1.3783 Acc@1: 43.654% [6761/15488]\n",
      "| Epoch [108/200] Iter[141/391]\t\tLoss: 1.7702 Acc@1: 43.462% [7844/18048]\n",
      "| Epoch [108/200] Iter[161/391]\t\tLoss: 1.4828 Acc@1: 43.744% [9014/20608]\n",
      "| Epoch [108/200] Iter[181/391]\t\tLoss: 3.2132 Acc@1: 43.306% [10033/23168]\n",
      "| Epoch [108/200] Iter[201/391]\t\tLoss: 3.2920 Acc@1: 43.134% [11097/25728]\n",
      "| Epoch [108/200] Iter[221/391]\t\tLoss: 1.4794 Acc@1: 43.225% [12227/28288]\n",
      "| Epoch [108/200] Iter[241/391]\t\tLoss: 2.9876 Acc@1: 42.970% [13255/30848]\n",
      "| Epoch [108/200] Iter[261/391]\t\tLoss: 3.0846 Acc@1: 43.156% [14417/33408]\n",
      "| Epoch [108/200] Iter[281/391]\t\tLoss: 3.1535 Acc@1: 42.784% [15388/35968]\n",
      "| Epoch [108/200] Iter[301/391]\t\tLoss: 3.2918 Acc@1: 42.820% [16497/38528]\n",
      "| Epoch [108/200] Iter[321/391]\t\tLoss: 3.4373 Acc@1: 42.896% [17624/41088]\n",
      "| Epoch [108/200] Iter[341/391]\t\tLoss: 2.5189 Acc@1: 43.110% [18816/43648]\n",
      "| Epoch [108/200] Iter[361/391]\t\tLoss: 3.1752 Acc@1: 43.362% [20036/46208]\n",
      "| Epoch [108/200] Iter[381/391]\t\tLoss: 3.1569 Acc@1: 43.421% [21175/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #108\n",
      "\n",
      "| Validation Epoch #108\t\t\tLoss: 1.6153 Acc@1: 60.29%\n",
      "\n",
      "----- Elapsed time : 0:31:41\n",
      "\n",
      "\n",
      "=> Training Epoch #109\n",
      "| Epoch [109/200] Iter[  1/391]\t\tLoss: 3.3140 Acc@1: 25.000% [ 32/128]\n",
      "| Epoch [109/200] Iter[ 21/391]\t\tLoss: 2.6719 Acc@1: 44.933% [1207/2688]\n",
      "| Epoch [109/200] Iter[ 41/391]\t\tLoss: 2.0260 Acc@1: 44.819% [2352/5248]\n",
      "| Epoch [109/200] Iter[ 61/391]\t\tLoss: 3.4622 Acc@1: 43.911% [3428/7808]\n",
      "| Epoch [109/200] Iter[ 81/391]\t\tLoss: 2.2790 Acc@1: 44.753% [4639/10368]\n",
      "| Epoch [109/200] Iter[101/391]\t\tLoss: 2.8266 Acc@1: 45.347% [5862/12928]\n",
      "| Epoch [109/200] Iter[121/391]\t\tLoss: 2.9621 Acc@1: 44.627% [6911/15488]\n",
      "| Epoch [109/200] Iter[141/391]\t\tLoss: 3.2214 Acc@1: 45.026% [8126/18048]\n",
      "| Epoch [109/200] Iter[161/391]\t\tLoss: 3.2743 Acc@1: 44.612% [9193/20608]\n",
      "| Epoch [109/200] Iter[181/391]\t\tLoss: 3.1456 Acc@1: 44.646% [10343/23168]\n",
      "| Epoch [109/200] Iter[201/391]\t\tLoss: 2.5337 Acc@1: 44.495% [11447/25728]\n",
      "| Epoch [109/200] Iter[221/391]\t\tLoss: 1.8399 Acc@1: 44.486% [12584/28288]\n",
      "| Epoch [109/200] Iter[241/391]\t\tLoss: 3.1177 Acc@1: 44.744% [13802/30848]\n",
      "| Epoch [109/200] Iter[261/391]\t\tLoss: 2.8790 Acc@1: 44.359% [14819/33408]\n",
      "| Epoch [109/200] Iter[281/391]\t\tLoss: 2.9252 Acc@1: 44.541% [16020/35968]\n",
      "| Epoch [109/200] Iter[301/391]\t\tLoss: 1.5729 Acc@1: 44.827% [17270/38528]\n",
      "| Epoch [109/200] Iter[321/391]\t\tLoss: 3.0592 Acc@1: 44.677% [18357/41088]\n",
      "| Epoch [109/200] Iter[341/391]\t\tLoss: 3.3560 Acc@1: 44.439% [19396/43648]\n",
      "| Epoch [109/200] Iter[361/391]\t\tLoss: 2.0723 Acc@1: 44.438% [20533/46208]\n",
      "| Epoch [109/200] Iter[381/391]\t\tLoss: 2.0264 Acc@1: 44.331% [21619/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #109\n",
      "\n",
      "| Validation Epoch #109\t\t\tLoss: 1.5127 Acc@1: 60.73%\n",
      "\n",
      "----- Elapsed time : 0:32:00\n",
      "\n",
      "\n",
      "=> Training Epoch #110\n",
      "| Epoch [110/200] Iter[  1/391]\t\tLoss: 2.9652 Acc@1: 34.407% [ 44/128]\n",
      "| Epoch [110/200] Iter[ 21/391]\t\tLoss: 2.5141 Acc@1: 37.875% [1018/2688]\n",
      "| Epoch [110/200] Iter[ 41/391]\t\tLoss: 2.3603 Acc@1: 43.618% [2289/5248]\n",
      "| Epoch [110/200] Iter[ 61/391]\t\tLoss: 3.0770 Acc@1: 43.809% [3420/7808]\n",
      "| Epoch [110/200] Iter[ 81/391]\t\tLoss: 3.1470 Acc@1: 42.531% [4409/10368]\n",
      "| Epoch [110/200] Iter[101/391]\t\tLoss: 2.4209 Acc@1: 43.145% [5577/12928]\n",
      "| Epoch [110/200] Iter[121/391]\t\tLoss: 3.2099 Acc@1: 42.755% [6621/15488]\n",
      "| Epoch [110/200] Iter[141/391]\t\tLoss: 3.0502 Acc@1: 42.901% [7742/18048]\n",
      "| Epoch [110/200] Iter[161/391]\t\tLoss: 3.4025 Acc@1: 43.108% [8883/20608]\n",
      "| Epoch [110/200] Iter[181/391]\t\tLoss: 3.2188 Acc@1: 42.831% [9923/23168]\n",
      "| Epoch [110/200] Iter[201/391]\t\tLoss: 3.2842 Acc@1: 42.694% [10984/25728]\n",
      "| Epoch [110/200] Iter[221/391]\t\tLoss: 3.2931 Acc@1: 42.795% [12105/28288]\n",
      "| Epoch [110/200] Iter[241/391]\t\tLoss: 1.9848 Acc@1: 42.856% [13220/30848]\n",
      "| Epoch [110/200] Iter[261/391]\t\tLoss: 3.0931 Acc@1: 42.705% [14267/33408]\n",
      "| Epoch [110/200] Iter[281/391]\t\tLoss: 3.1585 Acc@1: 42.855% [15414/35968]\n",
      "| Epoch [110/200] Iter[301/391]\t\tLoss: 2.3928 Acc@1: 42.975% [16557/38528]\n",
      "| Epoch [110/200] Iter[321/391]\t\tLoss: 2.2010 Acc@1: 43.096% [17707/41088]\n",
      "| Epoch [110/200] Iter[341/391]\t\tLoss: 2.4467 Acc@1: 43.242% [18874/43648]\n",
      "| Epoch [110/200] Iter[361/391]\t\tLoss: 3.1443 Acc@1: 43.130% [19929/46208]\n",
      "| Epoch [110/200] Iter[381/391]\t\tLoss: 3.0208 Acc@1: 43.368% [21149/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #110\n",
      "\n",
      "| Validation Epoch #110\t\t\tLoss: 1.4859 Acc@1: 59.93%\n",
      "\n",
      "----- Elapsed time : 0:32:18\n",
      "\n",
      "\n",
      "=> Training Epoch #111\n",
      "| Epoch [111/200] Iter[  1/391]\t\tLoss: 3.0459 Acc@1: 33.468% [ 42/128]\n",
      "| Epoch [111/200] Iter[ 21/391]\t\tLoss: 3.2772 Acc@1: 41.221% [1108/2688]\n",
      "| Epoch [111/200] Iter[ 41/391]\t\tLoss: 3.3505 Acc@1: 43.053% [2259/5248]\n",
      "| Epoch [111/200] Iter[ 61/391]\t\tLoss: 3.1662 Acc@1: 41.947% [3275/7808]\n",
      "| Epoch [111/200] Iter[ 81/391]\t\tLoss: 2.5333 Acc@1: 43.346% [4494/10368]\n",
      "| Epoch [111/200] Iter[101/391]\t\tLoss: 2.1221 Acc@1: 43.778% [5659/12928]\n",
      "| Epoch [111/200] Iter[121/391]\t\tLoss: 2.7447 Acc@1: 43.953% [6807/15488]\n",
      "| Epoch [111/200] Iter[141/391]\t\tLoss: 3.0909 Acc@1: 44.059% [7951/18048]\n",
      "| Epoch [111/200] Iter[161/391]\t\tLoss: 2.6207 Acc@1: 44.051% [9077/20608]\n",
      "| Epoch [111/200] Iter[181/391]\t\tLoss: 1.5769 Acc@1: 44.599% [10332/23168]\n",
      "| Epoch [111/200] Iter[201/391]\t\tLoss: 2.1080 Acc@1: 44.872% [11544/25728]\n",
      "| Epoch [111/200] Iter[221/391]\t\tLoss: 3.2747 Acc@1: 44.694% [12642/28288]\n",
      "| Epoch [111/200] Iter[241/391]\t\tLoss: 3.2539 Acc@1: 44.226% [13642/30848]\n",
      "| Epoch [111/200] Iter[261/391]\t\tLoss: 2.0903 Acc@1: 44.279% [14792/33408]\n",
      "| Epoch [111/200] Iter[281/391]\t\tLoss: 2.4013 Acc@1: 44.353% [15952/35968]\n",
      "| Epoch [111/200] Iter[301/391]\t\tLoss: 2.3421 Acc@1: 44.372% [17095/38528]\n",
      "| Epoch [111/200] Iter[321/391]\t\tLoss: 3.0057 Acc@1: 44.468% [18270/41088]\n",
      "| Epoch [111/200] Iter[341/391]\t\tLoss: 1.3025 Acc@1: 44.706% [19513/43648]\n",
      "| Epoch [111/200] Iter[361/391]\t\tLoss: 3.2740 Acc@1: 44.715% [20661/46208]\n",
      "| Epoch [111/200] Iter[381/391]\t\tLoss: 3.3108 Acc@1: 44.556% [21728/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #111\n",
      "\n",
      "| Validation Epoch #111\t\t\tLoss: 1.6765 Acc@1: 60.08%\n",
      "\n",
      "----- Elapsed time : 0:32:36\n",
      "\n",
      "\n",
      "=> Training Epoch #112\n",
      "| Epoch [112/200] Iter[  1/391]\t\tLoss: 3.0230 Acc@1: 36.412% [ 46/128]\n",
      "| Epoch [112/200] Iter[ 21/391]\t\tLoss: 3.2334 Acc@1: 44.880% [1206/2688]\n",
      "| Epoch [112/200] Iter[ 41/391]\t\tLoss: 1.3636 Acc@1: 45.157% [2369/5248]\n",
      "| Epoch [112/200] Iter[ 61/391]\t\tLoss: 1.4758 Acc@1: 45.191% [3528/7808]\n",
      "| Epoch [112/200] Iter[ 81/391]\t\tLoss: 2.2759 Acc@1: 44.928% [4658/10368]\n",
      "| Epoch [112/200] Iter[101/391]\t\tLoss: 1.4736 Acc@1: 45.536% [5886/12928]\n",
      "| Epoch [112/200] Iter[121/391]\t\tLoss: 2.6920 Acc@1: 45.557% [7055/15488]\n",
      "| Epoch [112/200] Iter[141/391]\t\tLoss: 2.8365 Acc@1: 45.605% [8230/18048]\n",
      "| Epoch [112/200] Iter[161/391]\t\tLoss: 2.6856 Acc@1: 44.931% [9259/20608]\n",
      "| Epoch [112/200] Iter[181/391]\t\tLoss: 2.8827 Acc@1: 44.830% [10386/23168]\n",
      "| Epoch [112/200] Iter[201/391]\t\tLoss: 2.1885 Acc@1: 44.325% [11403/25728]\n",
      "| Epoch [112/200] Iter[221/391]\t\tLoss: 2.9533 Acc@1: 44.294% [12529/28288]\n",
      "| Epoch [112/200] Iter[241/391]\t\tLoss: 3.3600 Acc@1: 44.293% [13663/30848]\n",
      "| Epoch [112/200] Iter[261/391]\t\tLoss: 2.7926 Acc@1: 44.392% [14830/33408]\n",
      "| Epoch [112/200] Iter[281/391]\t\tLoss: 2.2224 Acc@1: 44.515% [16011/35968]\n",
      "| Epoch [112/200] Iter[301/391]\t\tLoss: 3.3817 Acc@1: 44.355% [17089/38528]\n",
      "| Epoch [112/200] Iter[321/391]\t\tLoss: 3.0460 Acc@1: 44.417% [18250/41088]\n",
      "| Epoch [112/200] Iter[341/391]\t\tLoss: 3.2337 Acc@1: 44.839% [19571/43648]\n",
      "| Epoch [112/200] Iter[361/391]\t\tLoss: 1.3921 Acc@1: 44.587% [20602/46208]\n",
      "| Epoch [112/200] Iter[381/391]\t\tLoss: 2.1907 Acc@1: 44.605% [21753/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #112\n",
      "\n",
      "| Validation Epoch #112\t\t\tLoss: 1.6214 Acc@1: 60.80%\n",
      "\n",
      "----- Elapsed time : 0:32:55\n",
      "\n",
      "\n",
      "=> Training Epoch #113\n",
      "| Epoch [113/200] Iter[  1/391]\t\tLoss: 2.9296 Acc@1: 38.227% [ 48/128]\n",
      "| Epoch [113/200] Iter[ 21/391]\t\tLoss: 1.7172 Acc@1: 43.691% [1174/2688]\n",
      "| Epoch [113/200] Iter[ 41/391]\t\tLoss: 1.8902 Acc@1: 41.385% [2171/5248]\n",
      "| Epoch [113/200] Iter[ 61/391]\t\tLoss: 1.6554 Acc@1: 42.648% [3329/7808]\n",
      "| Epoch [113/200] Iter[ 81/391]\t\tLoss: 2.1317 Acc@1: 43.364% [4495/10368]\n",
      "| Epoch [113/200] Iter[101/391]\t\tLoss: 1.4170 Acc@1: 43.982% [5686/12928]\n",
      "| Epoch [113/200] Iter[121/391]\t\tLoss: 1.8859 Acc@1: 44.310% [6862/15488]\n",
      "| Epoch [113/200] Iter[141/391]\t\tLoss: 3.1749 Acc@1: 44.128% [7964/18048]\n",
      "| Epoch [113/200] Iter[161/391]\t\tLoss: 3.0840 Acc@1: 44.096% [9087/20608]\n",
      "| Epoch [113/200] Iter[181/391]\t\tLoss: 2.0642 Acc@1: 44.211% [10242/23168]\n",
      "| Epoch [113/200] Iter[201/391]\t\tLoss: 2.6448 Acc@1: 44.182% [11367/25728]\n",
      "| Epoch [113/200] Iter[221/391]\t\tLoss: 2.9038 Acc@1: 43.849% [12404/28288]\n",
      "| Epoch [113/200] Iter[241/391]\t\tLoss: 2.0585 Acc@1: 44.150% [13619/30848]\n",
      "| Epoch [113/200] Iter[261/391]\t\tLoss: 2.7387 Acc@1: 44.101% [14733/33408]\n",
      "| Epoch [113/200] Iter[281/391]\t\tLoss: 3.0725 Acc@1: 43.770% [15743/35968]\n",
      "| Epoch [113/200] Iter[301/391]\t\tLoss: 2.0472 Acc@1: 44.096% [16989/38528]\n",
      "| Epoch [113/200] Iter[321/391]\t\tLoss: 3.3220 Acc@1: 44.125% [18130/41088]\n",
      "| Epoch [113/200] Iter[341/391]\t\tLoss: 1.3507 Acc@1: 44.167% [19277/43648]\n",
      "| Epoch [113/200] Iter[361/391]\t\tLoss: 1.9181 Acc@1: 44.146% [20398/46208]\n",
      "| Epoch [113/200] Iter[381/391]\t\tLoss: 2.5778 Acc@1: 44.532% [21717/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #113\n",
      "\n",
      "| Validation Epoch #113\t\t\tLoss: 1.3848 Acc@1: 60.71%\n",
      "\n",
      "----- Elapsed time : 0:33:12\n",
      "\n",
      "\n",
      "=> Training Epoch #114\n",
      "| Epoch [114/200] Iter[  1/391]\t\tLoss: 2.6032 Acc@1: 44.760% [ 57/128]\n",
      "| Epoch [114/200] Iter[ 21/391]\t\tLoss: 2.6443 Acc@1: 47.375% [1273/2688]\n",
      "| Epoch [114/200] Iter[ 41/391]\t\tLoss: 2.0490 Acc@1: 45.968% [2412/5248]\n",
      "| Epoch [114/200] Iter[ 61/391]\t\tLoss: 1.8520 Acc@1: 46.167% [3604/7808]\n",
      "| Epoch [114/200] Iter[ 81/391]\t\tLoss: 3.1740 Acc@1: 45.358% [4702/10368]\n",
      "| Epoch [114/200] Iter[101/391]\t\tLoss: 2.7309 Acc@1: 45.801% [5921/12928]\n",
      "| Epoch [114/200] Iter[121/391]\t\tLoss: 1.2374 Acc@1: 46.036% [7130/15488]\n",
      "| Epoch [114/200] Iter[141/391]\t\tLoss: 2.0535 Acc@1: 46.646% [8418/18048]\n",
      "| Epoch [114/200] Iter[161/391]\t\tLoss: 2.7956 Acc@1: 45.874% [9453/20608]\n",
      "| Epoch [114/200] Iter[181/391]\t\tLoss: 3.2014 Acc@1: 46.104% [10681/23168]\n",
      "| Epoch [114/200] Iter[201/391]\t\tLoss: 1.5636 Acc@1: 46.471% [11956/25728]\n",
      "| Epoch [114/200] Iter[221/391]\t\tLoss: 3.2108 Acc@1: 45.812% [12959/28288]\n",
      "| Epoch [114/200] Iter[241/391]\t\tLoss: 1.7190 Acc@1: 45.630% [14075/30848]\n",
      "| Epoch [114/200] Iter[261/391]\t\tLoss: 2.9837 Acc@1: 45.355% [15152/33408]\n",
      "| Epoch [114/200] Iter[281/391]\t\tLoss: 2.8099 Acc@1: 45.072% [16211/35968]\n",
      "| Epoch [114/200] Iter[301/391]\t\tLoss: 3.2065 Acc@1: 44.573% [17173/38528]\n",
      "| Epoch [114/200] Iter[321/391]\t\tLoss: 2.1560 Acc@1: 44.547% [18303/41088]\n",
      "| Epoch [114/200] Iter[341/391]\t\tLoss: 1.6937 Acc@1: 44.558% [19448/43648]\n",
      "| Epoch [114/200] Iter[361/391]\t\tLoss: 3.4271 Acc@1: 44.444% [20536/46208]\n",
      "| Epoch [114/200] Iter[381/391]\t\tLoss: 3.2043 Acc@1: 44.523% [21712/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #114\n",
      "\n",
      "| Validation Epoch #114\t\t\tLoss: 1.4895 Acc@1: 61.09%\n",
      "\n",
      "----- Elapsed time : 0:33:30\n",
      "\n",
      "\n",
      "=> Training Epoch #115\n",
      "| Epoch [115/200] Iter[  1/391]\t\tLoss: 2.9082 Acc@1: 38.593% [ 49/128]\n",
      "| Epoch [115/200] Iter[ 21/391]\t\tLoss: 3.0204 Acc@1: 47.272% [1270/2688]\n",
      "| Epoch [115/200] Iter[ 41/391]\t\tLoss: 1.7318 Acc@1: 44.774% [2349/5248]\n",
      "| Epoch [115/200] Iter[ 61/391]\t\tLoss: 2.6519 Acc@1: 46.364% [3620/7808]\n",
      "| Epoch [115/200] Iter[ 81/391]\t\tLoss: 1.7735 Acc@1: 46.013% [4770/10368]\n",
      "| Epoch [115/200] Iter[101/391]\t\tLoss: 3.3761 Acc@1: 45.814% [5922/12928]\n",
      "| Epoch [115/200] Iter[121/391]\t\tLoss: 2.2789 Acc@1: 44.864% [6948/15488]\n",
      "| Epoch [115/200] Iter[141/391]\t\tLoss: 2.0258 Acc@1: 44.908% [8104/18048]\n",
      "| Epoch [115/200] Iter[161/391]\t\tLoss: 3.1344 Acc@1: 44.102% [9088/20608]\n",
      "| Epoch [115/200] Iter[181/391]\t\tLoss: 3.3374 Acc@1: 43.946% [10181/23168]\n",
      "| Epoch [115/200] Iter[201/391]\t\tLoss: 3.2509 Acc@1: 43.888% [11291/25728]\n",
      "| Epoch [115/200] Iter[221/391]\t\tLoss: 2.8654 Acc@1: 43.870% [12410/28288]\n",
      "| Epoch [115/200] Iter[241/391]\t\tLoss: 2.3699 Acc@1: 44.077% [13596/30848]\n",
      "| Epoch [115/200] Iter[261/391]\t\tLoss: 3.0044 Acc@1: 44.161% [14753/33408]\n",
      "| Epoch [115/200] Iter[281/391]\t\tLoss: 3.0180 Acc@1: 44.228% [15907/35968]\n",
      "| Epoch [115/200] Iter[301/391]\t\tLoss: 3.2406 Acc@1: 44.002% [16953/38528]\n",
      "| Epoch [115/200] Iter[321/391]\t\tLoss: 1.2498 Acc@1: 44.560% [18308/41088]\n",
      "| Epoch [115/200] Iter[341/391]\t\tLoss: 1.4399 Acc@1: 44.995% [19639/43648]\n",
      "| Epoch [115/200] Iter[361/391]\t\tLoss: 3.4089 Acc@1: 45.174% [20873/46208]\n",
      "| Epoch [115/200] Iter[381/391]\t\tLoss: 2.1690 Acc@1: 45.335% [22108/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #115\n",
      "\n",
      "| Validation Epoch #115\t\t\tLoss: 1.5313 Acc@1: 61.22%\n",
      "\n",
      "----- Elapsed time : 0:33:49\n",
      "\n",
      "\n",
      "=> Training Epoch #116\n",
      "| Epoch [116/200] Iter[  1/391]\t\tLoss: 2.9997 Acc@1: 34.950% [ 44/128]\n",
      "| Epoch [116/200] Iter[ 21/391]\t\tLoss: 3.2590 Acc@1: 47.024% [1263/2688]\n",
      "| Epoch [116/200] Iter[ 41/391]\t\tLoss: 2.9905 Acc@1: 45.039% [2363/5248]\n",
      "| Epoch [116/200] Iter[ 61/391]\t\tLoss: 3.0604 Acc@1: 44.167% [3448/7808]\n",
      "| Epoch [116/200] Iter[ 81/391]\t\tLoss: 2.7039 Acc@1: 45.266% [4693/10368]\n",
      "| Epoch [116/200] Iter[101/391]\t\tLoss: 2.8530 Acc@1: 45.039% [5822/12928]\n",
      "| Epoch [116/200] Iter[121/391]\t\tLoss: 3.4375 Acc@1: 44.596% [6906/15488]\n",
      "| Epoch [116/200] Iter[141/391]\t\tLoss: 2.3022 Acc@1: 44.666% [8061/18048]\n",
      "| Epoch [116/200] Iter[161/391]\t\tLoss: 1.4206 Acc@1: 44.521% [9174/20608]\n",
      "| Epoch [116/200] Iter[181/391]\t\tLoss: 1.2693 Acc@1: 44.783% [10375/23168]\n",
      "| Epoch [116/200] Iter[201/391]\t\tLoss: 2.9478 Acc@1: 44.687% [11497/25728]\n",
      "| Epoch [116/200] Iter[221/391]\t\tLoss: 3.1690 Acc@1: 44.370% [12551/28288]\n",
      "| Epoch [116/200] Iter[241/391]\t\tLoss: 1.6073 Acc@1: 44.256% [13652/30848]\n",
      "| Epoch [116/200] Iter[261/391]\t\tLoss: 2.9213 Acc@1: 44.360% [14819/33408]\n",
      "| Epoch [116/200] Iter[281/391]\t\tLoss: 1.6699 Acc@1: 44.521% [16013/35968]\n",
      "| Epoch [116/200] Iter[301/391]\t\tLoss: 2.3460 Acc@1: 44.804% [17262/38528]\n",
      "| Epoch [116/200] Iter[321/391]\t\tLoss: 3.1248 Acc@1: 44.597% [18323/41088]\n",
      "| Epoch [116/200] Iter[341/391]\t\tLoss: 3.1606 Acc@1: 44.511% [19428/43648]\n",
      "| Epoch [116/200] Iter[361/391]\t\tLoss: 2.5955 Acc@1: 44.665% [20638/46208]\n",
      "| Epoch [116/200] Iter[381/391]\t\tLoss: 3.2180 Acc@1: 44.537% [21719/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #116\n",
      "\n",
      "| Validation Epoch #116\t\t\tLoss: 1.3310 Acc@1: 60.97%\n",
      "\n",
      "----- Elapsed time : 0:34:07\n",
      "\n",
      "\n",
      "=> Training Epoch #117\n",
      "| Epoch [117/200] Iter[  1/391]\t\tLoss: 1.6558 Acc@1: 63.312% [ 81/128]\n",
      "| Epoch [117/200] Iter[ 21/391]\t\tLoss: 3.2338 Acc@1: 43.341% [1165/2688]\n",
      "| Epoch [117/200] Iter[ 41/391]\t\tLoss: 3.1478 Acc@1: 42.858% [2249/5248]\n",
      "| Epoch [117/200] Iter[ 61/391]\t\tLoss: 2.6802 Acc@1: 46.332% [3617/7808]\n",
      "| Epoch [117/200] Iter[ 81/391]\t\tLoss: 1.5059 Acc@1: 48.827% [5062/10368]\n",
      "| Epoch [117/200] Iter[101/391]\t\tLoss: 2.1407 Acc@1: 48.579% [6280/12928]\n",
      "| Epoch [117/200] Iter[121/391]\t\tLoss: 2.9831 Acc@1: 49.056% [7597/15488]\n",
      "| Epoch [117/200] Iter[141/391]\t\tLoss: 3.1427 Acc@1: 48.821% [8811/18048]\n",
      "| Epoch [117/200] Iter[161/391]\t\tLoss: 1.3725 Acc@1: 48.316% [9957/20608]\n",
      "| Epoch [117/200] Iter[181/391]\t\tLoss: 2.8169 Acc@1: 48.101% [11144/23168]\n",
      "| Epoch [117/200] Iter[201/391]\t\tLoss: 2.6825 Acc@1: 47.573% [12239/25728]\n",
      "| Epoch [117/200] Iter[221/391]\t\tLoss: 1.8467 Acc@1: 47.864% [13539/28288]\n",
      "| Epoch [117/200] Iter[241/391]\t\tLoss: 3.0062 Acc@1: 47.727% [14722/30848]\n",
      "| Epoch [117/200] Iter[261/391]\t\tLoss: 3.1734 Acc@1: 47.460% [15855/33408]\n",
      "| Epoch [117/200] Iter[281/391]\t\tLoss: 2.3435 Acc@1: 47.013% [16909/35968]\n",
      "| Epoch [117/200] Iter[301/391]\t\tLoss: 3.2522 Acc@1: 46.851% [18050/38528]\n",
      "| Epoch [117/200] Iter[321/391]\t\tLoss: 1.3395 Acc@1: 46.706% [19190/41088]\n",
      "| Epoch [117/200] Iter[341/391]\t\tLoss: 2.2672 Acc@1: 46.669% [20370/43648]\n",
      "| Epoch [117/200] Iter[361/391]\t\tLoss: 2.8219 Acc@1: 46.473% [21474/46208]\n",
      "| Epoch [117/200] Iter[381/391]\t\tLoss: 2.8473 Acc@1: 46.387% [22622/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #117\n",
      "\n",
      "| Validation Epoch #117\t\t\tLoss: 1.2948 Acc@1: 61.41%\n",
      "\n",
      "----- Elapsed time : 0:34:25\n",
      "\n",
      "\n",
      "=> Training Epoch #118\n",
      "| Epoch [118/200] Iter[  1/391]\t\tLoss: 1.1398 Acc@1: 74.736% [ 95/128]\n",
      "| Epoch [118/200] Iter[ 21/391]\t\tLoss: 3.2896 Acc@1: 46.954% [1262/2688]\n",
      "| Epoch [118/200] Iter[ 41/391]\t\tLoss: 2.1767 Acc@1: 50.519% [2651/5248]\n",
      "| Epoch [118/200] Iter[ 61/391]\t\tLoss: 2.8251 Acc@1: 49.134% [3836/7808]\n",
      "| Epoch [118/200] Iter[ 81/391]\t\tLoss: 1.7536 Acc@1: 48.416% [5019/10368]\n",
      "| Epoch [118/200] Iter[101/391]\t\tLoss: 3.1082 Acc@1: 47.735% [6171/12928]\n",
      "| Epoch [118/200] Iter[121/391]\t\tLoss: 2.8150 Acc@1: 47.474% [7352/15488]\n",
      "| Epoch [118/200] Iter[141/391]\t\tLoss: 2.3984 Acc@1: 47.050% [8491/18048]\n",
      "| Epoch [118/200] Iter[161/391]\t\tLoss: 1.7588 Acc@1: 46.687% [9621/20608]\n",
      "| Epoch [118/200] Iter[181/391]\t\tLoss: 2.4056 Acc@1: 46.569% [10789/23168]\n",
      "| Epoch [118/200] Iter[201/391]\t\tLoss: 2.7476 Acc@1: 46.252% [11899/25728]\n",
      "| Epoch [118/200] Iter[221/391]\t\tLoss: 2.3998 Acc@1: 46.116% [13045/28288]\n",
      "| Epoch [118/200] Iter[241/391]\t\tLoss: 3.1149 Acc@1: 45.799% [14127/30848]\n",
      "| Epoch [118/200] Iter[261/391]\t\tLoss: 2.6980 Acc@1: 45.486% [15195/33408]\n",
      "| Epoch [118/200] Iter[281/391]\t\tLoss: 1.2188 Acc@1: 46.281% [16646/35968]\n",
      "| Epoch [118/200] Iter[301/391]\t\tLoss: 1.9918 Acc@1: 46.166% [17786/38528]\n",
      "| Epoch [118/200] Iter[321/391]\t\tLoss: 3.1090 Acc@1: 45.887% [18853/41088]\n",
      "| Epoch [118/200] Iter[341/391]\t\tLoss: 2.8820 Acc@1: 45.894% [20031/43648]\n",
      "| Epoch [118/200] Iter[361/391]\t\tLoss: 2.7250 Acc@1: 45.582% [21062/46208]\n",
      "| Epoch [118/200] Iter[381/391]\t\tLoss: 2.9530 Acc@1: 45.619% [22247/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #118\n",
      "\n",
      "| Validation Epoch #118\t\t\tLoss: 1.6511 Acc@1: 61.88%\n",
      "\n",
      "----- Elapsed time : 0:34:43\n",
      "\n",
      "\n",
      "=> Training Epoch #119\n",
      "| Epoch [119/200] Iter[  1/391]\t\tLoss: 3.1501 Acc@1: 25.261% [ 32/128]\n",
      "| Epoch [119/200] Iter[ 21/391]\t\tLoss: 2.5002 Acc@1: 44.127% [1186/2688]\n",
      "| Epoch [119/200] Iter[ 41/391]\t\tLoss: 3.2916 Acc@1: 45.126% [2368/5248]\n",
      "| Epoch [119/200] Iter[ 61/391]\t\tLoss: 1.9018 Acc@1: 46.178% [3605/7808]\n",
      "| Epoch [119/200] Iter[ 81/391]\t\tLoss: 1.8705 Acc@1: 46.902% [4862/10368]\n",
      "| Epoch [119/200] Iter[101/391]\t\tLoss: 2.0924 Acc@1: 47.017% [6078/12928]\n",
      "| Epoch [119/200] Iter[121/391]\t\tLoss: 1.4948 Acc@1: 47.115% [7297/15488]\n",
      "| Epoch [119/200] Iter[141/391]\t\tLoss: 2.7761 Acc@1: 47.536% [8579/18048]\n",
      "| Epoch [119/200] Iter[161/391]\t\tLoss: 1.7303 Acc@1: 46.941% [9673/20608]\n",
      "| Epoch [119/200] Iter[181/391]\t\tLoss: 2.2830 Acc@1: 46.742% [10829/23168]\n",
      "| Epoch [119/200] Iter[201/391]\t\tLoss: 2.5044 Acc@1: 46.729% [12022/25728]\n",
      "| Epoch [119/200] Iter[221/391]\t\tLoss: 2.9992 Acc@1: 46.466% [13144/28288]\n",
      "| Epoch [119/200] Iter[241/391]\t\tLoss: 3.3405 Acc@1: 45.902% [14159/30848]\n",
      "| Epoch [119/200] Iter[261/391]\t\tLoss: 1.6399 Acc@1: 45.994% [15365/33408]\n",
      "| Epoch [119/200] Iter[281/391]\t\tLoss: 2.7215 Acc@1: 45.569% [16390/35968]\n",
      "| Epoch [119/200] Iter[301/391]\t\tLoss: 2.1387 Acc@1: 45.515% [17536/38528]\n",
      "| Epoch [119/200] Iter[321/391]\t\tLoss: 3.0573 Acc@1: 45.393% [18650/41088]\n",
      "| Epoch [119/200] Iter[341/391]\t\tLoss: 1.9558 Acc@1: 45.109% [19688/43648]\n",
      "| Epoch [119/200] Iter[361/391]\t\tLoss: 1.6132 Acc@1: 44.892% [20743/46208]\n",
      "| Epoch [119/200] Iter[381/391]\t\tLoss: 2.6100 Acc@1: 44.779% [21837/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #119\n",
      "\n",
      "| Validation Epoch #119\t\t\tLoss: 1.4443 Acc@1: 61.04%\n",
      "\n",
      "----- Elapsed time : 0:35:01\n",
      "\n",
      "\n",
      "=> Training Epoch #120\n",
      "| Epoch [120/200] Iter[  1/391]\t\tLoss: 2.8849 Acc@1: 40.154% [ 51/128]\n",
      "| Epoch [120/200] Iter[ 21/391]\t\tLoss: 1.6294 Acc@1: 44.821% [1204/2688]\n",
      "| Epoch [120/200] Iter[ 41/391]\t\tLoss: 2.7949 Acc@1: 43.743% [2295/5248]\n",
      "| Epoch [120/200] Iter[ 61/391]\t\tLoss: 3.1222 Acc@1: 42.304% [3303/7808]\n",
      "| Epoch [120/200] Iter[ 81/391]\t\tLoss: 2.5909 Acc@1: 42.765% [4433/10368]\n",
      "| Epoch [120/200] Iter[101/391]\t\tLoss: 1.6190 Acc@1: 43.510% [5624/12928]\n",
      "| Epoch [120/200] Iter[121/391]\t\tLoss: 3.1148 Acc@1: 44.039% [6820/15488]\n",
      "| Epoch [120/200] Iter[141/391]\t\tLoss: 3.2179 Acc@1: 44.192% [7975/18048]\n",
      "| Epoch [120/200] Iter[161/391]\t\tLoss: 1.7714 Acc@1: 44.519% [9174/20608]\n",
      "| Epoch [120/200] Iter[181/391]\t\tLoss: 1.3811 Acc@1: 44.621% [10337/23168]\n",
      "| Epoch [120/200] Iter[201/391]\t\tLoss: 2.7156 Acc@1: 44.648% [11486/25728]\n",
      "| Epoch [120/200] Iter[221/391]\t\tLoss: 3.1006 Acc@1: 44.717% [12649/28288]\n",
      "| Epoch [120/200] Iter[241/391]\t\tLoss: 3.3230 Acc@1: 45.054% [13898/30848]\n",
      "| Epoch [120/200] Iter[261/391]\t\tLoss: 1.7574 Acc@1: 45.062% [15054/33408]\n",
      "| Epoch [120/200] Iter[281/391]\t\tLoss: 2.9300 Acc@1: 44.509% [16008/35968]\n",
      "| Epoch [120/200] Iter[301/391]\t\tLoss: 3.1811 Acc@1: 44.444% [17123/38528]\n",
      "| Epoch [120/200] Iter[321/391]\t\tLoss: 2.1697 Acc@1: 44.387% [18237/41088]\n",
      "| Epoch [120/200] Iter[341/391]\t\tLoss: 3.1417 Acc@1: 44.067% [19234/43648]\n",
      "| Epoch [120/200] Iter[361/391]\t\tLoss: 3.1831 Acc@1: 43.939% [20303/46208]\n",
      "| Epoch [120/200] Iter[381/391]\t\tLoss: 2.8826 Acc@1: 43.906% [21412/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #120\n",
      "\n",
      "| Validation Epoch #120\t\t\tLoss: 1.1887 Acc@1: 62.10%\n",
      "\n",
      "----- Elapsed time : 0:35:19\n",
      "\n",
      "\n",
      "=> Training Epoch #121\n",
      "| Epoch [121/200] Iter[  1/391]\t\tLoss: 1.4269 Acc@1: 72.701% [ 93/128]\n",
      "| Epoch [121/200] Iter[ 21/391]\t\tLoss: 1.2731 Acc@1: 46.029% [1237/2688]\n",
      "| Epoch [121/200] Iter[ 41/391]\t\tLoss: 1.4895 Acc@1: 45.716% [2399/5248]\n",
      "| Epoch [121/200] Iter[ 61/391]\t\tLoss: 2.3874 Acc@1: 46.035% [3594/7808]\n",
      "| Epoch [121/200] Iter[ 81/391]\t\tLoss: 3.1145 Acc@1: 46.336% [4804/10368]\n",
      "| Epoch [121/200] Iter[101/391]\t\tLoss: 2.0984 Acc@1: 45.903% [5934/12928]\n",
      "| Epoch [121/200] Iter[121/391]\t\tLoss: 3.4104 Acc@1: 46.214% [7157/15488]\n",
      "| Epoch [121/200] Iter[141/391]\t\tLoss: 2.7458 Acc@1: 45.637% [8236/18048]\n",
      "| Epoch [121/200] Iter[161/391]\t\tLoss: 1.9549 Acc@1: 45.845% [9447/20608]\n",
      "| Epoch [121/200] Iter[181/391]\t\tLoss: 2.5514 Acc@1: 45.821% [10615/23168]\n",
      "| Epoch [121/200] Iter[201/391]\t\tLoss: 2.6781 Acc@1: 46.124% [11866/25728]\n",
      "| Epoch [121/200] Iter[221/391]\t\tLoss: 2.1671 Acc@1: 46.103% [13041/28288]\n",
      "| Epoch [121/200] Iter[241/391]\t\tLoss: 2.3867 Acc@1: 46.353% [14298/30848]\n",
      "| Epoch [121/200] Iter[261/391]\t\tLoss: 1.5231 Acc@1: 46.506% [15536/33408]\n",
      "| Epoch [121/200] Iter[281/391]\t\tLoss: 3.1788 Acc@1: 46.387% [16684/35968]\n",
      "| Epoch [121/200] Iter[301/391]\t\tLoss: 3.2967 Acc@1: 46.406% [17879/38528]\n",
      "| Epoch [121/200] Iter[321/391]\t\tLoss: 1.4361 Acc@1: 46.467% [19092/41088]\n",
      "| Epoch [121/200] Iter[341/391]\t\tLoss: 2.2385 Acc@1: 46.597% [20338/43648]\n",
      "| Epoch [121/200] Iter[361/391]\t\tLoss: 1.3313 Acc@1: 46.775% [21613/46208]\n",
      "| Epoch [121/200] Iter[381/391]\t\tLoss: 2.3061 Acc@1: 46.775% [22811/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #121\n",
      "\n",
      "| Validation Epoch #121\t\t\tLoss: 1.5380 Acc@1: 61.47%\n",
      "\n",
      "----- Elapsed time : 0:35:38\n",
      "\n",
      "\n",
      "=> Training Epoch #122\n",
      "| Epoch [122/200] Iter[  1/391]\t\tLoss: 2.4500 Acc@1: 50.428% [ 64/128]\n",
      "| Epoch [122/200] Iter[ 21/391]\t\tLoss: 3.3338 Acc@1: 52.648% [1415/2688]\n",
      "| Epoch [122/200] Iter[ 41/391]\t\tLoss: 2.6424 Acc@1: 50.980% [2675/5248]\n",
      "| Epoch [122/200] Iter[ 61/391]\t\tLoss: 3.2420 Acc@1: 48.095% [3755/7808]\n",
      "| Epoch [122/200] Iter[ 81/391]\t\tLoss: 1.6333 Acc@1: 47.756% [4951/10368]\n",
      "| Epoch [122/200] Iter[101/391]\t\tLoss: 3.1993 Acc@1: 47.611% [6155/12928]\n",
      "| Epoch [122/200] Iter[121/391]\t\tLoss: 1.9214 Acc@1: 46.312% [7172/15488]\n",
      "| Epoch [122/200] Iter[141/391]\t\tLoss: 3.3258 Acc@1: 46.264% [8349/18048]\n",
      "| Epoch [122/200] Iter[161/391]\t\tLoss: 1.5794 Acc@1: 46.831% [9650/20608]\n",
      "| Epoch [122/200] Iter[181/391]\t\tLoss: 2.0561 Acc@1: 47.692% [11049/23168]\n",
      "| Epoch [122/200] Iter[201/391]\t\tLoss: 2.6852 Acc@1: 47.234% [12152/25728]\n",
      "| Epoch [122/200] Iter[221/391]\t\tLoss: 2.0048 Acc@1: 46.274% [13089/28288]\n",
      "| Epoch [122/200] Iter[241/391]\t\tLoss: 2.4807 Acc@1: 46.690% [14403/30848]\n",
      "| Epoch [122/200] Iter[261/391]\t\tLoss: 3.1566 Acc@1: 46.696% [15600/33408]\n",
      "| Epoch [122/200] Iter[281/391]\t\tLoss: 2.6394 Acc@1: 46.849% [16850/35968]\n",
      "| Epoch [122/200] Iter[301/391]\t\tLoss: 1.9954 Acc@1: 46.860% [18054/38528]\n",
      "| Epoch [122/200] Iter[321/391]\t\tLoss: 1.3039 Acc@1: 46.823% [19238/41088]\n",
      "| Epoch [122/200] Iter[341/391]\t\tLoss: 2.8218 Acc@1: 47.210% [20606/43648]\n",
      "| Epoch [122/200] Iter[361/391]\t\tLoss: 3.1174 Acc@1: 47.289% [21851/46208]\n",
      "| Epoch [122/200] Iter[381/391]\t\tLoss: 2.5394 Acc@1: 47.054% [22947/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #122\n",
      "\n",
      "| Validation Epoch #122\t\t\tLoss: 1.2132 Acc@1: 61.68%\n",
      "\n",
      "----- Elapsed time : 0:35:56\n",
      "\n",
      "\n",
      "=> Training Epoch #123\n",
      "| Epoch [123/200] Iter[  1/391]\t\tLoss: 1.7809 Acc@1: 61.823% [ 79/128]\n",
      "| Epoch [123/200] Iter[ 21/391]\t\tLoss: 3.3144 Acc@1: 47.740% [1283/2688]\n",
      "| Epoch [123/200] Iter[ 41/391]\t\tLoss: 1.6720 Acc@1: 47.378% [2486/5248]\n",
      "| Epoch [123/200] Iter[ 61/391]\t\tLoss: 3.3266 Acc@1: 46.080% [3597/7808]\n",
      "| Epoch [123/200] Iter[ 81/391]\t\tLoss: 2.3525 Acc@1: 47.168% [4890/10368]\n",
      "| Epoch [123/200] Iter[101/391]\t\tLoss: 3.1947 Acc@1: 46.316% [5987/12928]\n",
      "| Epoch [123/200] Iter[121/391]\t\tLoss: 2.6659 Acc@1: 45.852% [7101/15488]\n",
      "| Epoch [123/200] Iter[141/391]\t\tLoss: 1.0849 Acc@1: 46.163% [8331/18048]\n",
      "| Epoch [123/200] Iter[161/391]\t\tLoss: 3.3383 Acc@1: 45.671% [9411/20608]\n",
      "| Epoch [123/200] Iter[181/391]\t\tLoss: 1.6019 Acc@1: 46.003% [10658/23168]\n",
      "| Epoch [123/200] Iter[201/391]\t\tLoss: 1.7867 Acc@1: 45.881% [11804/25728]\n",
      "| Epoch [123/200] Iter[221/391]\t\tLoss: 1.3880 Acc@1: 45.416% [12847/28288]\n",
      "| Epoch [123/200] Iter[241/391]\t\tLoss: 2.3069 Acc@1: 45.659% [14084/30848]\n",
      "| Epoch [123/200] Iter[261/391]\t\tLoss: 3.2941 Acc@1: 45.579% [15226/33408]\n",
      "| Epoch [123/200] Iter[281/391]\t\tLoss: 1.9702 Acc@1: 45.485% [16359/35968]\n",
      "| Epoch [123/200] Iter[301/391]\t\tLoss: 2.9861 Acc@1: 45.351% [17472/38528]\n",
      "| Epoch [123/200] Iter[321/391]\t\tLoss: 2.9955 Acc@1: 45.621% [18744/41088]\n",
      "| Epoch [123/200] Iter[341/391]\t\tLoss: 1.6163 Acc@1: 45.508% [19863/43648]\n",
      "| Epoch [123/200] Iter[361/391]\t\tLoss: 3.0149 Acc@1: 45.423% [20988/46208]\n",
      "| Epoch [123/200] Iter[381/391]\t\tLoss: 2.2169 Acc@1: 45.555% [22216/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #123\n",
      "\n",
      "| Validation Epoch #123\t\t\tLoss: 1.3232 Acc@1: 61.28%\n",
      "\n",
      "----- Elapsed time : 0:36:15\n",
      "\n",
      "\n",
      "=> Training Epoch #124\n",
      "| Epoch [124/200] Iter[  1/391]\t\tLoss: 1.3872 Acc@1: 71.283% [ 91/128]\n",
      "| Epoch [124/200] Iter[ 21/391]\t\tLoss: 3.2986 Acc@1: 43.909% [1180/2688]\n",
      "| Epoch [124/200] Iter[ 41/391]\t\tLoss: 3.0352 Acc@1: 44.064% [2312/5248]\n",
      "| Epoch [124/200] Iter[ 61/391]\t\tLoss: 2.1428 Acc@1: 46.524% [3632/7808]\n",
      "| Epoch [124/200] Iter[ 81/391]\t\tLoss: 3.1625 Acc@1: 45.607% [4728/10368]\n",
      "| Epoch [124/200] Iter[101/391]\t\tLoss: 2.7798 Acc@1: 47.205% [6102/12928]\n",
      "| Epoch [124/200] Iter[121/391]\t\tLoss: 1.9587 Acc@1: 46.906% [7264/15488]\n",
      "| Epoch [124/200] Iter[141/391]\t\tLoss: 3.1687 Acc@1: 46.580% [8406/18048]\n",
      "| Epoch [124/200] Iter[161/391]\t\tLoss: 3.1171 Acc@1: 46.261% [9533/20608]\n",
      "| Epoch [124/200] Iter[181/391]\t\tLoss: 2.0732 Acc@1: 46.218% [10707/23168]\n",
      "| Epoch [124/200] Iter[201/391]\t\tLoss: 2.1100 Acc@1: 46.056% [11849/25728]\n",
      "| Epoch [124/200] Iter[221/391]\t\tLoss: 1.5375 Acc@1: 46.184% [13064/28288]\n",
      "| Epoch [124/200] Iter[241/391]\t\tLoss: 1.8082 Acc@1: 46.129% [14229/30848]\n",
      "| Epoch [124/200] Iter[261/391]\t\tLoss: 3.1620 Acc@1: 46.127% [15410/33408]\n",
      "| Epoch [124/200] Iter[281/391]\t\tLoss: 2.9764 Acc@1: 45.950% [16527/35968]\n",
      "| Epoch [124/200] Iter[301/391]\t\tLoss: 3.2721 Acc@1: 46.033% [17735/38528]\n",
      "| Epoch [124/200] Iter[321/391]\t\tLoss: 3.3457 Acc@1: 45.832% [18831/41088]\n",
      "| Epoch [124/200] Iter[341/391]\t\tLoss: 2.2960 Acc@1: 45.829% [20003/43648]\n",
      "| Epoch [124/200] Iter[361/391]\t\tLoss: 1.9380 Acc@1: 45.822% [21173/46208]\n",
      "| Epoch [124/200] Iter[381/391]\t\tLoss: 2.0919 Acc@1: 45.722% [22297/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #124\n",
      "\n",
      "| Validation Epoch #124\t\t\tLoss: 1.4250 Acc@1: 61.32%\n",
      "\n",
      "----- Elapsed time : 0:36:34\n",
      "\n",
      "\n",
      "=> Training Epoch #125\n",
      "| Epoch [125/200] Iter[  1/391]\t\tLoss: 2.9954 Acc@1: 33.694% [ 43/128]\n",
      "| Epoch [125/200] Iter[ 21/391]\t\tLoss: 3.0603 Acc@1: 42.760% [1149/2688]\n",
      "| Epoch [125/200] Iter[ 41/391]\t\tLoss: 2.5031 Acc@1: 46.122% [2420/5248]\n",
      "| Epoch [125/200] Iter[ 61/391]\t\tLoss: 2.0921 Acc@1: 46.477% [3628/7808]\n",
      "| Epoch [125/200] Iter[ 81/391]\t\tLoss: 2.9704 Acc@1: 47.073% [4880/10368]\n",
      "| Epoch [125/200] Iter[101/391]\t\tLoss: 2.5949 Acc@1: 46.734% [6041/12928]\n",
      "| Epoch [125/200] Iter[121/391]\t\tLoss: 2.7479 Acc@1: 46.058% [7133/15488]\n",
      "| Epoch [125/200] Iter[141/391]\t\tLoss: 3.2954 Acc@1: 45.763% [8259/18048]\n",
      "| Epoch [125/200] Iter[161/391]\t\tLoss: 2.7012 Acc@1: 45.130% [9300/20608]\n",
      "| Epoch [125/200] Iter[181/391]\t\tLoss: 1.9488 Acc@1: 45.301% [10495/23168]\n",
      "| Epoch [125/200] Iter[201/391]\t\tLoss: 2.9074 Acc@1: 45.279% [11649/25728]\n",
      "| Epoch [125/200] Iter[221/391]\t\tLoss: 1.4720 Acc@1: 45.474% [12863/28288]\n",
      "| Epoch [125/200] Iter[241/391]\t\tLoss: 2.5720 Acc@1: 45.825% [14136/30848]\n",
      "| Epoch [125/200] Iter[261/391]\t\tLoss: 2.5028 Acc@1: 45.465% [15188/33408]\n",
      "| Epoch [125/200] Iter[281/391]\t\tLoss: 3.0490 Acc@1: 45.540% [16379/35968]\n",
      "| Epoch [125/200] Iter[301/391]\t\tLoss: 1.5823 Acc@1: 45.958% [17706/38528]\n",
      "| Epoch [125/200] Iter[321/391]\t\tLoss: 3.2447 Acc@1: 45.863% [18844/41088]\n",
      "| Epoch [125/200] Iter[341/391]\t\tLoss: 2.7041 Acc@1: 45.646% [19923/43648]\n",
      "| Epoch [125/200] Iter[361/391]\t\tLoss: 1.8570 Acc@1: 45.537% [21041/46208]\n",
      "| Epoch [125/200] Iter[381/391]\t\tLoss: 1.2668 Acc@1: 45.671% [22272/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #125\n",
      "\n",
      "| Validation Epoch #125\t\t\tLoss: 1.4821 Acc@1: 61.82%\n",
      "\n",
      "----- Elapsed time : 0:36:52\n",
      "\n",
      "\n",
      "=> Training Epoch #126\n",
      "| Epoch [126/200] Iter[  1/391]\t\tLoss: 1.5587 Acc@1: 69.269% [ 88/128]\n",
      "| Epoch [126/200] Iter[ 21/391]\t\tLoss: 3.2100 Acc@1: 47.366% [1273/2688]\n",
      "| Epoch [126/200] Iter[ 41/391]\t\tLoss: 1.7861 Acc@1: 45.009% [2362/5248]\n",
      "| Epoch [126/200] Iter[ 61/391]\t\tLoss: 2.0401 Acc@1: 48.786% [3809/7808]\n",
      "| Epoch [126/200] Iter[ 81/391]\t\tLoss: 2.7977 Acc@1: 47.779% [4953/10368]\n",
      "| Epoch [126/200] Iter[101/391]\t\tLoss: 1.8708 Acc@1: 46.564% [6019/12928]\n",
      "| Epoch [126/200] Iter[121/391]\t\tLoss: 2.1582 Acc@1: 46.259% [7164/15488]\n",
      "| Epoch [126/200] Iter[141/391]\t\tLoss: 2.6080 Acc@1: 46.020% [8305/18048]\n",
      "| Epoch [126/200] Iter[161/391]\t\tLoss: 2.9193 Acc@1: 46.510% [9584/20608]\n",
      "| Epoch [126/200] Iter[181/391]\t\tLoss: 1.2441 Acc@1: 47.012% [10891/23168]\n",
      "| Epoch [126/200] Iter[201/391]\t\tLoss: 3.2750 Acc@1: 47.075% [12111/25728]\n",
      "| Epoch [126/200] Iter[221/391]\t\tLoss: 1.2114 Acc@1: 47.316% [13384/28288]\n",
      "| Epoch [126/200] Iter[241/391]\t\tLoss: 1.5216 Acc@1: 47.091% [14526/30848]\n",
      "| Epoch [126/200] Iter[261/391]\t\tLoss: 2.1284 Acc@1: 47.299% [15801/33408]\n",
      "| Epoch [126/200] Iter[281/391]\t\tLoss: 1.2857 Acc@1: 46.937% [16882/35968]\n",
      "| Epoch [126/200] Iter[301/391]\t\tLoss: 2.9683 Acc@1: 46.631% [17966/38528]\n",
      "| Epoch [126/200] Iter[321/391]\t\tLoss: 2.9156 Acc@1: 46.430% [19077/41088]\n",
      "| Epoch [126/200] Iter[341/391]\t\tLoss: 3.1546 Acc@1: 46.339% [20226/43648]\n",
      "| Epoch [126/200] Iter[361/391]\t\tLoss: 1.8884 Acc@1: 46.481% [21477/46208]\n",
      "| Epoch [126/200] Iter[381/391]\t\tLoss: 3.2309 Acc@1: 46.311% [22584/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #126\n",
      "\n",
      "| Validation Epoch #126\t\t\tLoss: 1.2233 Acc@1: 62.04%\n",
      "\n",
      "----- Elapsed time : 0:37:10\n",
      "\n",
      "\n",
      "=> Training Epoch #127\n",
      "| Epoch [127/200] Iter[  1/391]\t\tLoss: 3.3021 Acc@1: 25.368% [ 32/128]\n",
      "| Epoch [127/200] Iter[ 21/391]\t\tLoss: 3.2336 Acc@1: 42.182% [1133/2688]\n",
      "| Epoch [127/200] Iter[ 41/391]\t\tLoss: 2.2719 Acc@1: 44.901% [2356/5248]\n",
      "| Epoch [127/200] Iter[ 61/391]\t\tLoss: 3.1751 Acc@1: 45.583% [3559/7808]\n",
      "| Epoch [127/200] Iter[ 81/391]\t\tLoss: 2.9610 Acc@1: 44.912% [4656/10368]\n",
      "| Epoch [127/200] Iter[101/391]\t\tLoss: 3.1544 Acc@1: 44.990% [5816/12928]\n",
      "| Epoch [127/200] Iter[121/391]\t\tLoss: 2.0543 Acc@1: 44.918% [6956/15488]\n",
      "| Epoch [127/200] Iter[141/391]\t\tLoss: 2.2505 Acc@1: 44.759% [8078/18048]\n",
      "| Epoch [127/200] Iter[161/391]\t\tLoss: 2.8543 Acc@1: 44.761% [9224/20608]\n",
      "| Epoch [127/200] Iter[181/391]\t\tLoss: 2.5500 Acc@1: 44.673% [10349/23168]\n",
      "| Epoch [127/200] Iter[201/391]\t\tLoss: 2.3036 Acc@1: 44.568% [11466/25728]\n",
      "| Epoch [127/200] Iter[221/391]\t\tLoss: 2.9856 Acc@1: 44.613% [12620/28288]\n",
      "| Epoch [127/200] Iter[241/391]\t\tLoss: 2.0590 Acc@1: 44.817% [13825/30848]\n",
      "| Epoch [127/200] Iter[261/391]\t\tLoss: 3.3385 Acc@1: 44.989% [15029/33408]\n",
      "| Epoch [127/200] Iter[281/391]\t\tLoss: 1.7844 Acc@1: 45.415% [16334/35968]\n",
      "| Epoch [127/200] Iter[301/391]\t\tLoss: 1.9364 Acc@1: 45.408% [17494/38528]\n",
      "| Epoch [127/200] Iter[321/391]\t\tLoss: 2.3769 Acc@1: 45.140% [18547/41088]\n",
      "| Epoch [127/200] Iter[341/391]\t\tLoss: 1.6234 Acc@1: 45.580% [19894/43648]\n",
      "| Epoch [127/200] Iter[361/391]\t\tLoss: 3.3587 Acc@1: 45.646% [21092/46208]\n",
      "| Epoch [127/200] Iter[381/391]\t\tLoss: 3.0680 Acc@1: 45.451% [22165/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #127\n",
      "\n",
      "| Validation Epoch #127\t\t\tLoss: 1.1126 Acc@1: 62.81%\n",
      "\n",
      "----- Elapsed time : 0:37:29\n",
      "\n",
      "\n",
      "=> Training Epoch #128\n",
      "| Epoch [128/200] Iter[  1/391]\t\tLoss: 3.0064 Acc@1: 37.283% [ 47/128]\n",
      "| Epoch [128/200] Iter[ 21/391]\t\tLoss: 2.9468 Acc@1: 45.436% [1221/2688]\n",
      "| Epoch [128/200] Iter[ 41/391]\t\tLoss: 2.8391 Acc@1: 45.494% [2387/5248]\n",
      "| Epoch [128/200] Iter[ 61/391]\t\tLoss: 2.9078 Acc@1: 47.960% [3744/7808]\n",
      "| Epoch [128/200] Iter[ 81/391]\t\tLoss: 2.6729 Acc@1: 45.670% [4735/10368]\n",
      "| Epoch [128/200] Iter[101/391]\t\tLoss: 2.6000 Acc@1: 45.064% [5825/12928]\n",
      "| Epoch [128/200] Iter[121/391]\t\tLoss: 2.7226 Acc@1: 45.799% [7093/15488]\n",
      "| Epoch [128/200] Iter[141/391]\t\tLoss: 1.6955 Acc@1: 45.702% [8248/18048]\n",
      "| Epoch [128/200] Iter[161/391]\t\tLoss: 1.4574 Acc@1: 45.941% [9467/20608]\n",
      "| Epoch [128/200] Iter[181/391]\t\tLoss: 1.7798 Acc@1: 46.446% [10760/23168]\n",
      "| Epoch [128/200] Iter[201/391]\t\tLoss: 1.3278 Acc@1: 46.437% [11947/25728]\n",
      "| Epoch [128/200] Iter[221/391]\t\tLoss: 2.9223 Acc@1: 46.436% [13135/28288]\n",
      "| Epoch [128/200] Iter[241/391]\t\tLoss: 2.8770 Acc@1: 46.298% [14282/30848]\n",
      "| Epoch [128/200] Iter[261/391]\t\tLoss: 3.3242 Acc@1: 46.153% [15418/33408]\n",
      "| Epoch [128/200] Iter[281/391]\t\tLoss: 2.5375 Acc@1: 46.304% [16654/35968]\n",
      "| Epoch [128/200] Iter[301/391]\t\tLoss: 1.9482 Acc@1: 46.315% [17844/38528]\n",
      "| Epoch [128/200] Iter[321/391]\t\tLoss: 2.1724 Acc@1: 46.287% [19018/41088]\n",
      "| Epoch [128/200] Iter[341/391]\t\tLoss: 1.9078 Acc@1: 46.460% [20278/43648]\n",
      "| Epoch [128/200] Iter[361/391]\t\tLoss: 1.5007 Acc@1: 46.503% [21487/46208]\n",
      "| Epoch [128/200] Iter[381/391]\t\tLoss: 1.5376 Acc@1: 46.467% [22660/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #128\n",
      "\n",
      "| Validation Epoch #128\t\t\tLoss: 1.5004 Acc@1: 62.75%\n",
      "\n",
      "----- Elapsed time : 0:37:48\n",
      "\n",
      "\n",
      "=> Training Epoch #129\n",
      "| Epoch [129/200] Iter[  1/391]\t\tLoss: 2.0112 Acc@1: 63.651% [ 81/128]\n",
      "| Epoch [129/200] Iter[ 21/391]\t\tLoss: 1.5328 Acc@1: 52.696% [1416/2688]\n",
      "| Epoch [129/200] Iter[ 41/391]\t\tLoss: 3.3841 Acc@1: 49.446% [2594/5248]\n",
      "| Epoch [129/200] Iter[ 61/391]\t\tLoss: 1.8886 Acc@1: 49.237% [3844/7808]\n",
      "| Epoch [129/200] Iter[ 81/391]\t\tLoss: 1.7827 Acc@1: 49.074% [5087/10368]\n",
      "| Epoch [129/200] Iter[101/391]\t\tLoss: 3.0633 Acc@1: 48.247% [6237/12928]\n",
      "| Epoch [129/200] Iter[121/391]\t\tLoss: 1.8119 Acc@1: 46.371% [7181/15488]\n",
      "| Epoch [129/200] Iter[141/391]\t\tLoss: 1.4254 Acc@1: 46.996% [8481/18048]\n",
      "| Epoch [129/200] Iter[161/391]\t\tLoss: 2.7818 Acc@1: 46.927% [9670/20608]\n",
      "| Epoch [129/200] Iter[181/391]\t\tLoss: 2.4870 Acc@1: 46.914% [10868/23168]\n",
      "| Epoch [129/200] Iter[201/391]\t\tLoss: 1.5037 Acc@1: 47.235% [12152/25728]\n",
      "| Epoch [129/200] Iter[221/391]\t\tLoss: 1.3297 Acc@1: 48.031% [13587/28288]\n",
      "| Epoch [129/200] Iter[241/391]\t\tLoss: 2.8459 Acc@1: 47.758% [14732/30848]\n",
      "| Epoch [129/200] Iter[261/391]\t\tLoss: 2.9052 Acc@1: 47.801% [15969/33408]\n",
      "| Epoch [129/200] Iter[281/391]\t\tLoss: 1.7417 Acc@1: 47.792% [17189/35968]\n",
      "| Epoch [129/200] Iter[301/391]\t\tLoss: 3.1991 Acc@1: 47.670% [18366/38528]\n",
      "| Epoch [129/200] Iter[321/391]\t\tLoss: 2.5461 Acc@1: 47.407% [19478/41088]\n",
      "| Epoch [129/200] Iter[341/391]\t\tLoss: 2.6408 Acc@1: 47.285% [20638/43648]\n",
      "| Epoch [129/200] Iter[361/391]\t\tLoss: 1.3834 Acc@1: 47.018% [21725/46208]\n",
      "| Epoch [129/200] Iter[381/391]\t\tLoss: 1.5584 Acc@1: 46.929% [22886/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #129\n",
      "\n",
      "| Validation Epoch #129\t\t\tLoss: 1.2416 Acc@1: 63.05%\n",
      "\n",
      "----- Elapsed time : 0:38:06\n",
      "\n",
      "\n",
      "=> Training Epoch #130\n",
      "| Epoch [130/200] Iter[  1/391]\t\tLoss: 2.4163 Acc@1: 53.876% [ 68/128]\n",
      "| Epoch [130/200] Iter[ 21/391]\t\tLoss: 2.5964 Acc@1: 43.669% [1173/2688]\n",
      "| Epoch [130/200] Iter[ 41/391]\t\tLoss: 1.5717 Acc@1: 46.554% [2443/5248]\n",
      "| Epoch [130/200] Iter[ 61/391]\t\tLoss: 2.6169 Acc@1: 45.560% [3557/7808]\n",
      "| Epoch [130/200] Iter[ 81/391]\t\tLoss: 1.5775 Acc@1: 45.121% [4678/10368]\n",
      "| Epoch [130/200] Iter[101/391]\t\tLoss: 2.3983 Acc@1: 45.285% [5854/12928]\n",
      "| Epoch [130/200] Iter[121/391]\t\tLoss: 2.4933 Acc@1: 46.674% [7228/15488]\n",
      "| Epoch [130/200] Iter[141/391]\t\tLoss: 2.9091 Acc@1: 46.867% [8458/18048]\n",
      "| Epoch [130/200] Iter[161/391]\t\tLoss: 2.6465 Acc@1: 46.691% [9622/20608]\n",
      "| Epoch [130/200] Iter[181/391]\t\tLoss: 3.1384 Acc@1: 46.591% [10794/23168]\n",
      "| Epoch [130/200] Iter[201/391]\t\tLoss: 2.2830 Acc@1: 46.814% [12044/25728]\n",
      "| Epoch [130/200] Iter[221/391]\t\tLoss: 3.2106 Acc@1: 47.122% [13329/28288]\n",
      "| Epoch [130/200] Iter[241/391]\t\tLoss: 3.1601 Acc@1: 46.991% [14495/30848]\n",
      "| Epoch [130/200] Iter[261/391]\t\tLoss: 1.5126 Acc@1: 47.090% [15731/33408]\n",
      "| Epoch [130/200] Iter[281/391]\t\tLoss: 2.0278 Acc@1: 47.110% [16944/35968]\n",
      "| Epoch [130/200] Iter[301/391]\t\tLoss: 1.7019 Acc@1: 46.909% [18073/38528]\n",
      "| Epoch [130/200] Iter[321/391]\t\tLoss: 2.7956 Acc@1: 46.829% [19241/41088]\n",
      "| Epoch [130/200] Iter[341/391]\t\tLoss: 3.2934 Acc@1: 46.735% [20398/43648]\n",
      "| Epoch [130/200] Iter[361/391]\t\tLoss: 2.6913 Acc@1: 46.683% [21571/46208]\n",
      "| Epoch [130/200] Iter[381/391]\t\tLoss: 3.1500 Acc@1: 46.545% [22698/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #130\n",
      "\n",
      "| Validation Epoch #130\t\t\tLoss: 1.3035 Acc@1: 62.94%\n",
      "\n",
      "----- Elapsed time : 0:38:25\n",
      "\n",
      "\n",
      "=> Training Epoch #131\n",
      "| Epoch [131/200] Iter[  1/391]\t\tLoss: 1.5283 Acc@1: 60.310% [ 77/128]\n",
      "| Epoch [131/200] Iter[ 21/391]\t\tLoss: 1.5988 Acc@1: 46.404% [1247/2688]\n",
      "| Epoch [131/200] Iter[ 41/391]\t\tLoss: 1.9006 Acc@1: 44.331% [2326/5248]\n",
      "| Epoch [131/200] Iter[ 61/391]\t\tLoss: 2.9435 Acc@1: 43.561% [3401/7808]\n",
      "| Epoch [131/200] Iter[ 81/391]\t\tLoss: 2.6823 Acc@1: 44.165% [4579/10368]\n",
      "| Epoch [131/200] Iter[101/391]\t\tLoss: 3.1588 Acc@1: 44.731% [5782/12928]\n",
      "| Epoch [131/200] Iter[121/391]\t\tLoss: 1.7071 Acc@1: 45.628% [7066/15488]\n",
      "| Epoch [131/200] Iter[141/391]\t\tLoss: 3.0655 Acc@1: 45.523% [8215/18048]\n",
      "| Epoch [131/200] Iter[161/391]\t\tLoss: 2.3911 Acc@1: 46.321% [9545/20608]\n",
      "| Epoch [131/200] Iter[181/391]\t\tLoss: 2.9728 Acc@1: 46.864% [10857/23168]\n",
      "| Epoch [131/200] Iter[201/391]\t\tLoss: 3.1705 Acc@1: 47.309% [12171/25728]\n",
      "| Epoch [131/200] Iter[221/391]\t\tLoss: 1.3701 Acc@1: 47.335% [13390/28288]\n",
      "| Epoch [131/200] Iter[241/391]\t\tLoss: 2.7573 Acc@1: 47.410% [14625/30848]\n",
      "| Epoch [131/200] Iter[261/391]\t\tLoss: 2.9842 Acc@1: 47.138% [15748/33408]\n",
      "| Epoch [131/200] Iter[281/391]\t\tLoss: 2.4466 Acc@1: 47.014% [16910/35968]\n",
      "| Epoch [131/200] Iter[301/391]\t\tLoss: 3.0110 Acc@1: 46.512% [17920/38528]\n",
      "| Epoch [131/200] Iter[321/391]\t\tLoss: 2.9032 Acc@1: 46.698% [19187/41088]\n",
      "| Epoch [131/200] Iter[341/391]\t\tLoss: 1.9530 Acc@1: 46.525% [20307/43648]\n",
      "| Epoch [131/200] Iter[361/391]\t\tLoss: 3.1129 Acc@1: 46.436% [21456/46208]\n",
      "| Epoch [131/200] Iter[381/391]\t\tLoss: 2.7105 Acc@1: 46.692% [22770/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #131\n",
      "\n",
      "| Validation Epoch #131\t\t\tLoss: 1.4300 Acc@1: 62.68%\n",
      "\n",
      "----- Elapsed time : 0:38:43\n",
      "\n",
      "\n",
      "=> Training Epoch #132\n",
      "| Epoch [132/200] Iter[  1/391]\t\tLoss: 3.1425 Acc@1: 29.579% [ 37/128]\n",
      "| Epoch [132/200] Iter[ 21/391]\t\tLoss: 3.1108 Acc@1: 45.258% [1216/2688]\n",
      "| Epoch [132/200] Iter[ 41/391]\t\tLoss: 1.8168 Acc@1: 48.226% [2530/5248]\n",
      "| Epoch [132/200] Iter[ 61/391]\t\tLoss: 1.8879 Acc@1: 48.420% [3780/7808]\n",
      "| Epoch [132/200] Iter[ 81/391]\t\tLoss: 3.0772 Acc@1: 49.296% [5111/10368]\n",
      "| Epoch [132/200] Iter[101/391]\t\tLoss: 2.2798 Acc@1: 48.725% [6299/12928]\n",
      "| Epoch [132/200] Iter[121/391]\t\tLoss: 1.6745 Acc@1: 48.021% [7437/15488]\n",
      "| Epoch [132/200] Iter[141/391]\t\tLoss: 3.3436 Acc@1: 48.055% [8672/18048]\n",
      "| Epoch [132/200] Iter[161/391]\t\tLoss: 2.9494 Acc@1: 48.275% [9948/20608]\n",
      "| Epoch [132/200] Iter[181/391]\t\tLoss: 3.2498 Acc@1: 47.816% [11077/23168]\n",
      "| Epoch [132/200] Iter[201/391]\t\tLoss: 2.9748 Acc@1: 47.563% [12236/25728]\n",
      "| Epoch [132/200] Iter[221/391]\t\tLoss: 1.4368 Acc@1: 47.426% [13415/28288]\n",
      "| Epoch [132/200] Iter[241/391]\t\tLoss: 1.6397 Acc@1: 47.253% [14576/30848]\n",
      "| Epoch [132/200] Iter[261/391]\t\tLoss: 1.5610 Acc@1: 47.197% [15767/33408]\n",
      "| Epoch [132/200] Iter[281/391]\t\tLoss: 1.7994 Acc@1: 46.856% [16853/35968]\n",
      "| Epoch [132/200] Iter[301/391]\t\tLoss: 3.0294 Acc@1: 46.209% [17803/38528]\n",
      "| Epoch [132/200] Iter[321/391]\t\tLoss: 1.8850 Acc@1: 46.273% [19012/41088]\n",
      "| Epoch [132/200] Iter[341/391]\t\tLoss: 1.7195 Acc@1: 46.302% [20210/43648]\n",
      "| Epoch [132/200] Iter[361/391]\t\tLoss: 2.0955 Acc@1: 46.110% [21306/46208]\n",
      "| Epoch [132/200] Iter[381/391]\t\tLoss: 3.1107 Acc@1: 46.330% [22594/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #132\n",
      "\n",
      "| Validation Epoch #132\t\t\tLoss: 1.2779 Acc@1: 62.53%\n",
      "\n",
      "----- Elapsed time : 0:39:02\n",
      "\n",
      "\n",
      "=> Training Epoch #133\n",
      "| Epoch [133/200] Iter[  1/391]\t\tLoss: 2.9457 Acc@1: 28.999% [ 37/128]\n",
      "| Epoch [133/200] Iter[ 21/391]\t\tLoss: 3.0146 Acc@1: 39.322% [1056/2688]\n",
      "| Epoch [133/200] Iter[ 41/391]\t\tLoss: 2.8061 Acc@1: 41.903% [2199/5248]\n",
      "| Epoch [133/200] Iter[ 61/391]\t\tLoss: 1.4009 Acc@1: 45.198% [3529/7808]\n",
      "| Epoch [133/200] Iter[ 81/391]\t\tLoss: 1.3890 Acc@1: 46.410% [4811/10368]\n",
      "| Epoch [133/200] Iter[101/391]\t\tLoss: 1.2497 Acc@1: 46.994% [6075/12928]\n",
      "| Epoch [133/200] Iter[121/391]\t\tLoss: 3.1405 Acc@1: 45.688% [7076/15488]\n",
      "| Epoch [133/200] Iter[141/391]\t\tLoss: 1.8836 Acc@1: 46.145% [8328/18048]\n",
      "| Epoch [133/200] Iter[161/391]\t\tLoss: 2.7692 Acc@1: 47.065% [9699/20608]\n",
      "| Epoch [133/200] Iter[181/391]\t\tLoss: 2.3711 Acc@1: 47.610% [11030/23168]\n",
      "| Epoch [133/200] Iter[201/391]\t\tLoss: 3.1759 Acc@1: 47.575% [12240/25728]\n",
      "| Epoch [133/200] Iter[221/391]\t\tLoss: 3.0840 Acc@1: 47.648% [13478/28288]\n",
      "| Epoch [133/200] Iter[241/391]\t\tLoss: 2.1622 Acc@1: 48.040% [14819/30848]\n",
      "| Epoch [133/200] Iter[261/391]\t\tLoss: 3.4056 Acc@1: 47.716% [15940/33408]\n",
      "| Epoch [133/200] Iter[281/391]\t\tLoss: 2.3751 Acc@1: 47.578% [17112/35968]\n",
      "| Epoch [133/200] Iter[301/391]\t\tLoss: 2.6746 Acc@1: 47.540% [18316/38528]\n",
      "| Epoch [133/200] Iter[321/391]\t\tLoss: 3.0964 Acc@1: 47.453% [19497/41088]\n",
      "| Epoch [133/200] Iter[341/391]\t\tLoss: 3.2687 Acc@1: 47.235% [20616/43648]\n",
      "| Epoch [133/200] Iter[361/391]\t\tLoss: 1.8954 Acc@1: 47.610% [21999/46208]\n",
      "| Epoch [133/200] Iter[381/391]\t\tLoss: 2.3393 Acc@1: 47.482% [23156/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #133\n",
      "\n",
      "| Validation Epoch #133\t\t\tLoss: 1.2919 Acc@1: 62.62%\n",
      "\n",
      "----- Elapsed time : 0:39:20\n",
      "\n",
      "\n",
      "=> Training Epoch #134\n",
      "| Epoch [134/200] Iter[  1/391]\t\tLoss: 3.3345 Acc@1: 26.301% [ 33/128]\n",
      "| Epoch [134/200] Iter[ 21/391]\t\tLoss: 2.2598 Acc@1: 43.796% [1177/2688]\n",
      "| Epoch [134/200] Iter[ 41/391]\t\tLoss: 3.1865 Acc@1: 42.812% [2246/5248]\n",
      "| Epoch [134/200] Iter[ 61/391]\t\tLoss: 3.1270 Acc@1: 43.945% [3431/7808]\n",
      "| Epoch [134/200] Iter[ 81/391]\t\tLoss: 2.2827 Acc@1: 44.033% [4565/10368]\n",
      "| Epoch [134/200] Iter[101/391]\t\tLoss: 1.9678 Acc@1: 43.769% [5658/12928]\n",
      "| Epoch [134/200] Iter[121/391]\t\tLoss: 1.3732 Acc@1: 44.670% [6918/15488]\n",
      "| Epoch [134/200] Iter[141/391]\t\tLoss: 1.2852 Acc@1: 45.389% [8191/18048]\n",
      "| Epoch [134/200] Iter[161/391]\t\tLoss: 3.0113 Acc@1: 44.772% [9226/20608]\n",
      "| Epoch [134/200] Iter[181/391]\t\tLoss: 2.4234 Acc@1: 45.177% [10466/23168]\n",
      "| Epoch [134/200] Iter[201/391]\t\tLoss: 1.7677 Acc@1: 45.071% [11595/25728]\n",
      "| Epoch [134/200] Iter[221/391]\t\tLoss: 2.2752 Acc@1: 44.695% [12643/28288]\n",
      "| Epoch [134/200] Iter[241/391]\t\tLoss: 2.9021 Acc@1: 44.256% [13651/30848]\n",
      "| Epoch [134/200] Iter[261/391]\t\tLoss: 2.7113 Acc@1: 44.582% [14894/33408]\n",
      "| Epoch [134/200] Iter[281/391]\t\tLoss: 1.7764 Acc@1: 44.672% [16067/35968]\n",
      "| Epoch [134/200] Iter[301/391]\t\tLoss: 2.7000 Acc@1: 44.774% [17250/38528]\n",
      "| Epoch [134/200] Iter[321/391]\t\tLoss: 3.1166 Acc@1: 44.801% [18407/41088]\n",
      "| Epoch [134/200] Iter[341/391]\t\tLoss: 3.3577 Acc@1: 45.113% [19690/43648]\n",
      "| Epoch [134/200] Iter[361/391]\t\tLoss: 2.1854 Acc@1: 45.566% [21055/46208]\n",
      "| Epoch [134/200] Iter[381/391]\t\tLoss: 2.0034 Acc@1: 45.598% [22237/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #134\n",
      "\n",
      "| Validation Epoch #134\t\t\tLoss: 1.0535 Acc@1: 63.21%\n",
      "\n",
      "----- Elapsed time : 0:39:39\n",
      "\n",
      "\n",
      "=> Training Epoch #135\n",
      "| Epoch [135/200] Iter[  1/391]\t\tLoss: 2.6253 Acc@1: 46.974% [ 60/128]\n",
      "| Epoch [135/200] Iter[ 21/391]\t\tLoss: 3.2116 Acc@1: 41.519% [1116/2688]\n",
      "| Epoch [135/200] Iter[ 41/391]\t\tLoss: 2.9453 Acc@1: 46.754% [2453/5248]\n",
      "| Epoch [135/200] Iter[ 61/391]\t\tLoss: 2.6460 Acc@1: 47.514% [3709/7808]\n",
      "| Epoch [135/200] Iter[ 81/391]\t\tLoss: 1.8430 Acc@1: 47.031% [4876/10368]\n",
      "| Epoch [135/200] Iter[101/391]\t\tLoss: 2.6773 Acc@1: 47.115% [6091/12928]\n",
      "| Epoch [135/200] Iter[121/391]\t\tLoss: 2.2376 Acc@1: 47.718% [7390/15488]\n",
      "| Epoch [135/200] Iter[141/391]\t\tLoss: 1.2965 Acc@1: 48.395% [8734/18048]\n",
      "| Epoch [135/200] Iter[161/391]\t\tLoss: 2.0168 Acc@1: 48.272% [9947/20608]\n",
      "| Epoch [135/200] Iter[181/391]\t\tLoss: 2.5326 Acc@1: 49.059% [11365/23168]\n",
      "| Epoch [135/200] Iter[201/391]\t\tLoss: 3.0183 Acc@1: 48.879% [12575/25728]\n",
      "| Epoch [135/200] Iter[221/391]\t\tLoss: 1.8184 Acc@1: 48.512% [13723/28288]\n",
      "| Epoch [135/200] Iter[241/391]\t\tLoss: 2.9820 Acc@1: 48.411% [14933/30848]\n",
      "| Epoch [135/200] Iter[261/391]\t\tLoss: 2.6259 Acc@1: 47.777% [15961/33408]\n",
      "| Epoch [135/200] Iter[281/391]\t\tLoss: 2.3989 Acc@1: 47.557% [17105/35968]\n",
      "| Epoch [135/200] Iter[301/391]\t\tLoss: 3.1543 Acc@1: 47.299% [18223/38528]\n",
      "| Epoch [135/200] Iter[321/391]\t\tLoss: 1.1593 Acc@1: 47.135% [19366/41088]\n",
      "| Epoch [135/200] Iter[341/391]\t\tLoss: 3.0067 Acc@1: 47.242% [20620/43648]\n",
      "| Epoch [135/200] Iter[361/391]\t\tLoss: 1.5537 Acc@1: 47.464% [21932/46208]\n",
      "| Epoch [135/200] Iter[381/391]\t\tLoss: 2.0249 Acc@1: 47.522% [23175/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #135\n",
      "\n",
      "| Validation Epoch #135\t\t\tLoss: 1.4893 Acc@1: 62.70%\n",
      "\n",
      "----- Elapsed time : 0:39:57\n",
      "\n",
      "\n",
      "=> Training Epoch #136\n",
      "| Epoch [136/200] Iter[  1/391]\t\tLoss: 2.4775 Acc@1: 49.733% [ 63/128]\n",
      "| Epoch [136/200] Iter[ 21/391]\t\tLoss: 2.8112 Acc@1: 42.925% [1153/2688]\n",
      "| Epoch [136/200] Iter[ 41/391]\t\tLoss: 2.3408 Acc@1: 45.851% [2406/5248]\n",
      "| Epoch [136/200] Iter[ 61/391]\t\tLoss: 2.7504 Acc@1: 48.347% [3774/7808]\n",
      "| Epoch [136/200] Iter[ 81/391]\t\tLoss: 1.9682 Acc@1: 47.125% [4885/10368]\n",
      "| Epoch [136/200] Iter[101/391]\t\tLoss: 1.1615 Acc@1: 48.969% [6330/12928]\n",
      "| Epoch [136/200] Iter[121/391]\t\tLoss: 1.6254 Acc@1: 48.600% [7527/15488]\n",
      "| Epoch [136/200] Iter[141/391]\t\tLoss: 3.3944 Acc@1: 47.639% [8597/18048]\n",
      "| Epoch [136/200] Iter[161/391]\t\tLoss: 1.3469 Acc@1: 46.932% [9671/20608]\n",
      "| Epoch [136/200] Iter[181/391]\t\tLoss: 3.2542 Acc@1: 47.176% [10929/23168]\n",
      "| Epoch [136/200] Iter[201/391]\t\tLoss: 1.2616 Acc@1: 47.154% [12131/25728]\n",
      "| Epoch [136/200] Iter[221/391]\t\tLoss: 2.7457 Acc@1: 47.509% [13439/28288]\n",
      "| Epoch [136/200] Iter[241/391]\t\tLoss: 1.5755 Acc@1: 47.455% [14638/30848]\n",
      "| Epoch [136/200] Iter[261/391]\t\tLoss: 1.8510 Acc@1: 47.279% [15795/33408]\n",
      "| Epoch [136/200] Iter[281/391]\t\tLoss: 2.6989 Acc@1: 47.222% [16984/35968]\n",
      "| Epoch [136/200] Iter[301/391]\t\tLoss: 3.0495 Acc@1: 47.208% [18188/38528]\n",
      "| Epoch [136/200] Iter[321/391]\t\tLoss: 2.0370 Acc@1: 47.189% [19389/41088]\n",
      "| Epoch [136/200] Iter[341/391]\t\tLoss: 2.6472 Acc@1: 47.080% [20549/43648]\n",
      "| Epoch [136/200] Iter[361/391]\t\tLoss: 2.0039 Acc@1: 47.153% [21788/46208]\n",
      "| Epoch [136/200] Iter[381/391]\t\tLoss: 3.1215 Acc@1: 47.164% [23000/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #136\n",
      "\n",
      "| Validation Epoch #136\t\t\tLoss: 1.1206 Acc@1: 62.60%\n",
      "\n",
      "----- Elapsed time : 0:40:16\n",
      "\n",
      "\n",
      "=> Training Epoch #137\n",
      "| Epoch [137/200] Iter[  1/391]\t\tLoss: 1.8459 Acc@1: 58.468% [ 74/128]\n",
      "| Epoch [137/200] Iter[ 21/391]\t\tLoss: 2.9004 Acc@1: 44.342% [1191/2688]\n",
      "| Epoch [137/200] Iter[ 41/391]\t\tLoss: 2.1454 Acc@1: 43.846% [2301/5248]\n",
      "| Epoch [137/200] Iter[ 61/391]\t\tLoss: 1.3894 Acc@1: 47.158% [3682/7808]\n",
      "| Epoch [137/200] Iter[ 81/391]\t\tLoss: 1.2786 Acc@1: 47.278% [4901/10368]\n",
      "| Epoch [137/200] Iter[101/391]\t\tLoss: 3.0430 Acc@1: 47.480% [6138/12928]\n",
      "| Epoch [137/200] Iter[121/391]\t\tLoss: 1.3856 Acc@1: 46.522% [7205/15488]\n",
      "| Epoch [137/200] Iter[141/391]\t\tLoss: 1.5879 Acc@1: 47.092% [8499/18048]\n",
      "| Epoch [137/200] Iter[161/391]\t\tLoss: 2.3313 Acc@1: 47.365% [9760/20608]\n",
      "| Epoch [137/200] Iter[181/391]\t\tLoss: 2.7851 Acc@1: 47.575% [11022/23168]\n",
      "| Epoch [137/200] Iter[201/391]\t\tLoss: 1.7042 Acc@1: 47.666% [12263/25728]\n",
      "| Epoch [137/200] Iter[221/391]\t\tLoss: 3.1716 Acc@1: 47.599% [13464/28288]\n",
      "| Epoch [137/200] Iter[241/391]\t\tLoss: 2.2701 Acc@1: 47.422% [14628/30848]\n",
      "| Epoch [137/200] Iter[261/391]\t\tLoss: 1.6310 Acc@1: 48.077% [16061/33408]\n",
      "| Epoch [137/200] Iter[281/391]\t\tLoss: 2.4826 Acc@1: 48.144% [17316/35968]\n",
      "| Epoch [137/200] Iter[301/391]\t\tLoss: 1.5711 Acc@1: 48.602% [18725/38528]\n",
      "| Epoch [137/200] Iter[321/391]\t\tLoss: 3.0483 Acc@1: 48.692% [20006/41088]\n",
      "| Epoch [137/200] Iter[341/391]\t\tLoss: 2.3672 Acc@1: 48.661% [21239/43648]\n",
      "| Epoch [137/200] Iter[361/391]\t\tLoss: 2.3536 Acc@1: 48.705% [22505/46208]\n",
      "| Epoch [137/200] Iter[381/391]\t\tLoss: 1.4360 Acc@1: 48.545% [23674/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #137\n",
      "\n",
      "| Validation Epoch #137\t\t\tLoss: 1.3988 Acc@1: 63.32%\n",
      "\n",
      "----- Elapsed time : 0:40:35\n",
      "\n",
      "\n",
      "=> Training Epoch #138\n",
      "| Epoch [138/200] Iter[  1/391]\t\tLoss: 2.8115 Acc@1: 42.083% [ 53/128]\n",
      "| Epoch [138/200] Iter[ 21/391]\t\tLoss: 2.1407 Acc@1: 46.711% [1255/2688]\n",
      "| Epoch [138/200] Iter[ 41/391]\t\tLoss: 1.8297 Acc@1: 49.054% [2574/5248]\n",
      "| Epoch [138/200] Iter[ 61/391]\t\tLoss: 2.9914 Acc@1: 49.301% [3849/7808]\n",
      "| Epoch [138/200] Iter[ 81/391]\t\tLoss: 1.3161 Acc@1: 48.897% [5069/10368]\n",
      "| Epoch [138/200] Iter[101/391]\t\tLoss: 1.6102 Acc@1: 49.516% [6401/12928]\n",
      "| Epoch [138/200] Iter[121/391]\t\tLoss: 1.2475 Acc@1: 49.592% [7680/15488]\n",
      "| Epoch [138/200] Iter[141/391]\t\tLoss: 2.0161 Acc@1: 49.766% [8981/18048]\n",
      "| Epoch [138/200] Iter[161/391]\t\tLoss: 1.1599 Acc@1: 48.699% [10035/20608]\n",
      "| Epoch [138/200] Iter[181/391]\t\tLoss: 2.9365 Acc@1: 48.971% [11345/23168]\n",
      "| Epoch [138/200] Iter[201/391]\t\tLoss: 2.9034 Acc@1: 48.632% [12512/25728]\n",
      "| Epoch [138/200] Iter[221/391]\t\tLoss: 2.4487 Acc@1: 48.393% [13689/28288]\n",
      "| Epoch [138/200] Iter[241/391]\t\tLoss: 1.0688 Acc@1: 48.395% [14928/30848]\n",
      "| Epoch [138/200] Iter[261/391]\t\tLoss: 2.8428 Acc@1: 48.169% [16092/33408]\n",
      "| Epoch [138/200] Iter[281/391]\t\tLoss: 3.0440 Acc@1: 48.052% [17283/35968]\n",
      "| Epoch [138/200] Iter[301/391]\t\tLoss: 3.0480 Acc@1: 48.023% [18502/38528]\n",
      "| Epoch [138/200] Iter[321/391]\t\tLoss: 3.0688 Acc@1: 48.301% [19845/41088]\n",
      "| Epoch [138/200] Iter[341/391]\t\tLoss: 1.2527 Acc@1: 48.646% [21233/43648]\n",
      "| Epoch [138/200] Iter[361/391]\t\tLoss: 2.9222 Acc@1: 48.748% [22525/46208]\n",
      "| Epoch [138/200] Iter[381/391]\t\tLoss: 3.1371 Acc@1: 48.667% [23734/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #138\n",
      "\n",
      "| Validation Epoch #138\t\t\tLoss: 1.3470 Acc@1: 63.02%\n",
      "\n",
      "----- Elapsed time : 0:40:53\n",
      "\n",
      "\n",
      "=> Training Epoch #139\n",
      "| Epoch [139/200] Iter[  1/391]\t\tLoss: 2.0822 Acc@1: 58.332% [ 74/128]\n",
      "| Epoch [139/200] Iter[ 21/391]\t\tLoss: 1.6149 Acc@1: 44.507% [1196/2688]\n",
      "| Epoch [139/200] Iter[ 41/391]\t\tLoss: 3.0254 Acc@1: 44.868% [2354/5248]\n",
      "| Epoch [139/200] Iter[ 61/391]\t\tLoss: 2.6537 Acc@1: 45.858% [3580/7808]\n",
      "| Epoch [139/200] Iter[ 81/391]\t\tLoss: 1.1821 Acc@1: 46.242% [4794/10368]\n",
      "| Epoch [139/200] Iter[101/391]\t\tLoss: 2.0723 Acc@1: 46.699% [6037/12928]\n",
      "| Epoch [139/200] Iter[121/391]\t\tLoss: 1.4993 Acc@1: 46.364% [7180/15488]\n",
      "| Epoch [139/200] Iter[141/391]\t\tLoss: 3.2028 Acc@1: 45.930% [8289/18048]\n",
      "| Epoch [139/200] Iter[161/391]\t\tLoss: 1.3178 Acc@1: 45.831% [9444/20608]\n",
      "| Epoch [139/200] Iter[181/391]\t\tLoss: 3.0789 Acc@1: 45.912% [10636/23168]\n",
      "| Epoch [139/200] Iter[201/391]\t\tLoss: 2.6716 Acc@1: 46.631% [11997/25728]\n",
      "| Epoch [139/200] Iter[221/391]\t\tLoss: 1.5339 Acc@1: 46.580% [13176/28288]\n",
      "| Epoch [139/200] Iter[241/391]\t\tLoss: 3.0903 Acc@1: 46.577% [14368/30848]\n",
      "| Epoch [139/200] Iter[261/391]\t\tLoss: 2.3283 Acc@1: 46.961% [15688/33408]\n",
      "| Epoch [139/200] Iter[281/391]\t\tLoss: 2.9665 Acc@1: 46.774% [16823/35968]\n",
      "| Epoch [139/200] Iter[301/391]\t\tLoss: 3.2063 Acc@1: 47.053% [18128/38528]\n",
      "| Epoch [139/200] Iter[321/391]\t\tLoss: 1.3232 Acc@1: 47.509% [19520/41088]\n",
      "| Epoch [139/200] Iter[341/391]\t\tLoss: 3.1899 Acc@1: 47.677% [20810/43648]\n",
      "| Epoch [139/200] Iter[361/391]\t\tLoss: 2.8136 Acc@1: 47.915% [22140/46208]\n",
      "| Epoch [139/200] Iter[381/391]\t\tLoss: 2.9355 Acc@1: 47.839% [23330/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #139\n",
      "\n",
      "| Validation Epoch #139\t\t\tLoss: 1.4491 Acc@1: 63.84%\n",
      "\n",
      "----- Elapsed time : 0:41:11\n",
      "\n",
      "\n",
      "=> Training Epoch #140\n",
      "| Epoch [140/200] Iter[  1/391]\t\tLoss: 3.2486 Acc@1: 26.480% [ 33/128]\n",
      "| Epoch [140/200] Iter[ 21/391]\t\tLoss: 2.2014 Acc@1: 46.857% [1259/2688]\n",
      "| Epoch [140/200] Iter[ 41/391]\t\tLoss: 3.2571 Acc@1: 45.838% [2405/5248]\n",
      "| Epoch [140/200] Iter[ 61/391]\t\tLoss: 1.9898 Acc@1: 46.443% [3626/7808]\n",
      "| Epoch [140/200] Iter[ 81/391]\t\tLoss: 1.4617 Acc@1: 46.011% [4770/10368]\n",
      "| Epoch [140/200] Iter[101/391]\t\tLoss: 3.1000 Acc@1: 46.701% [6037/12928]\n",
      "| Epoch [140/200] Iter[121/391]\t\tLoss: 2.2205 Acc@1: 46.028% [7128/15488]\n",
      "| Epoch [140/200] Iter[141/391]\t\tLoss: 1.9058 Acc@1: 45.866% [8277/18048]\n",
      "| Epoch [140/200] Iter[161/391]\t\tLoss: 2.7808 Acc@1: 45.737% [9425/20608]\n",
      "| Epoch [140/200] Iter[181/391]\t\tLoss: 3.0861 Acc@1: 45.876% [10628/23168]\n",
      "| Epoch [140/200] Iter[201/391]\t\tLoss: 3.0275 Acc@1: 46.208% [11888/25728]\n",
      "| Epoch [140/200] Iter[221/391]\t\tLoss: 3.1100 Acc@1: 46.314% [13101/28288]\n",
      "| Epoch [140/200] Iter[241/391]\t\tLoss: 1.9026 Acc@1: 46.350% [14297/30848]\n",
      "| Epoch [140/200] Iter[261/391]\t\tLoss: 3.1231 Acc@1: 46.360% [15488/33408]\n",
      "| Epoch [140/200] Iter[281/391]\t\tLoss: 1.8474 Acc@1: 46.495% [16723/35968]\n",
      "| Epoch [140/200] Iter[301/391]\t\tLoss: 2.2876 Acc@1: 46.669% [17980/38528]\n",
      "| Epoch [140/200] Iter[321/391]\t\tLoss: 1.6031 Acc@1: 46.589% [19142/41088]\n",
      "| Epoch [140/200] Iter[341/391]\t\tLoss: 3.1506 Acc@1: 46.513% [20301/43648]\n",
      "| Epoch [140/200] Iter[361/391]\t\tLoss: 3.3540 Acc@1: 46.429% [21453/46208]\n",
      "| Epoch [140/200] Iter[381/391]\t\tLoss: 1.7898 Acc@1: 46.405% [22631/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #140\n",
      "\n",
      "| Validation Epoch #140\t\t\tLoss: 1.3431 Acc@1: 63.49%\n",
      "\n",
      "----- Elapsed time : 0:41:30\n",
      "\n",
      "\n",
      "=> Training Epoch #141\n",
      "| Epoch [141/200] Iter[  1/391]\t\tLoss: 2.3134 Acc@1: 54.519% [ 69/128]\n",
      "| Epoch [141/200] Iter[ 21/391]\t\tLoss: 3.2406 Acc@1: 41.223% [1108/2688]\n",
      "| Epoch [141/200] Iter[ 41/391]\t\tLoss: 2.9870 Acc@1: 42.768% [2244/5248]\n",
      "| Epoch [141/200] Iter[ 61/391]\t\tLoss: 1.8193 Acc@1: 45.017% [3514/7808]\n",
      "| Epoch [141/200] Iter[ 81/391]\t\tLoss: 2.8695 Acc@1: 45.466% [4713/10368]\n",
      "| Epoch [141/200] Iter[101/391]\t\tLoss: 1.7307 Acc@1: 46.817% [6052/12928]\n",
      "| Epoch [141/200] Iter[121/391]\t\tLoss: 2.9252 Acc@1: 47.779% [7399/15488]\n",
      "| Epoch [141/200] Iter[141/391]\t\tLoss: 1.7990 Acc@1: 48.466% [8747/18048]\n",
      "| Epoch [141/200] Iter[161/391]\t\tLoss: 1.4322 Acc@1: 48.656% [10026/20608]\n",
      "| Epoch [141/200] Iter[181/391]\t\tLoss: 2.4656 Acc@1: 47.833% [11081/23168]\n",
      "| Epoch [141/200] Iter[201/391]\t\tLoss: 1.8257 Acc@1: 47.942% [12334/25728]\n",
      "| Epoch [141/200] Iter[221/391]\t\tLoss: 2.1749 Acc@1: 48.344% [13675/28288]\n",
      "| Epoch [141/200] Iter[241/391]\t\tLoss: 2.5063 Acc@1: 48.043% [14820/30848]\n",
      "| Epoch [141/200] Iter[261/391]\t\tLoss: 2.5824 Acc@1: 47.535% [15880/33408]\n",
      "| Epoch [141/200] Iter[281/391]\t\tLoss: 2.5384 Acc@1: 47.599% [17120/35968]\n",
      "| Epoch [141/200] Iter[301/391]\t\tLoss: 2.5862 Acc@1: 47.649% [18358/38528]\n",
      "| Epoch [141/200] Iter[321/391]\t\tLoss: 3.2907 Acc@1: 48.092% [19759/41088]\n",
      "| Epoch [141/200] Iter[341/391]\t\tLoss: 3.1809 Acc@1: 48.023% [20961/43648]\n",
      "| Epoch [141/200] Iter[361/391]\t\tLoss: 3.1167 Acc@1: 48.072% [22213/46208]\n",
      "| Epoch [141/200] Iter[381/391]\t\tLoss: 1.4704 Acc@1: 48.189% [23500/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #141\n",
      "\n",
      "| Validation Epoch #141\t\t\tLoss: 1.2881 Acc@1: 64.27%\n",
      "\n",
      "----- Elapsed time : 0:41:48\n",
      "\n",
      "\n",
      "=> Training Epoch #142\n",
      "| Epoch [142/200] Iter[  1/391]\t\tLoss: 2.3399 Acc@1: 55.141% [ 70/128]\n",
      "| Epoch [142/200] Iter[ 21/391]\t\tLoss: 3.0489 Acc@1: 46.978% [1262/2688]\n",
      "| Epoch [142/200] Iter[ 41/391]\t\tLoss: 3.1209 Acc@1: 44.957% [2359/5248]\n",
      "| Epoch [142/200] Iter[ 61/391]\t\tLoss: 3.2731 Acc@1: 45.269% [3534/7808]\n",
      "| Epoch [142/200] Iter[ 81/391]\t\tLoss: 1.3098 Acc@1: 46.843% [4856/10368]\n",
      "| Epoch [142/200] Iter[101/391]\t\tLoss: 2.8887 Acc@1: 47.747% [6172/12928]\n",
      "| Epoch [142/200] Iter[121/391]\t\tLoss: 2.9792 Acc@1: 47.472% [7352/15488]\n",
      "| Epoch [142/200] Iter[141/391]\t\tLoss: 3.1901 Acc@1: 47.793% [8625/18048]\n",
      "| Epoch [142/200] Iter[161/391]\t\tLoss: 2.8077 Acc@1: 47.842% [9859/20608]\n",
      "| Epoch [142/200] Iter[181/391]\t\tLoss: 2.3832 Acc@1: 47.841% [11083/23168]\n",
      "| Epoch [142/200] Iter[201/391]\t\tLoss: 3.0854 Acc@1: 47.493% [12218/25728]\n",
      "| Epoch [142/200] Iter[221/391]\t\tLoss: 1.8172 Acc@1: 46.877% [13260/28288]\n",
      "| Epoch [142/200] Iter[241/391]\t\tLoss: 3.0246 Acc@1: 46.896% [14466/30848]\n",
      "| Epoch [142/200] Iter[261/391]\t\tLoss: 3.0180 Acc@1: 47.187% [15764/33408]\n",
      "| Epoch [142/200] Iter[281/391]\t\tLoss: 1.4405 Acc@1: 47.477% [17076/35968]\n",
      "| Epoch [142/200] Iter[301/391]\t\tLoss: 1.7941 Acc@1: 48.145% [18549/38528]\n",
      "| Epoch [142/200] Iter[321/391]\t\tLoss: 2.6578 Acc@1: 48.549% [19947/41088]\n",
      "| Epoch [142/200] Iter[341/391]\t\tLoss: 3.0007 Acc@1: 48.108% [20998/43648]\n",
      "| Epoch [142/200] Iter[361/391]\t\tLoss: 2.9589 Acc@1: 48.058% [22206/46208]\n",
      "| Epoch [142/200] Iter[381/391]\t\tLoss: 3.2532 Acc@1: 47.856% [23338/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #142\n",
      "\n",
      "| Validation Epoch #142\t\t\tLoss: 1.2264 Acc@1: 63.86%\n",
      "\n",
      "----- Elapsed time : 0:42:06\n",
      "\n",
      "\n",
      "=> Training Epoch #143\n",
      "| Epoch [143/200] Iter[  1/391]\t\tLoss: 3.1634 Acc@1: 31.679% [ 40/128]\n",
      "| Epoch [143/200] Iter[ 21/391]\t\tLoss: 1.8558 Acc@1: 44.982% [1209/2688]\n",
      "| Epoch [143/200] Iter[ 41/391]\t\tLoss: 1.8467 Acc@1: 49.051% [2574/5248]\n",
      "| Epoch [143/200] Iter[ 61/391]\t\tLoss: 2.5173 Acc@1: 49.927% [3898/7808]\n",
      "| Epoch [143/200] Iter[ 81/391]\t\tLoss: 2.5361 Acc@1: 49.646% [5147/10368]\n",
      "| Epoch [143/200] Iter[101/391]\t\tLoss: 2.9914 Acc@1: 48.587% [6281/12928]\n",
      "| Epoch [143/200] Iter[121/391]\t\tLoss: 2.6129 Acc@1: 47.787% [7401/15488]\n",
      "| Epoch [143/200] Iter[141/391]\t\tLoss: 2.9818 Acc@1: 47.628% [8595/18048]\n",
      "| Epoch [143/200] Iter[161/391]\t\tLoss: 2.4326 Acc@1: 48.211% [9935/20608]\n",
      "| Epoch [143/200] Iter[181/391]\t\tLoss: 2.6452 Acc@1: 48.000% [11120/23168]\n",
      "| Epoch [143/200] Iter[201/391]\t\tLoss: 2.8973 Acc@1: 47.621% [12251/25728]\n",
      "| Epoch [143/200] Iter[221/391]\t\tLoss: 1.3807 Acc@1: 47.131% [13332/28288]\n",
      "| Epoch [143/200] Iter[241/391]\t\tLoss: 3.1939 Acc@1: 47.129% [14538/30848]\n",
      "| Epoch [143/200] Iter[261/391]\t\tLoss: 3.0543 Acc@1: 47.086% [15730/33408]\n",
      "| Epoch [143/200] Iter[281/391]\t\tLoss: 1.6144 Acc@1: 47.098% [16940/35968]\n",
      "| Epoch [143/200] Iter[301/391]\t\tLoss: 2.5113 Acc@1: 47.110% [18150/38528]\n",
      "| Epoch [143/200] Iter[321/391]\t\tLoss: 2.7468 Acc@1: 47.206% [19395/41088]\n",
      "| Epoch [143/200] Iter[341/391]\t\tLoss: 2.8831 Acc@1: 47.531% [20746/43648]\n",
      "| Epoch [143/200] Iter[361/391]\t\tLoss: 3.0094 Acc@1: 47.692% [22037/46208]\n",
      "| Epoch [143/200] Iter[381/391]\t\tLoss: 2.6940 Acc@1: 47.823% [23322/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #143\n",
      "\n",
      "| Validation Epoch #143\t\t\tLoss: 1.5665 Acc@1: 63.82%\n",
      "\n",
      "----- Elapsed time : 0:42:25\n",
      "\n",
      "\n",
      "=> Training Epoch #144\n",
      "| Epoch [144/200] Iter[  1/391]\t\tLoss: 1.5767 Acc@1: 72.558% [ 92/128]\n",
      "| Epoch [144/200] Iter[ 21/391]\t\tLoss: 3.1209 Acc@1: 52.388% [1408/2688]\n",
      "| Epoch [144/200] Iter[ 41/391]\t\tLoss: 2.0848 Acc@1: 48.019% [2520/5248]\n",
      "| Epoch [144/200] Iter[ 61/391]\t\tLoss: 1.0163 Acc@1: 49.005% [3826/7808]\n",
      "| Epoch [144/200] Iter[ 81/391]\t\tLoss: 1.8734 Acc@1: 49.506% [5132/10368]\n",
      "| Epoch [144/200] Iter[101/391]\t\tLoss: 1.7715 Acc@1: 50.661% [6549/12928]\n",
      "| Epoch [144/200] Iter[121/391]\t\tLoss: 3.1102 Acc@1: 50.302% [7790/15488]\n",
      "| Epoch [144/200] Iter[141/391]\t\tLoss: 2.9839 Acc@1: 51.112% [9224/18048]\n",
      "| Epoch [144/200] Iter[161/391]\t\tLoss: 2.8320 Acc@1: 50.180% [10341/20608]\n",
      "| Epoch [144/200] Iter[181/391]\t\tLoss: 3.0527 Acc@1: 50.234% [11638/23168]\n",
      "| Epoch [144/200] Iter[201/391]\t\tLoss: 2.1647 Acc@1: 50.537% [13002/25728]\n",
      "| Epoch [144/200] Iter[221/391]\t\tLoss: 1.4630 Acc@1: 50.340% [14240/28288]\n",
      "| Epoch [144/200] Iter[241/391]\t\tLoss: 2.7461 Acc@1: 50.134% [15465/30848]\n",
      "| Epoch [144/200] Iter[261/391]\t\tLoss: 2.6315 Acc@1: 50.226% [16779/33408]\n",
      "| Epoch [144/200] Iter[281/391]\t\tLoss: 1.6846 Acc@1: 49.809% [17915/35968]\n",
      "| Epoch [144/200] Iter[301/391]\t\tLoss: 2.8698 Acc@1: 49.502% [19072/38528]\n",
      "| Epoch [144/200] Iter[321/391]\t\tLoss: 3.0171 Acc@1: 49.471% [20326/41088]\n",
      "| Epoch [144/200] Iter[341/391]\t\tLoss: 2.1996 Acc@1: 49.742% [21711/43648]\n",
      "| Epoch [144/200] Iter[361/391]\t\tLoss: 2.1803 Acc@1: 49.826% [23023/46208]\n",
      "| Epoch [144/200] Iter[381/391]\t\tLoss: 2.7090 Acc@1: 49.683% [24229/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #144\n",
      "\n",
      "| Validation Epoch #144\t\t\tLoss: 1.3494 Acc@1: 63.31%\n",
      "\n",
      "----- Elapsed time : 0:42:43\n",
      "\n",
      "\n",
      "=> Training Epoch #145\n",
      "| Epoch [145/200] Iter[  1/391]\t\tLoss: 2.2957 Acc@1: 55.539% [ 71/128]\n",
      "| Epoch [145/200] Iter[ 21/391]\t\tLoss: 2.4034 Acc@1: 46.881% [1260/2688]\n",
      "| Epoch [145/200] Iter[ 41/391]\t\tLoss: 2.3772 Acc@1: 47.128% [2473/5248]\n",
      "| Epoch [145/200] Iter[ 61/391]\t\tLoss: 3.1810 Acc@1: 45.672% [3566/7808]\n",
      "| Epoch [145/200] Iter[ 81/391]\t\tLoss: 1.0743 Acc@1: 47.583% [4933/10368]\n",
      "| Epoch [145/200] Iter[101/391]\t\tLoss: 3.0103 Acc@1: 46.158% [5967/12928]\n",
      "| Epoch [145/200] Iter[121/391]\t\tLoss: 1.8969 Acc@1: 46.494% [7200/15488]\n",
      "| Epoch [145/200] Iter[141/391]\t\tLoss: 1.5316 Acc@1: 47.577% [8586/18048]\n",
      "| Epoch [145/200] Iter[161/391]\t\tLoss: 1.3725 Acc@1: 47.467% [9781/20608]\n",
      "| Epoch [145/200] Iter[181/391]\t\tLoss: 1.8539 Acc@1: 47.480% [11000/23168]\n",
      "| Epoch [145/200] Iter[201/391]\t\tLoss: 2.9465 Acc@1: 47.761% [12287/25728]\n",
      "| Epoch [145/200] Iter[221/391]\t\tLoss: 1.1228 Acc@1: 48.221% [13640/28288]\n",
      "| Epoch [145/200] Iter[241/391]\t\tLoss: 2.8588 Acc@1: 48.459% [14948/30848]\n",
      "| Epoch [145/200] Iter[261/391]\t\tLoss: 1.0771 Acc@1: 48.738% [16282/33408]\n",
      "| Epoch [145/200] Iter[281/391]\t\tLoss: 1.1084 Acc@1: 48.525% [17453/35968]\n",
      "| Epoch [145/200] Iter[301/391]\t\tLoss: 1.6600 Acc@1: 48.045% [18510/38528]\n",
      "| Epoch [145/200] Iter[321/391]\t\tLoss: 2.7996 Acc@1: 48.406% [19889/41088]\n",
      "| Epoch [145/200] Iter[341/391]\t\tLoss: 1.9425 Acc@1: 48.337% [21098/43648]\n",
      "| Epoch [145/200] Iter[361/391]\t\tLoss: 1.6925 Acc@1: 47.993% [22176/46208]\n",
      "| Epoch [145/200] Iter[381/391]\t\tLoss: 3.2196 Acc@1: 48.067% [23441/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #145\n",
      "\n",
      "| Validation Epoch #145\t\t\tLoss: 1.2575 Acc@1: 64.02%\n",
      "\n",
      "----- Elapsed time : 0:43:02\n",
      "\n",
      "\n",
      "=> Training Epoch #146\n",
      "| Epoch [146/200] Iter[  1/391]\t\tLoss: 3.0729 Acc@1: 27.810% [ 35/128]\n",
      "| Epoch [146/200] Iter[ 21/391]\t\tLoss: 3.0508 Acc@1: 47.012% [1263/2688]\n",
      "| Epoch [146/200] Iter[ 41/391]\t\tLoss: 1.9953 Acc@1: 47.869% [2512/5248]\n",
      "| Epoch [146/200] Iter[ 61/391]\t\tLoss: 2.3036 Acc@1: 48.250% [3767/7808]\n",
      "| Epoch [146/200] Iter[ 81/391]\t\tLoss: 2.9305 Acc@1: 48.310% [5008/10368]\n",
      "| Epoch [146/200] Iter[101/391]\t\tLoss: 2.7891 Acc@1: 48.027% [6208/12928]\n",
      "| Epoch [146/200] Iter[121/391]\t\tLoss: 2.9113 Acc@1: 48.559% [7520/15488]\n",
      "| Epoch [146/200] Iter[141/391]\t\tLoss: 2.9876 Acc@1: 48.161% [8692/18048]\n",
      "| Epoch [146/200] Iter[161/391]\t\tLoss: 2.9998 Acc@1: 48.345% [9962/20608]\n",
      "| Epoch [146/200] Iter[181/391]\t\tLoss: 2.7635 Acc@1: 48.746% [11293/23168]\n",
      "| Epoch [146/200] Iter[201/391]\t\tLoss: 2.8527 Acc@1: 48.859% [12570/25728]\n",
      "| Epoch [146/200] Iter[221/391]\t\tLoss: 2.9871 Acc@1: 48.288% [13659/28288]\n",
      "| Epoch [146/200] Iter[241/391]\t\tLoss: 3.0974 Acc@1: 48.163% [14857/30848]\n",
      "| Epoch [146/200] Iter[261/391]\t\tLoss: 1.7553 Acc@1: 48.381% [16163/33408]\n",
      "| Epoch [146/200] Iter[281/391]\t\tLoss: 3.0158 Acc@1: 48.012% [17268/35968]\n",
      "| Epoch [146/200] Iter[301/391]\t\tLoss: 2.3046 Acc@1: 48.380% [18639/38528]\n",
      "| Epoch [146/200] Iter[321/391]\t\tLoss: 3.0837 Acc@1: 48.260% [19828/41088]\n",
      "| Epoch [146/200] Iter[341/391]\t\tLoss: 2.6119 Acc@1: 48.296% [21080/43648]\n",
      "| Epoch [146/200] Iter[361/391]\t\tLoss: 2.2337 Acc@1: 48.248% [22294/46208]\n",
      "| Epoch [146/200] Iter[381/391]\t\tLoss: 1.4259 Acc@1: 48.239% [23525/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #146\n",
      "\n",
      "| Validation Epoch #146\t\t\tLoss: 1.3093 Acc@1: 63.58%\n",
      "\n",
      "----- Elapsed time : 0:43:20\n",
      "\n",
      "\n",
      "=> Training Epoch #147\n",
      "| Epoch [147/200] Iter[  1/391]\t\tLoss: 3.1357 Acc@1: 30.270% [ 38/128]\n",
      "| Epoch [147/200] Iter[ 21/391]\t\tLoss: 1.9433 Acc@1: 46.767% [1257/2688]\n",
      "| Epoch [147/200] Iter[ 41/391]\t\tLoss: 1.3504 Acc@1: 48.158% [2527/5248]\n",
      "| Epoch [147/200] Iter[ 61/391]\t\tLoss: 2.9367 Acc@1: 47.671% [3722/7808]\n",
      "| Epoch [147/200] Iter[ 81/391]\t\tLoss: 1.1863 Acc@1: 46.771% [4849/10368]\n",
      "| Epoch [147/200] Iter[101/391]\t\tLoss: 3.1177 Acc@1: 46.963% [6071/12928]\n",
      "| Epoch [147/200] Iter[121/391]\t\tLoss: 2.6591 Acc@1: 46.339% [7176/15488]\n",
      "| Epoch [147/200] Iter[141/391]\t\tLoss: 2.7909 Acc@1: 46.882% [8461/18048]\n",
      "| Epoch [147/200] Iter[161/391]\t\tLoss: 2.0263 Acc@1: 47.847% [9860/20608]\n",
      "| Epoch [147/200] Iter[181/391]\t\tLoss: 1.9433 Acc@1: 48.316% [11193/23168]\n",
      "| Epoch [147/200] Iter[201/391]\t\tLoss: 2.8991 Acc@1: 48.265% [12417/25728]\n",
      "| Epoch [147/200] Iter[221/391]\t\tLoss: 3.0630 Acc@1: 48.820% [13810/28288]\n",
      "| Epoch [147/200] Iter[241/391]\t\tLoss: 3.0745 Acc@1: 48.772% [15045/30848]\n",
      "| Epoch [147/200] Iter[261/391]\t\tLoss: 1.9622 Acc@1: 49.131% [16413/33408]\n",
      "| Epoch [147/200] Iter[281/391]\t\tLoss: 3.1628 Acc@1: 49.085% [17654/35968]\n",
      "| Epoch [147/200] Iter[301/391]\t\tLoss: 1.6341 Acc@1: 48.947% [18858/38528]\n",
      "| Epoch [147/200] Iter[321/391]\t\tLoss: 2.9886 Acc@1: 49.012% [20138/41088]\n",
      "| Epoch [147/200] Iter[341/391]\t\tLoss: 2.8894 Acc@1: 49.090% [21426/43648]\n",
      "| Epoch [147/200] Iter[361/391]\t\tLoss: 1.7274 Acc@1: 48.992% [22638/46208]\n",
      "| Epoch [147/200] Iter[381/391]\t\tLoss: 3.3091 Acc@1: 48.983% [23887/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #147\n",
      "\n",
      "| Validation Epoch #147\t\t\tLoss: 1.2596 Acc@1: 64.14%\n",
      "\n",
      "----- Elapsed time : 0:43:39\n",
      "\n",
      "\n",
      "=> Training Epoch #148\n",
      "| Epoch [148/200] Iter[  1/391]\t\tLoss: 3.2147 Acc@1: 24.494% [ 31/128]\n",
      "| Epoch [148/200] Iter[ 21/391]\t\tLoss: 2.3543 Acc@1: 47.506% [1276/2688]\n",
      "| Epoch [148/200] Iter[ 41/391]\t\tLoss: 1.7733 Acc@1: 50.167% [2632/5248]\n",
      "| Epoch [148/200] Iter[ 61/391]\t\tLoss: 3.3379 Acc@1: 50.226% [3921/7808]\n",
      "| Epoch [148/200] Iter[ 81/391]\t\tLoss: 2.7401 Acc@1: 49.939% [5177/10368]\n",
      "| Epoch [148/200] Iter[101/391]\t\tLoss: 3.0932 Acc@1: 49.266% [6369/12928]\n",
      "| Epoch [148/200] Iter[121/391]\t\tLoss: 1.6105 Acc@1: 49.352% [7643/15488]\n",
      "| Epoch [148/200] Iter[141/391]\t\tLoss: 1.7889 Acc@1: 49.522% [8937/18048]\n",
      "| Epoch [148/200] Iter[161/391]\t\tLoss: 2.6790 Acc@1: 49.519% [10204/20608]\n",
      "| Epoch [148/200] Iter[181/391]\t\tLoss: 3.1010 Acc@1: 49.245% [11409/23168]\n",
      "| Epoch [148/200] Iter[201/391]\t\tLoss: 2.9647 Acc@1: 48.464% [12468/25728]\n",
      "| Epoch [148/200] Iter[221/391]\t\tLoss: 2.9381 Acc@1: 48.199% [13634/28288]\n",
      "| Epoch [148/200] Iter[241/391]\t\tLoss: 1.8457 Acc@1: 48.196% [14867/30848]\n",
      "| Epoch [148/200] Iter[261/391]\t\tLoss: 1.7238 Acc@1: 47.803% [15970/33408]\n",
      "| Epoch [148/200] Iter[281/391]\t\tLoss: 3.2828 Acc@1: 48.467% [17432/35968]\n",
      "| Epoch [148/200] Iter[301/391]\t\tLoss: 1.3045 Acc@1: 49.006% [18881/38528]\n",
      "| Epoch [148/200] Iter[321/391]\t\tLoss: 1.1563 Acc@1: 48.765% [20036/41088]\n",
      "| Epoch [148/200] Iter[341/391]\t\tLoss: 1.6176 Acc@1: 48.735% [21271/43648]\n",
      "| Epoch [148/200] Iter[361/391]\t\tLoss: 3.0916 Acc@1: 48.433% [22380/46208]\n",
      "| Epoch [148/200] Iter[381/391]\t\tLoss: 1.2286 Acc@1: 48.755% [23776/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #148\n",
      "\n",
      "| Validation Epoch #148\t\t\tLoss: 1.2107 Acc@1: 63.23%\n",
      "\n",
      "----- Elapsed time : 0:43:57\n",
      "\n",
      "\n",
      "=> Training Epoch #149\n",
      "| Epoch [149/200] Iter[  1/391]\t\tLoss: 3.1526 Acc@1: 26.250% [ 33/128]\n",
      "| Epoch [149/200] Iter[ 21/391]\t\tLoss: 2.6788 Acc@1: 53.801% [1446/2688]\n",
      "| Epoch [149/200] Iter[ 41/391]\t\tLoss: 3.1488 Acc@1: 49.587% [2602/5248]\n",
      "| Epoch [149/200] Iter[ 61/391]\t\tLoss: 2.6853 Acc@1: 50.784% [3965/7808]\n",
      "| Epoch [149/200] Iter[ 81/391]\t\tLoss: 1.4674 Acc@1: 50.768% [5263/10368]\n",
      "| Epoch [149/200] Iter[101/391]\t\tLoss: 3.1091 Acc@1: 50.239% [6494/12928]\n",
      "| Epoch [149/200] Iter[121/391]\t\tLoss: 2.6205 Acc@1: 50.146% [7766/15488]\n",
      "| Epoch [149/200] Iter[141/391]\t\tLoss: 1.3589 Acc@1: 50.144% [9050/18048]\n",
      "| Epoch [149/200] Iter[161/391]\t\tLoss: 1.6542 Acc@1: 49.479% [10196/20608]\n",
      "| Epoch [149/200] Iter[181/391]\t\tLoss: 3.0062 Acc@1: 48.648% [11270/23168]\n",
      "| Epoch [149/200] Iter[201/391]\t\tLoss: 2.4849 Acc@1: 48.977% [12600/25728]\n",
      "| Epoch [149/200] Iter[221/391]\t\tLoss: 1.5213 Acc@1: 48.710% [13779/28288]\n",
      "| Epoch [149/200] Iter[241/391]\t\tLoss: 1.9132 Acc@1: 48.432% [14940/30848]\n",
      "| Epoch [149/200] Iter[261/391]\t\tLoss: 2.5852 Acc@1: 48.515% [16207/33408]\n",
      "| Epoch [149/200] Iter[281/391]\t\tLoss: 2.5811 Acc@1: 48.494% [17442/35968]\n",
      "| Epoch [149/200] Iter[301/391]\t\tLoss: 2.5993 Acc@1: 49.012% [18883/38528]\n",
      "| Epoch [149/200] Iter[321/391]\t\tLoss: 1.0404 Acc@1: 48.741% [20026/41088]\n",
      "| Epoch [149/200] Iter[341/391]\t\tLoss: 2.3842 Acc@1: 49.052% [21410/43648]\n",
      "| Epoch [149/200] Iter[361/391]\t\tLoss: 1.3798 Acc@1: 49.108% [22691/46208]\n",
      "| Epoch [149/200] Iter[381/391]\t\tLoss: 3.0484 Acc@1: 49.246% [24016/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #149\n",
      "\n",
      "| Validation Epoch #149\t\t\tLoss: 1.3993 Acc@1: 63.97%\n",
      "\n",
      "----- Elapsed time : 0:44:16\n",
      "\n",
      "\n",
      "=> Training Epoch #150\n",
      "| Epoch [150/200] Iter[  1/391]\t\tLoss: 2.1889 Acc@1: 58.396% [ 74/128]\n",
      "| Epoch [150/200] Iter[ 21/391]\t\tLoss: 2.5748 Acc@1: 46.715% [1255/2688]\n",
      "| Epoch [150/200] Iter[ 41/391]\t\tLoss: 3.1603 Acc@1: 46.685% [2450/5248]\n",
      "| Epoch [150/200] Iter[ 61/391]\t\tLoss: 2.0474 Acc@1: 48.077% [3753/7808]\n",
      "| Epoch [150/200] Iter[ 81/391]\t\tLoss: 2.8992 Acc@1: 48.278% [5005/10368]\n",
      "| Epoch [150/200] Iter[101/391]\t\tLoss: 2.3362 Acc@1: 47.317% [6117/12928]\n",
      "| Epoch [150/200] Iter[121/391]\t\tLoss: 2.5813 Acc@1: 46.617% [7220/15488]\n",
      "| Epoch [150/200] Iter[141/391]\t\tLoss: 2.9423 Acc@1: 46.393% [8373/18048]\n",
      "| Epoch [150/200] Iter[161/391]\t\tLoss: 3.1777 Acc@1: 46.229% [9526/20608]\n",
      "| Epoch [150/200] Iter[181/391]\t\tLoss: 2.9240 Acc@1: 46.670% [10812/23168]\n",
      "| Epoch [150/200] Iter[201/391]\t\tLoss: 2.3625 Acc@1: 47.276% [12163/25728]\n",
      "| Epoch [150/200] Iter[221/391]\t\tLoss: 1.5749 Acc@1: 47.464% [13426/28288]\n",
      "| Epoch [150/200] Iter[241/391]\t\tLoss: 3.0751 Acc@1: 47.849% [14760/30848]\n",
      "| Epoch [150/200] Iter[261/391]\t\tLoss: 1.1275 Acc@1: 47.539% [15881/33408]\n",
      "| Epoch [150/200] Iter[281/391]\t\tLoss: 2.9757 Acc@1: 47.703% [17157/35968]\n",
      "| Epoch [150/200] Iter[301/391]\t\tLoss: 3.0078 Acc@1: 47.400% [18262/38528]\n",
      "| Epoch [150/200] Iter[321/391]\t\tLoss: 2.3429 Acc@1: 47.117% [19359/41088]\n",
      "| Epoch [150/200] Iter[341/391]\t\tLoss: 2.6803 Acc@1: 47.372% [20677/43648]\n",
      "| Epoch [150/200] Iter[361/391]\t\tLoss: 1.5094 Acc@1: 47.385% [21895/46208]\n",
      "| Epoch [150/200] Iter[381/391]\t\tLoss: 3.1629 Acc@1: 47.441% [23136/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #150\n",
      "\n",
      "| Validation Epoch #150\t\t\tLoss: 1.2604 Acc@1: 63.68%\n",
      "\n",
      "----- Elapsed time : 0:44:35\n",
      "\n",
      "\n",
      "=> Training Epoch #151\n",
      "| Epoch [151/200] Iter[  1/391]\t\tLoss: 1.3610 Acc@1: 75.068% [ 96/128]\n",
      "| Epoch [151/200] Iter[ 21/391]\t\tLoss: 2.4425 Acc@1: 48.397% [1300/2688]\n",
      "| Epoch [151/200] Iter[ 41/391]\t\tLoss: 2.5076 Acc@1: 49.377% [2591/5248]\n",
      "| Epoch [151/200] Iter[ 61/391]\t\tLoss: 1.6878 Acc@1: 48.651% [3798/7808]\n",
      "| Epoch [151/200] Iter[ 81/391]\t\tLoss: 3.0123 Acc@1: 48.394% [5017/10368]\n",
      "| Epoch [151/200] Iter[101/391]\t\tLoss: 3.0630 Acc@1: 48.464% [6265/12928]\n",
      "| Epoch [151/200] Iter[121/391]\t\tLoss: 2.7635 Acc@1: 47.618% [7375/15488]\n",
      "| Epoch [151/200] Iter[141/391]\t\tLoss: 3.1193 Acc@1: 48.178% [8695/18048]\n",
      "| Epoch [151/200] Iter[161/391]\t\tLoss: 1.6673 Acc@1: 49.042% [10106/20608]\n",
      "| Epoch [151/200] Iter[181/391]\t\tLoss: 3.0708 Acc@1: 49.211% [11401/23168]\n",
      "| Epoch [151/200] Iter[201/391]\t\tLoss: 2.9598 Acc@1: 48.909% [12583/25728]\n",
      "| Epoch [151/200] Iter[221/391]\t\tLoss: 2.2288 Acc@1: 48.528% [13727/28288]\n",
      "| Epoch [151/200] Iter[241/391]\t\tLoss: 1.1952 Acc@1: 48.665% [15012/30848]\n",
      "| Epoch [151/200] Iter[261/391]\t\tLoss: 2.0384 Acc@1: 49.052% [16387/33408]\n",
      "| Epoch [151/200] Iter[281/391]\t\tLoss: 3.1939 Acc@1: 48.774% [17543/35968]\n",
      "| Epoch [151/200] Iter[301/391]\t\tLoss: 1.3789 Acc@1: 48.851% [18821/38528]\n",
      "| Epoch [151/200] Iter[321/391]\t\tLoss: 2.7569 Acc@1: 48.612% [19973/41088]\n",
      "| Epoch [151/200] Iter[341/391]\t\tLoss: 3.0505 Acc@1: 48.389% [21120/43648]\n",
      "| Epoch [151/200] Iter[361/391]\t\tLoss: 1.1208 Acc@1: 48.325% [22330/46208]\n",
      "| Epoch [151/200] Iter[381/391]\t\tLoss: 1.4549 Acc@1: 48.373% [23590/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #151\n",
      "\n",
      "| Validation Epoch #151\t\t\tLoss: 1.3653 Acc@1: 63.86%\n",
      "\n",
      "----- Elapsed time : 0:44:53\n",
      "\n",
      "\n",
      "=> Training Epoch #152\n",
      "| Epoch [152/200] Iter[  1/391]\t\tLoss: 3.0966 Acc@1: 28.260% [ 36/128]\n",
      "| Epoch [152/200] Iter[ 21/391]\t\tLoss: 2.5392 Acc@1: 47.143% [1267/2688]\n",
      "| Epoch [152/200] Iter[ 41/391]\t\tLoss: 2.3825 Acc@1: 49.192% [2581/5248]\n",
      "| Epoch [152/200] Iter[ 61/391]\t\tLoss: 2.8335 Acc@1: 48.136% [3758/7808]\n",
      "| Epoch [152/200] Iter[ 81/391]\t\tLoss: 2.1329 Acc@1: 49.166% [5097/10368]\n",
      "| Epoch [152/200] Iter[101/391]\t\tLoss: 3.1402 Acc@1: 49.885% [6449/12928]\n",
      "| Epoch [152/200] Iter[121/391]\t\tLoss: 1.7021 Acc@1: 50.242% [7781/15488]\n",
      "| Epoch [152/200] Iter[141/391]\t\tLoss: 3.3611 Acc@1: 50.187% [9057/18048]\n",
      "| Epoch [152/200] Iter[161/391]\t\tLoss: 2.2578 Acc@1: 50.859% [10480/20608]\n",
      "| Epoch [152/200] Iter[181/391]\t\tLoss: 1.9260 Acc@1: 50.641% [11732/23168]\n",
      "| Epoch [152/200] Iter[201/391]\t\tLoss: 1.8003 Acc@1: 50.844% [13081/25728]\n",
      "| Epoch [152/200] Iter[221/391]\t\tLoss: 2.3394 Acc@1: 50.750% [14356/28288]\n",
      "| Epoch [152/200] Iter[241/391]\t\tLoss: 3.0529 Acc@1: 50.644% [15622/30848]\n",
      "| Epoch [152/200] Iter[261/391]\t\tLoss: 1.4988 Acc@1: 50.916% [17010/33408]\n",
      "| Epoch [152/200] Iter[281/391]\t\tLoss: 2.7591 Acc@1: 50.846% [18288/35968]\n",
      "| Epoch [152/200] Iter[301/391]\t\tLoss: 2.7244 Acc@1: 50.618% [19501/38528]\n",
      "| Epoch [152/200] Iter[321/391]\t\tLoss: 3.0874 Acc@1: 50.596% [20789/41088]\n",
      "| Epoch [152/200] Iter[341/391]\t\tLoss: 2.7730 Acc@1: 50.432% [22012/43648]\n",
      "| Epoch [152/200] Iter[361/391]\t\tLoss: 3.0629 Acc@1: 50.378% [23278/46208]\n",
      "| Epoch [152/200] Iter[381/391]\t\tLoss: 1.5896 Acc@1: 50.404% [24580/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #152\n",
      "\n",
      "| Validation Epoch #152\t\t\tLoss: 1.2107 Acc@1: 64.75%\n",
      "\n",
      "----- Elapsed time : 0:45:12\n",
      "\n",
      "\n",
      "=> Training Epoch #153\n",
      "| Epoch [153/200] Iter[  1/391]\t\tLoss: 0.9121 Acc@1: 79.264% [101/128]\n",
      "| Epoch [153/200] Iter[ 21/391]\t\tLoss: 2.2535 Acc@1: 51.801% [1392/2688]\n",
      "| Epoch [153/200] Iter[ 41/391]\t\tLoss: 3.0811 Acc@1: 50.307% [2640/5248]\n",
      "| Epoch [153/200] Iter[ 61/391]\t\tLoss: 2.6722 Acc@1: 50.915% [3975/7808]\n",
      "| Epoch [153/200] Iter[ 81/391]\t\tLoss: 2.7264 Acc@1: 49.714% [5154/10368]\n",
      "| Epoch [153/200] Iter[101/391]\t\tLoss: 3.1122 Acc@1: 49.252% [6367/12928]\n",
      "| Epoch [153/200] Iter[121/391]\t\tLoss: 3.1934 Acc@1: 50.096% [7758/15488]\n",
      "| Epoch [153/200] Iter[141/391]\t\tLoss: 1.5603 Acc@1: 50.279% [9074/18048]\n",
      "| Epoch [153/200] Iter[161/391]\t\tLoss: 1.8957 Acc@1: 49.120% [10122/20608]\n",
      "| Epoch [153/200] Iter[181/391]\t\tLoss: 2.5667 Acc@1: 49.507% [11469/23168]\n",
      "| Epoch [153/200] Iter[201/391]\t\tLoss: 3.0556 Acc@1: 50.091% [12887/25728]\n",
      "| Epoch [153/200] Iter[221/391]\t\tLoss: 3.0656 Acc@1: 49.977% [14137/28288]\n",
      "| Epoch [153/200] Iter[241/391]\t\tLoss: 2.6320 Acc@1: 49.791% [15359/30848]\n",
      "| Epoch [153/200] Iter[261/391]\t\tLoss: 3.1382 Acc@1: 50.106% [16739/33408]\n",
      "| Epoch [153/200] Iter[281/391]\t\tLoss: 2.4969 Acc@1: 50.547% [18180/35968]\n",
      "| Epoch [153/200] Iter[301/391]\t\tLoss: 2.1194 Acc@1: 50.492% [19453/38528]\n",
      "| Epoch [153/200] Iter[321/391]\t\tLoss: 3.0175 Acc@1: 50.371% [20696/41088]\n",
      "| Epoch [153/200] Iter[341/391]\t\tLoss: 1.2090 Acc@1: 50.145% [21887/43648]\n",
      "| Epoch [153/200] Iter[361/391]\t\tLoss: 1.0838 Acc@1: 50.096% [23148/46208]\n",
      "| Epoch [153/200] Iter[381/391]\t\tLoss: 2.7142 Acc@1: 50.213% [24487/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #153\n",
      "\n",
      "| Validation Epoch #153\t\t\tLoss: 1.3224 Acc@1: 64.46%\n",
      "\n",
      "----- Elapsed time : 0:45:30\n",
      "\n",
      "\n",
      "=> Training Epoch #154\n",
      "| Epoch [154/200] Iter[  1/391]\t\tLoss: 1.7751 Acc@1: 67.077% [ 85/128]\n",
      "| Epoch [154/200] Iter[ 21/391]\t\tLoss: 1.5772 Acc@1: 53.539% [1439/2688]\n",
      "| Epoch [154/200] Iter[ 41/391]\t\tLoss: 3.2202 Acc@1: 52.645% [2762/5248]\n",
      "| Epoch [154/200] Iter[ 61/391]\t\tLoss: 1.2708 Acc@1: 53.136% [4148/7808]\n",
      "| Epoch [154/200] Iter[ 81/391]\t\tLoss: 1.5289 Acc@1: 53.618% [5559/10368]\n",
      "| Epoch [154/200] Iter[101/391]\t\tLoss: 2.5636 Acc@1: 53.345% [6896/12928]\n",
      "| Epoch [154/200] Iter[121/391]\t\tLoss: 2.3260 Acc@1: 52.264% [8094/15488]\n",
      "| Epoch [154/200] Iter[141/391]\t\tLoss: 1.9036 Acc@1: 51.701% [9330/18048]\n",
      "| Epoch [154/200] Iter[161/391]\t\tLoss: 2.5983 Acc@1: 51.489% [10610/20608]\n",
      "| Epoch [154/200] Iter[181/391]\t\tLoss: 1.0105 Acc@1: 51.921% [12028/23168]\n",
      "| Epoch [154/200] Iter[201/391]\t\tLoss: 3.0956 Acc@1: 51.567% [13267/25728]\n",
      "| Epoch [154/200] Iter[221/391]\t\tLoss: 2.8065 Acc@1: 51.046% [14439/28288]\n",
      "| Epoch [154/200] Iter[241/391]\t\tLoss: 1.0922 Acc@1: 50.957% [15719/30848]\n",
      "| Epoch [154/200] Iter[261/391]\t\tLoss: 3.2609 Acc@1: 50.709% [16940/33408]\n",
      "| Epoch [154/200] Iter[281/391]\t\tLoss: 2.7533 Acc@1: 50.484% [18158/35968]\n",
      "| Epoch [154/200] Iter[301/391]\t\tLoss: 2.8468 Acc@1: 50.405% [19419/38528]\n",
      "| Epoch [154/200] Iter[321/391]\t\tLoss: 1.7406 Acc@1: 50.642% [20807/41088]\n",
      "| Epoch [154/200] Iter[341/391]\t\tLoss: 1.8728 Acc@1: 50.551% [22064/43648]\n",
      "| Epoch [154/200] Iter[361/391]\t\tLoss: 1.7731 Acc@1: 50.672% [23414/46208]\n",
      "| Epoch [154/200] Iter[381/391]\t\tLoss: 2.6248 Acc@1: 50.757% [24753/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #154\n",
      "\n",
      "| Validation Epoch #154\t\t\tLoss: 1.3250 Acc@1: 64.81%\n",
      "\n",
      "----- Elapsed time : 0:45:48\n",
      "\n",
      "\n",
      "=> Training Epoch #155\n",
      "| Epoch [155/200] Iter[  1/391]\t\tLoss: 3.1536 Acc@1: 28.241% [ 36/128]\n",
      "| Epoch [155/200] Iter[ 21/391]\t\tLoss: 2.5753 Acc@1: 55.524% [1492/2688]\n",
      "| Epoch [155/200] Iter[ 41/391]\t\tLoss: 2.6188 Acc@1: 56.708% [2976/5248]\n",
      "| Epoch [155/200] Iter[ 61/391]\t\tLoss: 3.0248 Acc@1: 54.111% [4225/7808]\n",
      "| Epoch [155/200] Iter[ 81/391]\t\tLoss: 1.5118 Acc@1: 52.904% [5485/10368]\n",
      "| Epoch [155/200] Iter[101/391]\t\tLoss: 2.4161 Acc@1: 53.195% [6877/12928]\n",
      "| Epoch [155/200] Iter[121/391]\t\tLoss: 3.1289 Acc@1: 51.910% [8039/15488]\n",
      "| Epoch [155/200] Iter[141/391]\t\tLoss: 1.0835 Acc@1: 52.079% [9399/18048]\n",
      "| Epoch [155/200] Iter[161/391]\t\tLoss: 1.5355 Acc@1: 52.621% [10844/20608]\n",
      "| Epoch [155/200] Iter[181/391]\t\tLoss: 1.7644 Acc@1: 52.933% [12263/23168]\n",
      "| Epoch [155/200] Iter[201/391]\t\tLoss: 1.6627 Acc@1: 52.930% [13617/25728]\n",
      "| Epoch [155/200] Iter[221/391]\t\tLoss: 3.0081 Acc@1: 52.707% [14909/28288]\n",
      "| Epoch [155/200] Iter[241/391]\t\tLoss: 2.7805 Acc@1: 52.037% [16052/30848]\n",
      "| Epoch [155/200] Iter[261/391]\t\tLoss: 3.1314 Acc@1: 51.519% [17211/33408]\n",
      "| Epoch [155/200] Iter[281/391]\t\tLoss: 3.0104 Acc@1: 51.368% [18476/35968]\n",
      "| Epoch [155/200] Iter[301/391]\t\tLoss: 2.2784 Acc@1: 51.107% [19690/38528]\n",
      "| Epoch [155/200] Iter[321/391]\t\tLoss: 3.1298 Acc@1: 50.703% [20832/41088]\n",
      "| Epoch [155/200] Iter[341/391]\t\tLoss: 2.8487 Acc@1: 50.483% [22035/43648]\n",
      "| Epoch [155/200] Iter[361/391]\t\tLoss: 1.5882 Acc@1: 50.164% [23179/46208]\n",
      "| Epoch [155/200] Iter[381/391]\t\tLoss: 3.0867 Acc@1: 50.126% [24445/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #155\n",
      "\n",
      "| Validation Epoch #155\t\t\tLoss: 1.3369 Acc@1: 65.13%\n",
      "\n",
      "----- Elapsed time : 0:46:06\n",
      "\n",
      "\n",
      "=> Training Epoch #156\n",
      "| Epoch [156/200] Iter[  1/391]\t\tLoss: 2.6165 Acc@1: 47.562% [ 60/128]\n",
      "| Epoch [156/200] Iter[ 21/391]\t\tLoss: 1.0319 Acc@1: 47.137% [1267/2688]\n",
      "| Epoch [156/200] Iter[ 41/391]\t\tLoss: 2.1032 Acc@1: 48.905% [2566/5248]\n",
      "| Epoch [156/200] Iter[ 61/391]\t\tLoss: 2.9296 Acc@1: 46.968% [3667/7808]\n",
      "| Epoch [156/200] Iter[ 81/391]\t\tLoss: 2.3328 Acc@1: 47.423% [4916/10368]\n",
      "| Epoch [156/200] Iter[101/391]\t\tLoss: 3.0433 Acc@1: 48.092% [6217/12928]\n",
      "| Epoch [156/200] Iter[121/391]\t\tLoss: 2.6525 Acc@1: 47.568% [7367/15488]\n",
      "| Epoch [156/200] Iter[141/391]\t\tLoss: 2.2368 Acc@1: 48.350% [8726/18048]\n",
      "| Epoch [156/200] Iter[161/391]\t\tLoss: 2.6747 Acc@1: 48.292% [9952/20608]\n",
      "| Epoch [156/200] Iter[181/391]\t\tLoss: 2.8922 Acc@1: 48.198% [11166/23168]\n",
      "| Epoch [156/200] Iter[201/391]\t\tLoss: 2.6912 Acc@1: 48.312% [12429/25728]\n",
      "| Epoch [156/200] Iter[221/391]\t\tLoss: 2.4293 Acc@1: 48.867% [13823/28288]\n",
      "| Epoch [156/200] Iter[241/391]\t\tLoss: 2.9776 Acc@1: 48.606% [14993/30848]\n",
      "| Epoch [156/200] Iter[261/391]\t\tLoss: 1.5482 Acc@1: 48.616% [16241/33408]\n",
      "| Epoch [156/200] Iter[281/391]\t\tLoss: 2.7369 Acc@1: 48.786% [17547/35968]\n",
      "| Epoch [156/200] Iter[301/391]\t\tLoss: 1.3097 Acc@1: 49.433% [19045/38528]\n",
      "| Epoch [156/200] Iter[321/391]\t\tLoss: 2.3387 Acc@1: 49.523% [20348/41088]\n",
      "| Epoch [156/200] Iter[341/391]\t\tLoss: 3.0005 Acc@1: 49.695% [21690/43648]\n",
      "| Epoch [156/200] Iter[361/391]\t\tLoss: 3.0831 Acc@1: 49.706% [22968/46208]\n",
      "| Epoch [156/200] Iter[381/391]\t\tLoss: 2.9279 Acc@1: 49.760% [24266/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #156\n",
      "\n",
      "| Validation Epoch #156\t\t\tLoss: 1.3566 Acc@1: 64.77%\n",
      "\n",
      "----- Elapsed time : 0:46:24\n",
      "\n",
      "\n",
      "=> Training Epoch #157\n",
      "| Epoch [157/200] Iter[  1/391]\t\tLoss: 3.0781 Acc@1: 32.185% [ 41/128]\n",
      "| Epoch [157/200] Iter[ 21/391]\t\tLoss: 2.5219 Acc@1: 52.138% [1401/2688]\n",
      "| Epoch [157/200] Iter[ 41/391]\t\tLoss: 1.4125 Acc@1: 51.079% [2680/5248]\n",
      "| Epoch [157/200] Iter[ 61/391]\t\tLoss: 1.8995 Acc@1: 51.566% [4026/7808]\n",
      "| Epoch [157/200] Iter[ 81/391]\t\tLoss: 1.0223 Acc@1: 52.063% [5397/10368]\n",
      "| Epoch [157/200] Iter[101/391]\t\tLoss: 2.4806 Acc@1: 51.173% [6615/12928]\n",
      "| Epoch [157/200] Iter[121/391]\t\tLoss: 1.5543 Acc@1: 50.833% [7872/15488]\n",
      "| Epoch [157/200] Iter[141/391]\t\tLoss: 1.6285 Acc@1: 51.727% [9335/18048]\n",
      "| Epoch [157/200] Iter[161/391]\t\tLoss: 1.9584 Acc@1: 51.459% [10604/20608]\n",
      "| Epoch [157/200] Iter[181/391]\t\tLoss: 3.1395 Acc@1: 52.189% [12091/23168]\n",
      "| Epoch [157/200] Iter[201/391]\t\tLoss: 2.9994 Acc@1: 51.453% [13237/25728]\n",
      "| Epoch [157/200] Iter[221/391]\t\tLoss: 1.1554 Acc@1: 51.443% [14552/28288]\n",
      "| Epoch [157/200] Iter[241/391]\t\tLoss: 2.8263 Acc@1: 50.646% [15623/30848]\n",
      "| Epoch [157/200] Iter[261/391]\t\tLoss: 3.0317 Acc@1: 50.804% [16972/33408]\n",
      "| Epoch [157/200] Iter[281/391]\t\tLoss: 2.4221 Acc@1: 50.827% [18281/35968]\n",
      "| Epoch [157/200] Iter[301/391]\t\tLoss: 2.6114 Acc@1: 50.763% [19557/38528]\n",
      "| Epoch [157/200] Iter[321/391]\t\tLoss: 2.8771 Acc@1: 50.368% [20695/41088]\n",
      "| Epoch [157/200] Iter[341/391]\t\tLoss: 2.6349 Acc@1: 49.975% [21813/43648]\n",
      "| Epoch [157/200] Iter[361/391]\t\tLoss: 1.1288 Acc@1: 50.146% [23171/46208]\n",
      "| Epoch [157/200] Iter[381/391]\t\tLoss: 1.7645 Acc@1: 49.952% [24360/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #157\n",
      "\n",
      "| Validation Epoch #157\t\t\tLoss: 1.3423 Acc@1: 64.62%\n",
      "\n",
      "----- Elapsed time : 0:46:42\n",
      "\n",
      "\n",
      "=> Training Epoch #158\n",
      "| Epoch [158/200] Iter[  1/391]\t\tLoss: 2.8125 Acc@1: 38.457% [ 49/128]\n",
      "| Epoch [158/200] Iter[ 21/391]\t\tLoss: 2.6607 Acc@1: 47.581% [1278/2688]\n",
      "| Epoch [158/200] Iter[ 41/391]\t\tLoss: 2.2740 Acc@1: 47.095% [2471/5248]\n",
      "| Epoch [158/200] Iter[ 61/391]\t\tLoss: 2.3117 Acc@1: 48.443% [3782/7808]\n",
      "| Epoch [158/200] Iter[ 81/391]\t\tLoss: 2.4652 Acc@1: 47.795% [4955/10368]\n",
      "| Epoch [158/200] Iter[101/391]\t\tLoss: 2.0150 Acc@1: 48.476% [6267/12928]\n",
      "| Epoch [158/200] Iter[121/391]\t\tLoss: 2.6326 Acc@1: 48.557% [7520/15488]\n",
      "| Epoch [158/200] Iter[141/391]\t\tLoss: 1.2027 Acc@1: 48.640% [8778/18048]\n",
      "| Epoch [158/200] Iter[161/391]\t\tLoss: 1.1143 Acc@1: 49.489% [10198/20608]\n",
      "| Epoch [158/200] Iter[181/391]\t\tLoss: 1.5167 Acc@1: 49.731% [11521/23168]\n",
      "| Epoch [158/200] Iter[201/391]\t\tLoss: 3.0446 Acc@1: 50.100% [12889/25728]\n",
      "| Epoch [158/200] Iter[221/391]\t\tLoss: 1.3475 Acc@1: 50.004% [14145/28288]\n",
      "| Epoch [158/200] Iter[241/391]\t\tLoss: 3.0804 Acc@1: 49.678% [15324/30848]\n",
      "| Epoch [158/200] Iter[261/391]\t\tLoss: 2.7197 Acc@1: 49.556% [16555/33408]\n",
      "| Epoch [158/200] Iter[281/391]\t\tLoss: 2.9952 Acc@1: 49.550% [17822/35968]\n",
      "| Epoch [158/200] Iter[301/391]\t\tLoss: 2.4313 Acc@1: 49.654% [19130/38528]\n",
      "| Epoch [158/200] Iter[321/391]\t\tLoss: 1.2343 Acc@1: 49.828% [20473/41088]\n",
      "| Epoch [158/200] Iter[341/391]\t\tLoss: 1.6774 Acc@1: 50.109% [21871/43648]\n",
      "| Epoch [158/200] Iter[361/391]\t\tLoss: 2.7101 Acc@1: 50.123% [23161/46208]\n",
      "| Epoch [158/200] Iter[381/391]\t\tLoss: 3.1181 Acc@1: 50.217% [24489/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #158\n",
      "\n",
      "| Validation Epoch #158\t\t\tLoss: 1.3145 Acc@1: 65.07%\n",
      "\n",
      "----- Elapsed time : 0:47:00\n",
      "\n",
      "\n",
      "=> Training Epoch #159\n",
      "| Epoch [159/200] Iter[  1/391]\t\tLoss: 3.0491 Acc@1: 33.854% [ 43/128]\n",
      "| Epoch [159/200] Iter[ 21/391]\t\tLoss: 1.9257 Acc@1: 57.046% [1533/2688]\n",
      "| Epoch [159/200] Iter[ 41/391]\t\tLoss: 3.0387 Acc@1: 50.747% [2663/5248]\n",
      "| Epoch [159/200] Iter[ 61/391]\t\tLoss: 3.2411 Acc@1: 49.854% [3892/7808]\n",
      "| Epoch [159/200] Iter[ 81/391]\t\tLoss: 3.0314 Acc@1: 50.942% [5281/10368]\n",
      "| Epoch [159/200] Iter[101/391]\t\tLoss: 2.4269 Acc@1: 51.486% [6656/12928]\n",
      "| Epoch [159/200] Iter[121/391]\t\tLoss: 2.8415 Acc@1: 50.971% [7894/15488]\n",
      "| Epoch [159/200] Iter[141/391]\t\tLoss: 1.4900 Acc@1: 51.137% [9229/18048]\n",
      "| Epoch [159/200] Iter[161/391]\t\tLoss: 1.5990 Acc@1: 51.506% [10614/20608]\n",
      "| Epoch [159/200] Iter[181/391]\t\tLoss: 1.0750 Acc@1: 52.135% [12078/23168]\n",
      "| Epoch [159/200] Iter[201/391]\t\tLoss: 1.5115 Acc@1: 51.953% [13366/25728]\n",
      "| Epoch [159/200] Iter[221/391]\t\tLoss: 2.2954 Acc@1: 51.777% [14646/28288]\n",
      "| Epoch [159/200] Iter[241/391]\t\tLoss: 2.0869 Acc@1: 51.338% [15836/30848]\n",
      "| Epoch [159/200] Iter[261/391]\t\tLoss: 3.0099 Acc@1: 51.079% [17064/33408]\n",
      "| Epoch [159/200] Iter[281/391]\t\tLoss: 1.9610 Acc@1: 51.235% [18428/35968]\n",
      "| Epoch [159/200] Iter[301/391]\t\tLoss: 2.4164 Acc@1: 50.910% [19614/38528]\n",
      "| Epoch [159/200] Iter[321/391]\t\tLoss: 1.9350 Acc@1: 51.409% [21122/41088]\n",
      "| Epoch [159/200] Iter[341/391]\t\tLoss: 3.1925 Acc@1: 51.479% [22469/43648]\n",
      "| Epoch [159/200] Iter[361/391]\t\tLoss: 3.0848 Acc@1: 51.439% [23768/46208]\n",
      "| Epoch [159/200] Iter[381/391]\t\tLoss: 1.8479 Acc@1: 51.294% [25015/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #159\n",
      "\n",
      "| Validation Epoch #159\t\t\tLoss: 1.4035 Acc@1: 65.21%\n",
      "\n",
      "----- Elapsed time : 0:47:18\n",
      "\n",
      "\n",
      "=> Training Epoch #160\n",
      "| Epoch [160/200] Iter[  1/391]\t\tLoss: 2.5612 Acc@1: 43.194% [ 55/128]\n",
      "| Epoch [160/200] Iter[ 21/391]\t\tLoss: 1.4406 Acc@1: 48.017% [1290/2688]\n",
      "| Epoch [160/200] Iter[ 41/391]\t\tLoss: 3.0796 Acc@1: 49.273% [2585/5248]\n",
      "| Epoch [160/200] Iter[ 61/391]\t\tLoss: 1.5228 Acc@1: 51.407% [4013/7808]\n",
      "| Epoch [160/200] Iter[ 81/391]\t\tLoss: 2.9751 Acc@1: 50.276% [5212/10368]\n",
      "| Epoch [160/200] Iter[101/391]\t\tLoss: 1.4456 Acc@1: 50.349% [6509/12928]\n",
      "| Epoch [160/200] Iter[121/391]\t\tLoss: 1.5506 Acc@1: 50.630% [7841/15488]\n",
      "| Epoch [160/200] Iter[141/391]\t\tLoss: 1.5523 Acc@1: 50.437% [9102/18048]\n",
      "| Epoch [160/200] Iter[161/391]\t\tLoss: 2.4140 Acc@1: 50.302% [10366/20608]\n",
      "| Epoch [160/200] Iter[181/391]\t\tLoss: 3.0582 Acc@1: 50.707% [11747/23168]\n",
      "| Epoch [160/200] Iter[201/391]\t\tLoss: 1.1945 Acc@1: 50.614% [13022/25728]\n",
      "| Epoch [160/200] Iter[221/391]\t\tLoss: 3.0430 Acc@1: 50.686% [14338/28288]\n",
      "| Epoch [160/200] Iter[241/391]\t\tLoss: 1.8173 Acc@1: 50.921% [15708/30848]\n",
      "| Epoch [160/200] Iter[261/391]\t\tLoss: 1.2837 Acc@1: 51.513% [17209/33408]\n",
      "| Epoch [160/200] Iter[281/391]\t\tLoss: 1.3998 Acc@1: 51.230% [18426/35968]\n",
      "| Epoch [160/200] Iter[301/391]\t\tLoss: 3.0451 Acc@1: 50.880% [19602/38528]\n",
      "| Epoch [160/200] Iter[321/391]\t\tLoss: 3.0218 Acc@1: 50.786% [20866/41088]\n",
      "| Epoch [160/200] Iter[341/391]\t\tLoss: 1.3967 Acc@1: 50.950% [22238/43648]\n",
      "| Epoch [160/200] Iter[361/391]\t\tLoss: 2.6718 Acc@1: 50.771% [23460/46208]\n",
      "| Epoch [160/200] Iter[381/391]\t\tLoss: 1.8458 Acc@1: 50.619% [24685/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #160\n",
      "\n",
      "| Validation Epoch #160\t\t\tLoss: 1.3105 Acc@1: 64.88%\n",
      "\n",
      "----- Elapsed time : 0:47:36\n",
      "\n",
      "\n",
      "=> Training Epoch #161\n",
      "| Epoch [161/200] Iter[  1/391]\t\tLoss: 2.0210 Acc@1: 60.174% [ 77/128]\n",
      "| Epoch [161/200] Iter[ 21/391]\t\tLoss: 2.2439 Acc@1: 50.726% [1363/2688]\n",
      "| Epoch [161/200] Iter[ 41/391]\t\tLoss: 1.3714 Acc@1: 53.068% [2784/5248]\n",
      "| Epoch [161/200] Iter[ 61/391]\t\tLoss: 2.5851 Acc@1: 52.183% [4074/7808]\n",
      "| Epoch [161/200] Iter[ 81/391]\t\tLoss: 2.6404 Acc@1: 50.644% [5250/10368]\n",
      "| Epoch [161/200] Iter[101/391]\t\tLoss: 2.2435 Acc@1: 50.924% [6583/12928]\n",
      "| Epoch [161/200] Iter[121/391]\t\tLoss: 1.0626 Acc@1: 51.468% [7971/15488]\n",
      "| Epoch [161/200] Iter[141/391]\t\tLoss: 3.1973 Acc@1: 51.304% [9259/18048]\n",
      "| Epoch [161/200] Iter[161/391]\t\tLoss: 2.4564 Acc@1: 51.682% [10650/20608]\n",
      "| Epoch [161/200] Iter[181/391]\t\tLoss: 2.7175 Acc@1: 51.407% [11910/23168]\n",
      "| Epoch [161/200] Iter[201/391]\t\tLoss: 3.0587 Acc@1: 51.285% [13194/25728]\n",
      "| Epoch [161/200] Iter[221/391]\t\tLoss: 3.0536 Acc@1: 51.477% [14561/28288]\n",
      "| Epoch [161/200] Iter[241/391]\t\tLoss: 2.6109 Acc@1: 51.512% [15890/30848]\n",
      "| Epoch [161/200] Iter[261/391]\t\tLoss: 3.0908 Acc@1: 51.497% [17204/33408]\n",
      "| Epoch [161/200] Iter[281/391]\t\tLoss: 1.7869 Acc@1: 51.730% [18606/35968]\n",
      "| Epoch [161/200] Iter[301/391]\t\tLoss: 2.8695 Acc@1: 51.177% [19717/38528]\n",
      "| Epoch [161/200] Iter[321/391]\t\tLoss: 1.5573 Acc@1: 51.279% [21069/41088]\n",
      "| Epoch [161/200] Iter[341/391]\t\tLoss: 3.1666 Acc@1: 51.199% [22347/43648]\n",
      "| Epoch [161/200] Iter[361/391]\t\tLoss: 2.7391 Acc@1: 51.477% [23786/46208]\n",
      "| Epoch [161/200] Iter[381/391]\t\tLoss: 3.0061 Acc@1: 51.791% [25257/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #161\n",
      "\n",
      "| Validation Epoch #161\t\t\tLoss: 1.2553 Acc@1: 66.08%\n",
      "\n",
      "----- Elapsed time : 0:47:53\n",
      "\n",
      "\n",
      "=> Training Epoch #162\n",
      "| Epoch [162/200] Iter[  1/391]\t\tLoss: 3.2568 Acc@1: 26.235% [ 33/128]\n",
      "| Epoch [162/200] Iter[ 21/391]\t\tLoss: 2.6109 Acc@1: 50.566% [1359/2688]\n",
      "| Epoch [162/200] Iter[ 41/391]\t\tLoss: 1.7318 Acc@1: 50.540% [2652/5248]\n",
      "| Epoch [162/200] Iter[ 61/391]\t\tLoss: 3.3105 Acc@1: 51.384% [4012/7808]\n",
      "| Epoch [162/200] Iter[ 81/391]\t\tLoss: 2.8008 Acc@1: 49.779% [5161/10368]\n",
      "| Epoch [162/200] Iter[101/391]\t\tLoss: 0.9520 Acc@1: 49.025% [6337/12928]\n",
      "| Epoch [162/200] Iter[121/391]\t\tLoss: 2.6670 Acc@1: 48.443% [7502/15488]\n",
      "| Epoch [162/200] Iter[141/391]\t\tLoss: 2.0143 Acc@1: 48.244% [8707/18048]\n",
      "| Epoch [162/200] Iter[161/391]\t\tLoss: 3.4155 Acc@1: 48.078% [9907/20608]\n",
      "| Epoch [162/200] Iter[181/391]\t\tLoss: 2.1815 Acc@1: 48.507% [11238/23168]\n",
      "| Epoch [162/200] Iter[201/391]\t\tLoss: 3.1255 Acc@1: 47.832% [12306/25728]\n",
      "| Epoch [162/200] Iter[221/391]\t\tLoss: 1.4120 Acc@1: 48.393% [13689/28288]\n",
      "| Epoch [162/200] Iter[241/391]\t\tLoss: 2.9130 Acc@1: 48.512% [14965/30848]\n",
      "| Epoch [162/200] Iter[261/391]\t\tLoss: 2.9262 Acc@1: 48.228% [16112/33408]\n",
      "| Epoch [162/200] Iter[281/391]\t\tLoss: 3.0012 Acc@1: 48.441% [17423/35968]\n",
      "| Epoch [162/200] Iter[301/391]\t\tLoss: 3.0942 Acc@1: 48.713% [18768/38528]\n",
      "| Epoch [162/200] Iter[321/391]\t\tLoss: 2.6199 Acc@1: 48.958% [20115/41088]\n",
      "| Epoch [162/200] Iter[341/391]\t\tLoss: 3.1257 Acc@1: 48.955% [21367/43648]\n",
      "| Epoch [162/200] Iter[361/391]\t\tLoss: 2.1152 Acc@1: 49.375% [22815/46208]\n",
      "| Epoch [162/200] Iter[381/391]\t\tLoss: 1.6590 Acc@1: 49.314% [24049/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #162\n",
      "\n",
      "| Validation Epoch #162\t\t\tLoss: 1.3249 Acc@1: 65.43%\n",
      "\n",
      "----- Elapsed time : 0:48:11\n",
      "\n",
      "\n",
      "=> Training Epoch #163\n",
      "| Epoch [163/200] Iter[  1/391]\t\tLoss: 1.5246 Acc@1: 69.530% [ 88/128]\n",
      "| Epoch [163/200] Iter[ 21/391]\t\tLoss: 2.8824 Acc@1: 52.933% [1422/2688]\n",
      "| Epoch [163/200] Iter[ 41/391]\t\tLoss: 1.8955 Acc@1: 50.154% [2632/5248]\n",
      "| Epoch [163/200] Iter[ 61/391]\t\tLoss: 2.9484 Acc@1: 48.191% [3762/7808]\n",
      "| Epoch [163/200] Iter[ 81/391]\t\tLoss: 2.7919 Acc@1: 47.337% [4907/10368]\n",
      "| Epoch [163/200] Iter[101/391]\t\tLoss: 1.4758 Acc@1: 46.184% [5970/12928]\n",
      "| Epoch [163/200] Iter[121/391]\t\tLoss: 2.4332 Acc@1: 46.722% [7236/15488]\n",
      "| Epoch [163/200] Iter[141/391]\t\tLoss: 1.9649 Acc@1: 47.279% [8532/18048]\n",
      "| Epoch [163/200] Iter[161/391]\t\tLoss: 2.4962 Acc@1: 47.424% [9773/20608]\n",
      "| Epoch [163/200] Iter[181/391]\t\tLoss: 1.7844 Acc@1: 48.572% [11253/23168]\n",
      "| Epoch [163/200] Iter[201/391]\t\tLoss: 3.0455 Acc@1: 48.745% [12541/25728]\n",
      "| Epoch [163/200] Iter[221/391]\t\tLoss: 3.0557 Acc@1: 49.163% [13907/28288]\n",
      "| Epoch [163/200] Iter[241/391]\t\tLoss: 2.9385 Acc@1: 49.283% [15202/30848]\n",
      "| Epoch [163/200] Iter[261/391]\t\tLoss: 1.5942 Acc@1: 49.734% [16615/33408]\n",
      "| Epoch [163/200] Iter[281/391]\t\tLoss: 2.8218 Acc@1: 49.079% [17652/35968]\n",
      "| Epoch [163/200] Iter[301/391]\t\tLoss: 2.3618 Acc@1: 49.108% [18920/38528]\n",
      "| Epoch [163/200] Iter[321/391]\t\tLoss: 1.6497 Acc@1: 48.975% [20122/41088]\n",
      "| Epoch [163/200] Iter[341/391]\t\tLoss: 2.3924 Acc@1: 48.852% [21322/43648]\n",
      "| Epoch [163/200] Iter[361/391]\t\tLoss: 1.7057 Acc@1: 49.272% [22767/46208]\n",
      "| Epoch [163/200] Iter[381/391]\t\tLoss: 1.6550 Acc@1: 49.478% [24129/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #163\n",
      "\n",
      "| Validation Epoch #163\t\t\tLoss: 1.3573 Acc@1: 65.58%\n",
      "\n",
      "----- Elapsed time : 0:48:28\n",
      "\n",
      "\n",
      "=> Training Epoch #164\n",
      "| Epoch [164/200] Iter[  1/391]\t\tLoss: 1.6506 Acc@1: 66.303% [ 84/128]\n",
      "| Epoch [164/200] Iter[ 21/391]\t\tLoss: 1.2615 Acc@1: 47.323% [1272/2688]\n",
      "| Epoch [164/200] Iter[ 41/391]\t\tLoss: 1.8172 Acc@1: 49.399% [2592/5248]\n",
      "| Epoch [164/200] Iter[ 61/391]\t\tLoss: 1.9544 Acc@1: 48.525% [3788/7808]\n",
      "| Epoch [164/200] Iter[ 81/391]\t\tLoss: 2.9219 Acc@1: 48.847% [5064/10368]\n",
      "| Epoch [164/200] Iter[101/391]\t\tLoss: 2.2652 Acc@1: 48.478% [6267/12928]\n",
      "| Epoch [164/200] Iter[121/391]\t\tLoss: 1.8980 Acc@1: 48.625% [7530/15488]\n",
      "| Epoch [164/200] Iter[141/391]\t\tLoss: 2.9238 Acc@1: 48.125% [8685/18048]\n",
      "| Epoch [164/200] Iter[161/391]\t\tLoss: 2.2645 Acc@1: 48.217% [9936/20608]\n",
      "| Epoch [164/200] Iter[181/391]\t\tLoss: 1.1442 Acc@1: 48.885% [11325/23168]\n",
      "| Epoch [164/200] Iter[201/391]\t\tLoss: 1.1703 Acc@1: 48.721% [12535/25728]\n",
      "| Epoch [164/200] Iter[221/391]\t\tLoss: 3.1456 Acc@1: 49.042% [13873/28288]\n",
      "| Epoch [164/200] Iter[241/391]\t\tLoss: 2.4894 Acc@1: 48.925% [15092/30848]\n",
      "| Epoch [164/200] Iter[261/391]\t\tLoss: 2.8177 Acc@1: 48.974% [16361/33408]\n",
      "| Epoch [164/200] Iter[281/391]\t\tLoss: 2.9840 Acc@1: 48.781% [17545/35968]\n",
      "| Epoch [164/200] Iter[301/391]\t\tLoss: 3.2416 Acc@1: 48.919% [18847/38528]\n",
      "| Epoch [164/200] Iter[321/391]\t\tLoss: 3.0265 Acc@1: 49.068% [20161/41088]\n",
      "| Epoch [164/200] Iter[341/391]\t\tLoss: 3.0333 Acc@1: 49.510% [21610/43648]\n",
      "| Epoch [164/200] Iter[361/391]\t\tLoss: 2.4404 Acc@1: 49.581% [22910/46208]\n",
      "| Epoch [164/200] Iter[381/391]\t\tLoss: 3.0427 Acc@1: 49.846% [24308/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #164\n",
      "\n",
      "| Validation Epoch #164\t\t\tLoss: 1.3256 Acc@1: 65.35%\n",
      "\n",
      "----- Elapsed time : 0:48:46\n",
      "\n",
      "\n",
      "=> Training Epoch #165\n",
      "| Epoch [165/200] Iter[  1/391]\t\tLoss: 1.9771 Acc@1: 64.271% [ 82/128]\n",
      "| Epoch [165/200] Iter[ 21/391]\t\tLoss: 3.0033 Acc@1: 49.653% [1334/2688]\n",
      "| Epoch [165/200] Iter[ 41/391]\t\tLoss: 3.0104 Acc@1: 47.485% [2492/5248]\n",
      "| Epoch [165/200] Iter[ 61/391]\t\tLoss: 3.0219 Acc@1: 48.693% [3801/7808]\n",
      "| Epoch [165/200] Iter[ 81/391]\t\tLoss: 3.1715 Acc@1: 48.814% [5061/10368]\n",
      "| Epoch [165/200] Iter[101/391]\t\tLoss: 2.8209 Acc@1: 48.447% [6263/12928]\n",
      "| Epoch [165/200] Iter[121/391]\t\tLoss: 3.0043 Acc@1: 48.684% [7540/15488]\n",
      "| Epoch [165/200] Iter[141/391]\t\tLoss: 3.1902 Acc@1: 48.169% [8693/18048]\n",
      "| Epoch [165/200] Iter[161/391]\t\tLoss: 1.4436 Acc@1: 48.222% [9937/20608]\n",
      "| Epoch [165/200] Iter[181/391]\t\tLoss: 2.8918 Acc@1: 48.559% [11250/23168]\n",
      "| Epoch [165/200] Iter[201/391]\t\tLoss: 3.2139 Acc@1: 48.684% [12525/25728]\n",
      "| Epoch [165/200] Iter[221/391]\t\tLoss: 2.7963 Acc@1: 48.475% [13712/28288]\n",
      "| Epoch [165/200] Iter[241/391]\t\tLoss: 1.0060 Acc@1: 48.722% [15029/30848]\n",
      "| Epoch [165/200] Iter[261/391]\t\tLoss: 2.0203 Acc@1: 48.987% [16365/33408]\n",
      "| Epoch [165/200] Iter[281/391]\t\tLoss: 2.6946 Acc@1: 48.825% [17561/35968]\n",
      "| Epoch [165/200] Iter[301/391]\t\tLoss: 2.6322 Acc@1: 48.962% [18864/38528]\n",
      "| Epoch [165/200] Iter[321/391]\t\tLoss: 1.1723 Acc@1: 49.064% [20159/41088]\n",
      "| Epoch [165/200] Iter[341/391]\t\tLoss: 2.8409 Acc@1: 49.187% [21469/43648]\n",
      "| Epoch [165/200] Iter[361/391]\t\tLoss: 2.8423 Acc@1: 48.919% [22604/46208]\n",
      "| Epoch [165/200] Iter[381/391]\t\tLoss: 2.6398 Acc@1: 48.636% [23718/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #165\n",
      "\n",
      "| Validation Epoch #165\t\t\tLoss: 1.3772 Acc@1: 64.90%\n",
      "\n",
      "----- Elapsed time : 0:49:04\n",
      "\n",
      "\n",
      "=> Training Epoch #166\n",
      "| Epoch [166/200] Iter[  1/391]\t\tLoss: 3.1496 Acc@1: 24.667% [ 31/128]\n",
      "| Epoch [166/200] Iter[ 21/391]\t\tLoss: 2.5175 Acc@1: 53.403% [1435/2688]\n",
      "| Epoch [166/200] Iter[ 41/391]\t\tLoss: 3.0049 Acc@1: 51.113% [2682/5248]\n",
      "| Epoch [166/200] Iter[ 61/391]\t\tLoss: 2.5858 Acc@1: 49.639% [3875/7808]\n",
      "| Epoch [166/200] Iter[ 81/391]\t\tLoss: 2.3977 Acc@1: 49.490% [5131/10368]\n",
      "| Epoch [166/200] Iter[101/391]\t\tLoss: 2.6405 Acc@1: 49.788% [6436/12928]\n",
      "| Epoch [166/200] Iter[121/391]\t\tLoss: 1.7603 Acc@1: 49.997% [7743/15488]\n",
      "| Epoch [166/200] Iter[141/391]\t\tLoss: 2.8240 Acc@1: 49.602% [8952/18048]\n",
      "| Epoch [166/200] Iter[161/391]\t\tLoss: 1.2887 Acc@1: 50.195% [10344/20608]\n",
      "| Epoch [166/200] Iter[181/391]\t\tLoss: 2.2328 Acc@1: 50.030% [11590/23168]\n",
      "| Epoch [166/200] Iter[201/391]\t\tLoss: 2.7955 Acc@1: 50.181% [12910/25728]\n",
      "| Epoch [166/200] Iter[221/391]\t\tLoss: 3.1144 Acc@1: 50.301% [14229/28288]\n",
      "| Epoch [166/200] Iter[241/391]\t\tLoss: 1.1826 Acc@1: 50.183% [15480/30848]\n",
      "| Epoch [166/200] Iter[261/391]\t\tLoss: 1.5349 Acc@1: 50.626% [16913/33408]\n",
      "| Epoch [166/200] Iter[281/391]\t\tLoss: 3.0746 Acc@1: 50.951% [18326/35968]\n",
      "| Epoch [166/200] Iter[301/391]\t\tLoss: 2.6470 Acc@1: 50.519% [19463/38528]\n",
      "| Epoch [166/200] Iter[321/391]\t\tLoss: 1.1763 Acc@1: 50.639% [20806/41088]\n",
      "| Epoch [166/200] Iter[341/391]\t\tLoss: 2.0241 Acc@1: 50.901% [22217/43648]\n",
      "| Epoch [166/200] Iter[361/391]\t\tLoss: 1.6145 Acc@1: 50.707% [23430/46208]\n",
      "| Epoch [166/200] Iter[381/391]\t\tLoss: 1.3771 Acc@1: 50.652% [24701/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #166\n",
      "\n",
      "| Validation Epoch #166\t\t\tLoss: 1.2383 Acc@1: 65.58%\n",
      "\n",
      "----- Elapsed time : 0:49:22\n",
      "\n",
      "\n",
      "=> Training Epoch #167\n",
      "| Epoch [167/200] Iter[  1/391]\t\tLoss: 2.8048 Acc@1: 38.256% [ 48/128]\n",
      "| Epoch [167/200] Iter[ 21/391]\t\tLoss: 2.8134 Acc@1: 52.365% [1407/2688]\n",
      "| Epoch [167/200] Iter[ 41/391]\t\tLoss: 2.6902 Acc@1: 53.547% [2810/5248]\n",
      "| Epoch [167/200] Iter[ 61/391]\t\tLoss: 2.0183 Acc@1: 52.355% [4087/7808]\n",
      "| Epoch [167/200] Iter[ 81/391]\t\tLoss: 1.0024 Acc@1: 51.754% [5365/10368]\n",
      "| Epoch [167/200] Iter[101/391]\t\tLoss: 2.9710 Acc@1: 52.202% [6748/12928]\n",
      "| Epoch [167/200] Iter[121/391]\t\tLoss: 2.9624 Acc@1: 53.439% [8276/15488]\n",
      "| Epoch [167/200] Iter[141/391]\t\tLoss: 3.0436 Acc@1: 52.822% [9533/18048]\n",
      "| Epoch [167/200] Iter[161/391]\t\tLoss: 2.6910 Acc@1: 52.416% [10801/20608]\n",
      "| Epoch [167/200] Iter[181/391]\t\tLoss: 2.6095 Acc@1: 53.095% [12301/23168]\n",
      "| Epoch [167/200] Iter[201/391]\t\tLoss: 1.8644 Acc@1: 52.561% [13523/25728]\n",
      "| Epoch [167/200] Iter[221/391]\t\tLoss: 3.1510 Acc@1: 51.754% [14640/28288]\n",
      "| Epoch [167/200] Iter[241/391]\t\tLoss: 3.1196 Acc@1: 51.375% [15848/30848]\n",
      "| Epoch [167/200] Iter[261/391]\t\tLoss: 1.2849 Acc@1: 51.427% [17180/33408]\n",
      "| Epoch [167/200] Iter[281/391]\t\tLoss: 2.8545 Acc@1: 51.126% [18389/35968]\n",
      "| Epoch [167/200] Iter[301/391]\t\tLoss: 2.2083 Acc@1: 50.695% [19531/38528]\n",
      "| Epoch [167/200] Iter[321/391]\t\tLoss: 2.9150 Acc@1: 50.510% [20753/41088]\n",
      "| Epoch [167/200] Iter[341/391]\t\tLoss: 2.8950 Acc@1: 50.151% [21890/43648]\n",
      "| Epoch [167/200] Iter[361/391]\t\tLoss: 1.5807 Acc@1: 50.273% [23230/46208]\n",
      "| Epoch [167/200] Iter[381/391]\t\tLoss: 3.0504 Acc@1: 50.063% [24414/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #167\n",
      "\n",
      "| Validation Epoch #167\t\t\tLoss: 1.2874 Acc@1: 65.52%\n",
      "\n",
      "----- Elapsed time : 0:49:40\n",
      "\n",
      "\n",
      "=> Training Epoch #168\n",
      "| Epoch [168/200] Iter[  1/391]\t\tLoss: 2.4600 Acc@1: 53.049% [ 67/128]\n",
      "| Epoch [168/200] Iter[ 21/391]\t\tLoss: 3.1595 Acc@1: 51.348% [1380/2688]\n",
      "| Epoch [168/200] Iter[ 41/391]\t\tLoss: 1.8846 Acc@1: 48.482% [2544/5248]\n",
      "| Epoch [168/200] Iter[ 61/391]\t\tLoss: 2.8846 Acc@1: 49.455% [3861/7808]\n",
      "| Epoch [168/200] Iter[ 81/391]\t\tLoss: 2.2728 Acc@1: 49.413% [5123/10368]\n",
      "| Epoch [168/200] Iter[101/391]\t\tLoss: 3.1688 Acc@1: 48.703% [6296/12928]\n",
      "| Epoch [168/200] Iter[121/391]\t\tLoss: 2.1946 Acc@1: 48.380% [7493/15488]\n",
      "| Epoch [168/200] Iter[141/391]\t\tLoss: 1.1295 Acc@1: 49.524% [8938/18048]\n",
      "| Epoch [168/200] Iter[161/391]\t\tLoss: 1.7970 Acc@1: 50.751% [10458/20608]\n",
      "| Epoch [168/200] Iter[181/391]\t\tLoss: 1.9514 Acc@1: 50.245% [11640/23168]\n",
      "| Epoch [168/200] Iter[201/391]\t\tLoss: 1.1135 Acc@1: 50.957% [13110/25728]\n",
      "| Epoch [168/200] Iter[221/391]\t\tLoss: 2.5057 Acc@1: 51.546% [14581/28288]\n",
      "| Epoch [168/200] Iter[241/391]\t\tLoss: 2.9899 Acc@1: 51.090% [15760/30848]\n",
      "| Epoch [168/200] Iter[261/391]\t\tLoss: 2.9407 Acc@1: 51.044% [17052/33408]\n",
      "| Epoch [168/200] Iter[281/391]\t\tLoss: 2.9897 Acc@1: 50.817% [18277/35968]\n",
      "| Epoch [168/200] Iter[301/391]\t\tLoss: 2.9450 Acc@1: 50.938% [19625/38528]\n",
      "| Epoch [168/200] Iter[321/391]\t\tLoss: 2.8648 Acc@1: 51.233% [21050/41088]\n",
      "| Epoch [168/200] Iter[341/391]\t\tLoss: 2.5794 Acc@1: 51.552% [22501/43648]\n",
      "| Epoch [168/200] Iter[361/391]\t\tLoss: 2.0186 Acc@1: 51.421% [23760/46208]\n",
      "| Epoch [168/200] Iter[381/391]\t\tLoss: 3.1973 Acc@1: 51.213% [24975/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #168\n",
      "\n",
      "| Validation Epoch #168\t\t\tLoss: 1.3462 Acc@1: 65.78%\n",
      "\n",
      "----- Elapsed time : 0:49:57\n",
      "\n",
      "\n",
      "=> Training Epoch #169\n",
      "| Epoch [169/200] Iter[  1/391]\t\tLoss: 2.5248 Acc@1: 50.611% [ 64/128]\n",
      "| Epoch [169/200] Iter[ 21/391]\t\tLoss: 2.2679 Acc@1: 50.971% [1370/2688]\n",
      "| Epoch [169/200] Iter[ 41/391]\t\tLoss: 2.8915 Acc@1: 50.099% [2629/5248]\n",
      "| Epoch [169/200] Iter[ 61/391]\t\tLoss: 2.8570 Acc@1: 50.671% [3956/7808]\n",
      "| Epoch [169/200] Iter[ 81/391]\t\tLoss: 3.1791 Acc@1: 50.990% [5286/10368]\n",
      "| Epoch [169/200] Iter[101/391]\t\tLoss: 2.2278 Acc@1: 51.402% [6645/12928]\n",
      "| Epoch [169/200] Iter[121/391]\t\tLoss: 2.4153 Acc@1: 51.873% [8034/15488]\n",
      "| Epoch [169/200] Iter[141/391]\t\tLoss: 1.7578 Acc@1: 51.211% [9242/18048]\n",
      "| Epoch [169/200] Iter[161/391]\t\tLoss: 2.8558 Acc@1: 50.544% [10416/20608]\n",
      "| Epoch [169/200] Iter[181/391]\t\tLoss: 2.8643 Acc@1: 50.267% [11645/23168]\n",
      "| Epoch [169/200] Iter[201/391]\t\tLoss: 1.3501 Acc@1: 50.630% [13026/25728]\n",
      "| Epoch [169/200] Iter[221/391]\t\tLoss: 1.4643 Acc@1: 50.403% [14258/28288]\n",
      "| Epoch [169/200] Iter[241/391]\t\tLoss: 2.6629 Acc@1: 50.534% [15588/30848]\n",
      "| Epoch [169/200] Iter[261/391]\t\tLoss: 3.0447 Acc@1: 50.688% [16933/33408]\n",
      "| Epoch [169/200] Iter[281/391]\t\tLoss: 1.2679 Acc@1: 50.794% [18269/35968]\n",
      "| Epoch [169/200] Iter[301/391]\t\tLoss: 3.0455 Acc@1: 50.420% [19425/38528]\n",
      "| Epoch [169/200] Iter[321/391]\t\tLoss: 1.3687 Acc@1: 50.694% [20829/41088]\n",
      "| Epoch [169/200] Iter[341/391]\t\tLoss: 2.8014 Acc@1: 50.353% [21978/43648]\n",
      "| Epoch [169/200] Iter[361/391]\t\tLoss: 1.7813 Acc@1: 50.095% [23147/46208]\n",
      "| Epoch [169/200] Iter[381/391]\t\tLoss: 3.0386 Acc@1: 50.206% [24484/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #169\n",
      "\n",
      "| Validation Epoch #169\t\t\tLoss: 1.3502 Acc@1: 65.56%\n",
      "\n",
      "----- Elapsed time : 0:50:15\n",
      "\n",
      "\n",
      "=> Training Epoch #170\n",
      "| Epoch [170/200] Iter[  1/391]\t\tLoss: 2.4221 Acc@1: 52.228% [ 66/128]\n",
      "| Epoch [170/200] Iter[ 21/391]\t\tLoss: 3.1807 Acc@1: 54.543% [1466/2688]\n",
      "| Epoch [170/200] Iter[ 41/391]\t\tLoss: 1.6390 Acc@1: 54.189% [2843/5248]\n",
      "| Epoch [170/200] Iter[ 61/391]\t\tLoss: 1.9059 Acc@1: 50.930% [3976/7808]\n",
      "| Epoch [170/200] Iter[ 81/391]\t\tLoss: 2.5586 Acc@1: 49.800% [5163/10368]\n",
      "| Epoch [170/200] Iter[101/391]\t\tLoss: 3.1203 Acc@1: 49.034% [6339/12928]\n",
      "| Epoch [170/200] Iter[121/391]\t\tLoss: 2.6159 Acc@1: 48.935% [7579/15488]\n",
      "| Epoch [170/200] Iter[141/391]\t\tLoss: 2.9652 Acc@1: 49.052% [8852/18048]\n",
      "| Epoch [170/200] Iter[161/391]\t\tLoss: 2.8359 Acc@1: 49.563% [10213/20608]\n",
      "| Epoch [170/200] Iter[181/391]\t\tLoss: 2.6033 Acc@1: 50.201% [11630/23168]\n",
      "| Epoch [170/200] Iter[201/391]\t\tLoss: 2.7774 Acc@1: 50.074% [12882/25728]\n",
      "| Epoch [170/200] Iter[221/391]\t\tLoss: 1.8330 Acc@1: 49.959% [14132/28288]\n",
      "| Epoch [170/200] Iter[241/391]\t\tLoss: 2.7241 Acc@1: 50.435% [15558/30848]\n",
      "| Epoch [170/200] Iter[261/391]\t\tLoss: 3.0481 Acc@1: 50.326% [16813/33408]\n",
      "| Epoch [170/200] Iter[281/391]\t\tLoss: 2.7552 Acc@1: 50.668% [18224/35968]\n",
      "| Epoch [170/200] Iter[301/391]\t\tLoss: 2.4960 Acc@1: 50.972% [19638/38528]\n",
      "| Epoch [170/200] Iter[321/391]\t\tLoss: 3.0158 Acc@1: 50.929% [20925/41088]\n",
      "| Epoch [170/200] Iter[341/391]\t\tLoss: 1.1865 Acc@1: 51.125% [22315/43648]\n",
      "| Epoch [170/200] Iter[361/391]\t\tLoss: 1.5832 Acc@1: 51.317% [23712/46208]\n",
      "| Epoch [170/200] Iter[381/391]\t\tLoss: 2.5430 Acc@1: 50.947% [24845/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #170\n",
      "\n",
      "| Validation Epoch #170\t\t\tLoss: 1.3017 Acc@1: 65.46%\n",
      "\n",
      "----- Elapsed time : 0:50:33\n",
      "\n",
      "\n",
      "=> Training Epoch #171\n",
      "| Epoch [171/200] Iter[  1/391]\t\tLoss: 2.3731 Acc@1: 55.673% [ 71/128]\n",
      "| Epoch [171/200] Iter[ 21/391]\t\tLoss: 3.0824 Acc@1: 55.990% [1505/2688]\n",
      "| Epoch [171/200] Iter[ 41/391]\t\tLoss: 1.2720 Acc@1: 55.171% [2895/5248]\n",
      "| Epoch [171/200] Iter[ 61/391]\t\tLoss: 2.9559 Acc@1: 53.531% [4179/7808]\n",
      "| Epoch [171/200] Iter[ 81/391]\t\tLoss: 2.5441 Acc@1: 52.499% [5443/10368]\n",
      "| Epoch [171/200] Iter[101/391]\t\tLoss: 1.0887 Acc@1: 52.062% [6730/12928]\n",
      "| Epoch [171/200] Iter[121/391]\t\tLoss: 3.3267 Acc@1: 52.061% [8063/15488]\n",
      "| Epoch [171/200] Iter[141/391]\t\tLoss: 3.1338 Acc@1: 51.373% [9271/18048]\n",
      "| Epoch [171/200] Iter[161/391]\t\tLoss: 2.1028 Acc@1: 51.122% [10535/20608]\n",
      "| Epoch [171/200] Iter[181/391]\t\tLoss: 3.0479 Acc@1: 51.530% [11938/23168]\n",
      "| Epoch [171/200] Iter[201/391]\t\tLoss: 2.7551 Acc@1: 51.053% [13134/25728]\n",
      "| Epoch [171/200] Iter[221/391]\t\tLoss: 1.0523 Acc@1: 51.323% [14518/28288]\n",
      "| Epoch [171/200] Iter[241/391]\t\tLoss: 3.1920 Acc@1: 51.653% [15933/30848]\n",
      "| Epoch [171/200] Iter[261/391]\t\tLoss: 2.4486 Acc@1: 52.066% [17394/33408]\n",
      "| Epoch [171/200] Iter[281/391]\t\tLoss: 2.4758 Acc@1: 52.380% [18840/35968]\n",
      "| Epoch [171/200] Iter[301/391]\t\tLoss: 2.4974 Acc@1: 52.222% [20120/38528]\n",
      "| Epoch [171/200] Iter[321/391]\t\tLoss: 2.3104 Acc@1: 51.845% [21301/41088]\n",
      "| Epoch [171/200] Iter[341/391]\t\tLoss: 1.8084 Acc@1: 51.852% [22632/43648]\n",
      "| Epoch [171/200] Iter[361/391]\t\tLoss: 3.0902 Acc@1: 51.633% [23858/46208]\n",
      "| Epoch [171/200] Iter[381/391]\t\tLoss: 3.1356 Acc@1: 51.709% [25217/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #171\n",
      "\n",
      "| Validation Epoch #171\t\t\tLoss: 1.3359 Acc@1: 66.49%\n",
      "\n",
      "----- Elapsed time : 0:50:51\n",
      "\n",
      "\n",
      "=> Training Epoch #172\n",
      "| Epoch [172/200] Iter[  1/391]\t\tLoss: 1.7402 Acc@1: 71.118% [ 91/128]\n",
      "| Epoch [172/200] Iter[ 21/391]\t\tLoss: 3.1837 Acc@1: 51.554% [1385/2688]\n",
      "| Epoch [172/200] Iter[ 41/391]\t\tLoss: 2.8618 Acc@1: 51.827% [2719/5248]\n",
      "| Epoch [172/200] Iter[ 61/391]\t\tLoss: 2.6739 Acc@1: 49.920% [3897/7808]\n",
      "| Epoch [172/200] Iter[ 81/391]\t\tLoss: 3.0797 Acc@1: 50.970% [5284/10368]\n",
      "| Epoch [172/200] Iter[101/391]\t\tLoss: 2.4364 Acc@1: 52.252% [6755/12928]\n",
      "| Epoch [172/200] Iter[121/391]\t\tLoss: 1.4296 Acc@1: 53.122% [8227/15488]\n",
      "| Epoch [172/200] Iter[141/391]\t\tLoss: 2.6749 Acc@1: 53.272% [9614/18048]\n",
      "| Epoch [172/200] Iter[161/391]\t\tLoss: 3.1824 Acc@1: 52.086% [10733/20608]\n",
      "| Epoch [172/200] Iter[181/391]\t\tLoss: 3.2052 Acc@1: 51.558% [11945/23168]\n",
      "| Epoch [172/200] Iter[201/391]\t\tLoss: 0.9674 Acc@1: 51.553% [13263/25728]\n",
      "| Epoch [172/200] Iter[221/391]\t\tLoss: 2.6469 Acc@1: 51.641% [14608/28288]\n",
      "| Epoch [172/200] Iter[241/391]\t\tLoss: 1.6490 Acc@1: 51.737% [15959/30848]\n",
      "| Epoch [172/200] Iter[261/391]\t\tLoss: 1.9459 Acc@1: 51.350% [17155/33408]\n",
      "| Epoch [172/200] Iter[281/391]\t\tLoss: 3.1984 Acc@1: 50.956% [18327/35968]\n",
      "| Epoch [172/200] Iter[301/391]\t\tLoss: 2.6196 Acc@1: 50.871% [19599/38528]\n",
      "| Epoch [172/200] Iter[321/391]\t\tLoss: 3.1351 Acc@1: 50.785% [20866/41088]\n",
      "| Epoch [172/200] Iter[341/391]\t\tLoss: 2.8020 Acc@1: 50.802% [22174/43648]\n",
      "| Epoch [172/200] Iter[361/391]\t\tLoss: 2.3485 Acc@1: 50.658% [23408/46208]\n",
      "| Epoch [172/200] Iter[381/391]\t\tLoss: 2.5316 Acc@1: 50.485% [24620/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #172\n",
      "\n",
      "| Validation Epoch #172\t\t\tLoss: 1.3382 Acc@1: 65.90%\n",
      "\n",
      "----- Elapsed time : 0:51:09\n",
      "\n",
      "\n",
      "=> Training Epoch #173\n",
      "| Epoch [173/200] Iter[  1/391]\t\tLoss: 2.3513 Acc@1: 54.324% [ 69/128]\n",
      "| Epoch [173/200] Iter[ 21/391]\t\tLoss: 2.3049 Acc@1: 50.843% [1366/2688]\n",
      "| Epoch [173/200] Iter[ 41/391]\t\tLoss: 2.9141 Acc@1: 50.704% [2660/5248]\n",
      "| Epoch [173/200] Iter[ 61/391]\t\tLoss: 1.0557 Acc@1: 52.600% [4107/7808]\n",
      "| Epoch [173/200] Iter[ 81/391]\t\tLoss: 2.9799 Acc@1: 52.182% [5410/10368]\n",
      "| Epoch [173/200] Iter[101/391]\t\tLoss: 3.0386 Acc@1: 52.168% [6744/12928]\n",
      "| Epoch [173/200] Iter[121/391]\t\tLoss: 2.7280 Acc@1: 52.689% [8160/15488]\n",
      "| Epoch [173/200] Iter[141/391]\t\tLoss: 1.9827 Acc@1: 53.307% [9620/18048]\n",
      "| Epoch [173/200] Iter[161/391]\t\tLoss: 3.3634 Acc@1: 53.180% [10959/20608]\n",
      "| Epoch [173/200] Iter[181/391]\t\tLoss: 1.0620 Acc@1: 53.214% [12328/23168]\n",
      "| Epoch [173/200] Iter[201/391]\t\tLoss: 0.8771 Acc@1: 53.239% [13697/25728]\n",
      "| Epoch [173/200] Iter[221/391]\t\tLoss: 2.8692 Acc@1: 53.351% [15092/28288]\n",
      "| Epoch [173/200] Iter[241/391]\t\tLoss: 3.0085 Acc@1: 53.044% [16363/30848]\n",
      "| Epoch [173/200] Iter[261/391]\t\tLoss: 2.9200 Acc@1: 52.388% [17501/33408]\n",
      "| Epoch [173/200] Iter[281/391]\t\tLoss: 2.2354 Acc@1: 52.038% [18717/35968]\n",
      "| Epoch [173/200] Iter[301/391]\t\tLoss: 2.6145 Acc@1: 51.601% [19880/38528]\n",
      "| Epoch [173/200] Iter[321/391]\t\tLoss: 1.6553 Acc@1: 51.488% [21155/41088]\n",
      "| Epoch [173/200] Iter[341/391]\t\tLoss: 1.2856 Acc@1: 51.363% [22419/43648]\n",
      "| Epoch [173/200] Iter[361/391]\t\tLoss: 1.1844 Acc@1: 51.289% [23699/46208]\n",
      "| Epoch [173/200] Iter[381/391]\t\tLoss: 3.0725 Acc@1: 51.073% [24907/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #173\n",
      "\n",
      "| Validation Epoch #173\t\t\tLoss: 1.3807 Acc@1: 65.58%\n",
      "\n",
      "----- Elapsed time : 0:51:26\n",
      "\n",
      "\n",
      "=> Training Epoch #174\n",
      "| Epoch [174/200] Iter[  1/391]\t\tLoss: 0.8838 Acc@1: 82.434% [105/128]\n",
      "| Epoch [174/200] Iter[ 21/391]\t\tLoss: 1.9728 Acc@1: 48.362% [1299/2688]\n",
      "| Epoch [174/200] Iter[ 41/391]\t\tLoss: 2.9523 Acc@1: 47.637% [2499/5248]\n",
      "| Epoch [174/200] Iter[ 61/391]\t\tLoss: 3.0957 Acc@1: 47.540% [3711/7808]\n",
      "| Epoch [174/200] Iter[ 81/391]\t\tLoss: 3.1625 Acc@1: 46.478% [4818/10368]\n",
      "| Epoch [174/200] Iter[101/391]\t\tLoss: 3.0079 Acc@1: 48.077% [6215/12928]\n",
      "| Epoch [174/200] Iter[121/391]\t\tLoss: 2.4442 Acc@1: 49.339% [7641/15488]\n",
      "| Epoch [174/200] Iter[141/391]\t\tLoss: 2.0584 Acc@1: 49.854% [8997/18048]\n",
      "| Epoch [174/200] Iter[161/391]\t\tLoss: 2.6952 Acc@1: 50.171% [10339/20608]\n",
      "| Epoch [174/200] Iter[181/391]\t\tLoss: 2.8685 Acc@1: 50.601% [11723/23168]\n",
      "| Epoch [174/200] Iter[201/391]\t\tLoss: 2.9148 Acc@1: 50.360% [12956/25728]\n",
      "| Epoch [174/200] Iter[221/391]\t\tLoss: 0.9233 Acc@1: 50.574% [14306/28288]\n",
      "| Epoch [174/200] Iter[241/391]\t\tLoss: 2.1329 Acc@1: 50.471% [15569/30848]\n",
      "| Epoch [174/200] Iter[261/391]\t\tLoss: 3.0812 Acc@1: 50.689% [16934/33408]\n",
      "| Epoch [174/200] Iter[281/391]\t\tLoss: 2.9424 Acc@1: 51.249% [18433/35968]\n",
      "| Epoch [174/200] Iter[301/391]\t\tLoss: 2.8652 Acc@1: 51.001% [19649/38528]\n",
      "| Epoch [174/200] Iter[321/391]\t\tLoss: 3.0120 Acc@1: 51.117% [21003/41088]\n",
      "| Epoch [174/200] Iter[341/391]\t\tLoss: 2.3128 Acc@1: 50.860% [22199/43648]\n",
      "| Epoch [174/200] Iter[361/391]\t\tLoss: 2.9203 Acc@1: 50.967% [23550/46208]\n",
      "| Epoch [174/200] Iter[381/391]\t\tLoss: 3.1372 Acc@1: 50.612% [24682/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #174\n",
      "\n",
      "| Validation Epoch #174\t\t\tLoss: 1.3491 Acc@1: 65.95%\n",
      "\n",
      "----- Elapsed time : 0:51:44\n",
      "\n",
      "\n",
      "=> Training Epoch #175\n",
      "| Epoch [175/200] Iter[  1/391]\t\tLoss: 3.0130 Acc@1: 33.109% [ 42/128]\n",
      "| Epoch [175/200] Iter[ 21/391]\t\tLoss: 2.2802 Acc@1: 49.530% [1331/2688]\n",
      "| Epoch [175/200] Iter[ 41/391]\t\tLoss: 2.3104 Acc@1: 49.378% [2591/5248]\n",
      "| Epoch [175/200] Iter[ 61/391]\t\tLoss: 1.5493 Acc@1: 50.991% [3981/7808]\n",
      "| Epoch [175/200] Iter[ 81/391]\t\tLoss: 3.0506 Acc@1: 49.750% [5158/10368]\n",
      "| Epoch [175/200] Iter[101/391]\t\tLoss: 1.6608 Acc@1: 49.633% [6416/12928]\n",
      "| Epoch [175/200] Iter[121/391]\t\tLoss: 1.9924 Acc@1: 50.131% [7764/15488]\n",
      "| Epoch [175/200] Iter[141/391]\t\tLoss: 2.8069 Acc@1: 50.347% [9086/18048]\n",
      "| Epoch [175/200] Iter[161/391]\t\tLoss: 0.9296 Acc@1: 50.810% [10470/20608]\n",
      "| Epoch [175/200] Iter[181/391]\t\tLoss: 2.8279 Acc@1: 51.092% [11836/23168]\n",
      "| Epoch [175/200] Iter[201/391]\t\tLoss: 1.0722 Acc@1: 51.315% [13202/25728]\n",
      "| Epoch [175/200] Iter[221/391]\t\tLoss: 2.6158 Acc@1: 51.314% [14515/28288]\n",
      "| Epoch [175/200] Iter[241/391]\t\tLoss: 2.6768 Acc@1: 51.309% [15827/30848]\n",
      "| Epoch [175/200] Iter[261/391]\t\tLoss: 2.1229 Acc@1: 51.766% [17293/33408]\n",
      "| Epoch [175/200] Iter[281/391]\t\tLoss: 1.1525 Acc@1: 51.975% [18694/35968]\n",
      "| Epoch [175/200] Iter[301/391]\t\tLoss: 0.9016 Acc@1: 51.841% [19973/38528]\n",
      "| Epoch [175/200] Iter[321/391]\t\tLoss: 3.0826 Acc@1: 51.647% [21220/41088]\n",
      "| Epoch [175/200] Iter[341/391]\t\tLoss: 2.6955 Acc@1: 51.587% [22516/43648]\n",
      "| Epoch [175/200] Iter[361/391]\t\tLoss: 2.9967 Acc@1: 51.341% [23723/46208]\n",
      "| Epoch [175/200] Iter[381/391]\t\tLoss: 0.9046 Acc@1: 51.449% [25090/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #175\n",
      "\n",
      "| Validation Epoch #175\t\t\tLoss: 1.3839 Acc@1: 66.02%\n",
      "\n",
      "----- Elapsed time : 0:52:02\n",
      "\n",
      "\n",
      "=> Training Epoch #176\n",
      "| Epoch [176/200] Iter[  1/391]\t\tLoss: 1.0714 Acc@1: 76.925% [ 98/128]\n",
      "| Epoch [176/200] Iter[ 21/391]\t\tLoss: 1.5169 Acc@1: 54.394% [1462/2688]\n",
      "| Epoch [176/200] Iter[ 41/391]\t\tLoss: 3.1317 Acc@1: 50.322% [2640/5248]\n",
      "| Epoch [176/200] Iter[ 61/391]\t\tLoss: 2.5640 Acc@1: 52.069% [4065/7808]\n",
      "| Epoch [176/200] Iter[ 81/391]\t\tLoss: 1.2426 Acc@1: 52.433% [5436/10368]\n",
      "| Epoch [176/200] Iter[101/391]\t\tLoss: 2.8605 Acc@1: 52.058% [6730/12928]\n",
      "| Epoch [176/200] Iter[121/391]\t\tLoss: 1.8191 Acc@1: 51.811% [8024/15488]\n",
      "| Epoch [176/200] Iter[141/391]\t\tLoss: 2.8325 Acc@1: 50.856% [9178/18048]\n",
      "| Epoch [176/200] Iter[161/391]\t\tLoss: 1.2337 Acc@1: 51.500% [10613/20608]\n",
      "| Epoch [176/200] Iter[181/391]\t\tLoss: 1.6229 Acc@1: 51.801% [12001/23168]\n",
      "| Epoch [176/200] Iter[201/391]\t\tLoss: 1.2349 Acc@1: 51.838% [13336/25728]\n",
      "| Epoch [176/200] Iter[221/391]\t\tLoss: 1.4564 Acc@1: 51.785% [14648/28288]\n",
      "| Epoch [176/200] Iter[241/391]\t\tLoss: 1.8367 Acc@1: 51.855% [15996/30848]\n",
      "| Epoch [176/200] Iter[261/391]\t\tLoss: 2.6504 Acc@1: 51.471% [17195/33408]\n",
      "| Epoch [176/200] Iter[281/391]\t\tLoss: 2.9572 Acc@1: 51.255% [18435/35968]\n",
      "| Epoch [176/200] Iter[301/391]\t\tLoss: 2.8769 Acc@1: 51.343% [19781/38528]\n",
      "| Epoch [176/200] Iter[321/391]\t\tLoss: 2.8769 Acc@1: 51.365% [21104/41088]\n",
      "| Epoch [176/200] Iter[341/391]\t\tLoss: 3.0846 Acc@1: 51.379% [22426/43648]\n",
      "| Epoch [176/200] Iter[361/391]\t\tLoss: 1.8792 Acc@1: 51.625% [23855/46208]\n",
      "| Epoch [176/200] Iter[381/391]\t\tLoss: 2.5613 Acc@1: 51.250% [24993/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #176\n",
      "\n",
      "| Validation Epoch #176\t\t\tLoss: 1.3499 Acc@1: 65.75%\n",
      "\n",
      "----- Elapsed time : 0:52:20\n",
      "\n",
      "\n",
      "=> Training Epoch #177\n",
      "| Epoch [177/200] Iter[  1/391]\t\tLoss: 2.5654 Acc@1: 44.734% [ 57/128]\n",
      "| Epoch [177/200] Iter[ 21/391]\t\tLoss: 3.1319 Acc@1: 49.985% [1343/2688]\n",
      "| Epoch [177/200] Iter[ 41/391]\t\tLoss: 2.5624 Acc@1: 52.800% [2770/5248]\n",
      "| Epoch [177/200] Iter[ 61/391]\t\tLoss: 2.4651 Acc@1: 52.598% [4106/7808]\n",
      "| Epoch [177/200] Iter[ 81/391]\t\tLoss: 3.0273 Acc@1: 53.049% [5500/10368]\n",
      "| Epoch [177/200] Iter[101/391]\t\tLoss: 2.8250 Acc@1: 51.940% [6714/12928]\n",
      "| Epoch [177/200] Iter[121/391]\t\tLoss: 1.2788 Acc@1: 52.560% [8140/15488]\n",
      "| Epoch [177/200] Iter[141/391]\t\tLoss: 1.2618 Acc@1: 52.770% [9523/18048]\n",
      "| Epoch [177/200] Iter[161/391]\t\tLoss: 2.9861 Acc@1: 51.823% [10679/20608]\n",
      "| Epoch [177/200] Iter[181/391]\t\tLoss: 1.8969 Acc@1: 51.860% [12015/23168]\n",
      "| Epoch [177/200] Iter[201/391]\t\tLoss: 1.8970 Acc@1: 51.709% [13303/25728]\n",
      "| Epoch [177/200] Iter[221/391]\t\tLoss: 3.0679 Acc@1: 51.895% [14680/28288]\n",
      "| Epoch [177/200] Iter[241/391]\t\tLoss: 2.7922 Acc@1: 51.827% [15987/30848]\n",
      "| Epoch [177/200] Iter[261/391]\t\tLoss: 2.9502 Acc@1: 51.589% [17234/33408]\n",
      "| Epoch [177/200] Iter[281/391]\t\tLoss: 2.6458 Acc@1: 51.421% [18495/35968]\n",
      "| Epoch [177/200] Iter[301/391]\t\tLoss: 2.5087 Acc@1: 51.260% [19749/38528]\n",
      "| Epoch [177/200] Iter[321/391]\t\tLoss: 2.4555 Acc@1: 51.454% [21141/41088]\n",
      "| Epoch [177/200] Iter[341/391]\t\tLoss: 1.7262 Acc@1: 51.425% [22445/43648]\n",
      "| Epoch [177/200] Iter[361/391]\t\tLoss: 3.1513 Acc@1: 51.092% [23608/46208]\n",
      "| Epoch [177/200] Iter[381/391]\t\tLoss: 3.1232 Acc@1: 50.981% [24862/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #177\n",
      "\n",
      "| Validation Epoch #177\t\t\tLoss: 1.3558 Acc@1: 65.81%\n",
      "\n",
      "----- Elapsed time : 0:52:38\n",
      "\n",
      "\n",
      "=> Training Epoch #178\n",
      "| Epoch [178/200] Iter[  1/391]\t\tLoss: 2.1155 Acc@1: 61.568% [ 78/128]\n",
      "| Epoch [178/200] Iter[ 21/391]\t\tLoss: 1.7868 Acc@1: 47.177% [1268/2688]\n",
      "| Epoch [178/200] Iter[ 41/391]\t\tLoss: 2.7911 Acc@1: 49.706% [2608/5248]\n",
      "| Epoch [178/200] Iter[ 61/391]\t\tLoss: 2.6232 Acc@1: 48.715% [3803/7808]\n",
      "| Epoch [178/200] Iter[ 81/391]\t\tLoss: 3.0044 Acc@1: 48.815% [5061/10368]\n",
      "| Epoch [178/200] Iter[101/391]\t\tLoss: 1.4261 Acc@1: 49.966% [6459/12928]\n",
      "| Epoch [178/200] Iter[121/391]\t\tLoss: 2.9011 Acc@1: 49.621% [7685/15488]\n",
      "| Epoch [178/200] Iter[141/391]\t\tLoss: 1.7520 Acc@1: 49.872% [9000/18048]\n",
      "| Epoch [178/200] Iter[161/391]\t\tLoss: 2.6732 Acc@1: 49.548% [10210/20608]\n",
      "| Epoch [178/200] Iter[181/391]\t\tLoss: 1.7477 Acc@1: 50.898% [11792/23168]\n",
      "| Epoch [178/200] Iter[201/391]\t\tLoss: 2.6124 Acc@1: 51.060% [13136/25728]\n",
      "| Epoch [178/200] Iter[221/391]\t\tLoss: 3.0502 Acc@1: 51.128% [14463/28288]\n",
      "| Epoch [178/200] Iter[241/391]\t\tLoss: 1.2417 Acc@1: 51.055% [15749/30848]\n",
      "| Epoch [178/200] Iter[261/391]\t\tLoss: 2.5704 Acc@1: 51.359% [17157/33408]\n",
      "| Epoch [178/200] Iter[281/391]\t\tLoss: 2.3022 Acc@1: 51.573% [18549/35968]\n",
      "| Epoch [178/200] Iter[301/391]\t\tLoss: 2.7147 Acc@1: 51.441% [19819/38528]\n",
      "| Epoch [178/200] Iter[321/391]\t\tLoss: 2.5218 Acc@1: 51.638% [21216/41088]\n",
      "| Epoch [178/200] Iter[341/391]\t\tLoss: 2.3355 Acc@1: 51.265% [22376/43648]\n",
      "| Epoch [178/200] Iter[361/391]\t\tLoss: 1.2864 Acc@1: 51.434% [23766/46208]\n",
      "| Epoch [178/200] Iter[381/391]\t\tLoss: 1.7410 Acc@1: 51.876% [25298/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #178\n",
      "\n",
      "| Validation Epoch #178\t\t\tLoss: 1.2898 Acc@1: 66.18%\n",
      "\n",
      "----- Elapsed time : 0:52:55\n",
      "\n",
      "\n",
      "=> Training Epoch #179\n",
      "| Epoch [179/200] Iter[  1/391]\t\tLoss: 3.0182 Acc@1: 25.637% [ 32/128]\n",
      "| Epoch [179/200] Iter[ 21/391]\t\tLoss: 1.7283 Acc@1: 47.329% [1272/2688]\n",
      "| Epoch [179/200] Iter[ 41/391]\t\tLoss: 1.3937 Acc@1: 48.445% [2542/5248]\n",
      "| Epoch [179/200] Iter[ 61/391]\t\tLoss: 1.1416 Acc@1: 51.984% [4058/7808]\n",
      "| Epoch [179/200] Iter[ 81/391]\t\tLoss: 2.9711 Acc@1: 50.767% [5263/10368]\n",
      "| Epoch [179/200] Iter[101/391]\t\tLoss: 2.5323 Acc@1: 50.938% [6585/12928]\n",
      "| Epoch [179/200] Iter[121/391]\t\tLoss: 1.1431 Acc@1: 51.997% [8053/15488]\n",
      "| Epoch [179/200] Iter[141/391]\t\tLoss: 3.0527 Acc@1: 52.398% [9456/18048]\n",
      "| Epoch [179/200] Iter[161/391]\t\tLoss: 3.1511 Acc@1: 51.661% [10646/20608]\n",
      "| Epoch [179/200] Iter[181/391]\t\tLoss: 2.6301 Acc@1: 51.580% [11950/23168]\n",
      "| Epoch [179/200] Iter[201/391]\t\tLoss: 1.6523 Acc@1: 51.659% [13290/25728]\n",
      "| Epoch [179/200] Iter[221/391]\t\tLoss: 2.0623 Acc@1: 51.696% [14623/28288]\n",
      "| Epoch [179/200] Iter[241/391]\t\tLoss: 1.2113 Acc@1: 51.814% [15983/30848]\n",
      "| Epoch [179/200] Iter[261/391]\t\tLoss: 1.2455 Acc@1: 52.095% [17403/33408]\n",
      "| Epoch [179/200] Iter[281/391]\t\tLoss: 3.0280 Acc@1: 51.856% [18651/35968]\n",
      "| Epoch [179/200] Iter[301/391]\t\tLoss: 2.9637 Acc@1: 51.932% [20008/38528]\n",
      "| Epoch [179/200] Iter[321/391]\t\tLoss: 1.4750 Acc@1: 51.781% [21275/41088]\n",
      "| Epoch [179/200] Iter[341/391]\t\tLoss: 2.5941 Acc@1: 51.758% [22591/43648]\n",
      "| Epoch [179/200] Iter[361/391]\t\tLoss: 2.1138 Acc@1: 51.633% [23858/46208]\n",
      "| Epoch [179/200] Iter[381/391]\t\tLoss: 0.8674 Acc@1: 51.666% [25196/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #179\n",
      "\n",
      "| Validation Epoch #179\t\t\tLoss: 1.2704 Acc@1: 66.01%\n",
      "\n",
      "----- Elapsed time : 0:53:13\n",
      "\n",
      "\n",
      "=> Training Epoch #180\n",
      "| Epoch [180/200] Iter[  1/391]\t\tLoss: 2.0484 Acc@1: 62.730% [ 80/128]\n",
      "| Epoch [180/200] Iter[ 21/391]\t\tLoss: 1.2397 Acc@1: 58.370% [1568/2688]\n",
      "| Epoch [180/200] Iter[ 41/391]\t\tLoss: 2.2179 Acc@1: 55.327% [2903/5248]\n",
      "| Epoch [180/200] Iter[ 61/391]\t\tLoss: 1.6846 Acc@1: 55.411% [4326/7808]\n",
      "| Epoch [180/200] Iter[ 81/391]\t\tLoss: 3.1499 Acc@1: 53.795% [5577/10368]\n",
      "| Epoch [180/200] Iter[101/391]\t\tLoss: 1.7500 Acc@1: 52.634% [6804/12928]\n",
      "| Epoch [180/200] Iter[121/391]\t\tLoss: 1.2579 Acc@1: 52.207% [8085/15488]\n",
      "| Epoch [180/200] Iter[141/391]\t\tLoss: 2.3849 Acc@1: 52.566% [9487/18048]\n",
      "| Epoch [180/200] Iter[161/391]\t\tLoss: 2.1861 Acc@1: 52.482% [10815/20608]\n",
      "| Epoch [180/200] Iter[181/391]\t\tLoss: 2.7960 Acc@1: 52.156% [12083/23168]\n",
      "| Epoch [180/200] Iter[201/391]\t\tLoss: 3.0145 Acc@1: 51.939% [13362/25728]\n",
      "| Epoch [180/200] Iter[221/391]\t\tLoss: 2.8163 Acc@1: 51.378% [14533/28288]\n",
      "| Epoch [180/200] Iter[241/391]\t\tLoss: 1.9735 Acc@1: 51.586% [15913/30848]\n",
      "| Epoch [180/200] Iter[261/391]\t\tLoss: 3.0399 Acc@1: 51.797% [17304/33408]\n",
      "| Epoch [180/200] Iter[281/391]\t\tLoss: 2.8618 Acc@1: 51.674% [18586/35968]\n",
      "| Epoch [180/200] Iter[301/391]\t\tLoss: 2.2650 Acc@1: 51.860% [19980/38528]\n",
      "| Epoch [180/200] Iter[321/391]\t\tLoss: 2.3556 Acc@1: 51.682% [21235/41088]\n",
      "| Epoch [180/200] Iter[341/391]\t\tLoss: 2.9390 Acc@1: 51.157% [22328/43648]\n",
      "| Epoch [180/200] Iter[361/391]\t\tLoss: 2.8881 Acc@1: 50.884% [23512/46208]\n",
      "| Epoch [180/200] Iter[381/391]\t\tLoss: 1.0741 Acc@1: 51.032% [24887/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #180\n",
      "\n",
      "| Validation Epoch #180\t\t\tLoss: 1.2631 Acc@1: 66.29%\n",
      "\n",
      "----- Elapsed time : 0:53:31\n",
      "\n",
      "\n",
      "=> Training Epoch #181\n",
      "| Epoch [181/200] Iter[  1/391]\t\tLoss: 1.5000 Acc@1: 68.393% [ 87/128]\n",
      "| Epoch [181/200] Iter[ 21/391]\t\tLoss: 1.0120 Acc@1: 64.247% [1726/2688]\n",
      "| Epoch [181/200] Iter[ 41/391]\t\tLoss: 2.4110 Acc@1: 57.177% [3000/5248]\n",
      "| Epoch [181/200] Iter[ 61/391]\t\tLoss: 2.3579 Acc@1: 55.567% [4338/7808]\n",
      "| Epoch [181/200] Iter[ 81/391]\t\tLoss: 2.1854 Acc@1: 54.977% [5700/10368]\n",
      "| Epoch [181/200] Iter[101/391]\t\tLoss: 2.2245 Acc@1: 54.650% [7065/12928]\n",
      "| Epoch [181/200] Iter[121/391]\t\tLoss: 2.9250 Acc@1: 54.144% [8385/15488]\n",
      "| Epoch [181/200] Iter[141/391]\t\tLoss: 2.6237 Acc@1: 52.588% [9491/18048]\n",
      "| Epoch [181/200] Iter[161/391]\t\tLoss: 1.2831 Acc@1: 52.716% [10863/20608]\n",
      "| Epoch [181/200] Iter[181/391]\t\tLoss: 3.1232 Acc@1: 52.914% [12259/23168]\n",
      "| Epoch [181/200] Iter[201/391]\t\tLoss: 2.1825 Acc@1: 52.669% [13550/25728]\n",
      "| Epoch [181/200] Iter[221/391]\t\tLoss: 3.0815 Acc@1: 52.482% [14846/28288]\n",
      "| Epoch [181/200] Iter[241/391]\t\tLoss: 2.7194 Acc@1: 52.362% [16152/30848]\n",
      "| Epoch [181/200] Iter[261/391]\t\tLoss: 1.0509 Acc@1: 52.825% [17647/33408]\n",
      "| Epoch [181/200] Iter[281/391]\t\tLoss: 1.0232 Acc@1: 52.679% [18947/35968]\n",
      "| Epoch [181/200] Iter[301/391]\t\tLoss: 2.1821 Acc@1: 53.213% [20502/38528]\n",
      "| Epoch [181/200] Iter[321/391]\t\tLoss: 0.7970 Acc@1: 53.337% [21915/41088]\n",
      "| Epoch [181/200] Iter[341/391]\t\tLoss: 1.3993 Acc@1: 53.273% [23252/43648]\n",
      "| Epoch [181/200] Iter[361/391]\t\tLoss: 1.6126 Acc@1: 53.235% [24598/46208]\n",
      "| Epoch [181/200] Iter[381/391]\t\tLoss: 0.9652 Acc@1: 53.482% [26082/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #181\n",
      "\n",
      "| Validation Epoch #181\t\t\tLoss: 1.2923 Acc@1: 66.29%\n",
      "\n",
      "----- Elapsed time : 0:53:49\n",
      "\n",
      "\n",
      "=> Training Epoch #182\n",
      "| Epoch [182/200] Iter[  1/391]\t\tLoss: 3.2337 Acc@1: 26.203% [ 33/128]\n",
      "| Epoch [182/200] Iter[ 21/391]\t\tLoss: 1.3221 Acc@1: 51.888% [1394/2688]\n",
      "| Epoch [182/200] Iter[ 41/391]\t\tLoss: 1.5071 Acc@1: 52.541% [2757/5248]\n",
      "| Epoch [182/200] Iter[ 61/391]\t\tLoss: 2.3569 Acc@1: 52.093% [4067/7808]\n",
      "| Epoch [182/200] Iter[ 81/391]\t\tLoss: 3.0326 Acc@1: 52.651% [5458/10368]\n",
      "| Epoch [182/200] Iter[101/391]\t\tLoss: 2.1336 Acc@1: 53.671% [6938/12928]\n",
      "| Epoch [182/200] Iter[121/391]\t\tLoss: 1.4268 Acc@1: 52.717% [8164/15488]\n",
      "| Epoch [182/200] Iter[141/391]\t\tLoss: 2.0245 Acc@1: 52.516% [9478/18048]\n",
      "| Epoch [182/200] Iter[161/391]\t\tLoss: 1.0971 Acc@1: 52.253% [10768/20608]\n",
      "| Epoch [182/200] Iter[181/391]\t\tLoss: 2.6996 Acc@1: 52.235% [12101/23168]\n",
      "| Epoch [182/200] Iter[201/391]\t\tLoss: 2.6611 Acc@1: 52.068% [13395/25728]\n",
      "| Epoch [182/200] Iter[221/391]\t\tLoss: 3.1986 Acc@1: 51.707% [14626/28288]\n",
      "| Epoch [182/200] Iter[241/391]\t\tLoss: 1.3315 Acc@1: 52.250% [16117/30848]\n",
      "| Epoch [182/200] Iter[261/391]\t\tLoss: 3.0847 Acc@1: 52.236% [17450/33408]\n",
      "| Epoch [182/200] Iter[281/391]\t\tLoss: 1.9164 Acc@1: 52.100% [18739/35968]\n",
      "| Epoch [182/200] Iter[301/391]\t\tLoss: 2.3627 Acc@1: 51.731% [19930/38528]\n",
      "| Epoch [182/200] Iter[321/391]\t\tLoss: 2.5874 Acc@1: 51.589% [21197/41088]\n",
      "| Epoch [182/200] Iter[341/391]\t\tLoss: 3.0447 Acc@1: 51.624% [22532/43648]\n",
      "| Epoch [182/200] Iter[361/391]\t\tLoss: 3.2094 Acc@1: 52.065% [24058/46208]\n",
      "| Epoch [182/200] Iter[381/391]\t\tLoss: 1.8417 Acc@1: 52.204% [25458/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #182\n",
      "\n",
      "| Validation Epoch #182\t\t\tLoss: 1.3143 Acc@1: 66.45%\n",
      "\n",
      "----- Elapsed time : 0:54:06\n",
      "\n",
      "\n",
      "=> Training Epoch #183\n",
      "| Epoch [183/200] Iter[  1/391]\t\tLoss: 3.0027 Acc@1: 35.299% [ 45/128]\n",
      "| Epoch [183/200] Iter[ 21/391]\t\tLoss: 2.6357 Acc@1: 58.042% [1560/2688]\n",
      "| Epoch [183/200] Iter[ 41/391]\t\tLoss: 2.6736 Acc@1: 55.536% [2914/5248]\n",
      "| Epoch [183/200] Iter[ 61/391]\t\tLoss: 1.4757 Acc@1: 55.296% [4317/7808]\n",
      "| Epoch [183/200] Iter[ 81/391]\t\tLoss: 1.6294 Acc@1: 54.041% [5603/10368]\n",
      "| Epoch [183/200] Iter[101/391]\t\tLoss: 1.7439 Acc@1: 54.161% [7001/12928]\n",
      "| Epoch [183/200] Iter[121/391]\t\tLoss: 1.5244 Acc@1: 54.049% [8371/15488]\n",
      "| Epoch [183/200] Iter[141/391]\t\tLoss: 1.2110 Acc@1: 53.781% [9706/18048]\n",
      "| Epoch [183/200] Iter[161/391]\t\tLoss: 1.1230 Acc@1: 53.971% [11122/20608]\n",
      "| Epoch [183/200] Iter[181/391]\t\tLoss: 2.7962 Acc@1: 53.429% [12378/23168]\n",
      "| Epoch [183/200] Iter[201/391]\t\tLoss: 3.0225 Acc@1: 52.987% [13632/25728]\n",
      "| Epoch [183/200] Iter[221/391]\t\tLoss: 2.7707 Acc@1: 52.794% [14934/28288]\n",
      "| Epoch [183/200] Iter[241/391]\t\tLoss: 3.1127 Acc@1: 53.496% [16502/30848]\n",
      "| Epoch [183/200] Iter[261/391]\t\tLoss: 2.6526 Acc@1: 53.456% [17858/33408]\n",
      "| Epoch [183/200] Iter[281/391]\t\tLoss: 1.1949 Acc@1: 53.586% [19273/35968]\n",
      "| Epoch [183/200] Iter[301/391]\t\tLoss: 2.5253 Acc@1: 53.271% [20524/38528]\n",
      "| Epoch [183/200] Iter[321/391]\t\tLoss: 1.1128 Acc@1: 53.267% [21886/41088]\n",
      "| Epoch [183/200] Iter[341/391]\t\tLoss: 2.7750 Acc@1: 52.861% [23072/43648]\n",
      "| Epoch [183/200] Iter[361/391]\t\tLoss: 3.0649 Acc@1: 52.630% [24319/46208]\n",
      "| Epoch [183/200] Iter[381/391]\t\tLoss: 3.0571 Acc@1: 52.599% [25651/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #183\n",
      "\n",
      "| Validation Epoch #183\t\t\tLoss: 1.3204 Acc@1: 66.45%\n",
      "\n",
      "----- Elapsed time : 0:54:24\n",
      "\n",
      "\n",
      "=> Training Epoch #184\n",
      "| Epoch [184/200] Iter[  1/391]\t\tLoss: 1.5995 Acc@1: 63.803% [ 81/128]\n",
      "| Epoch [184/200] Iter[ 21/391]\t\tLoss: 2.9710 Acc@1: 52.891% [1421/2688]\n",
      "| Epoch [184/200] Iter[ 41/391]\t\tLoss: 1.0570 Acc@1: 56.129% [2945/5248]\n",
      "| Epoch [184/200] Iter[ 61/391]\t\tLoss: 2.9468 Acc@1: 54.923% [4288/7808]\n",
      "| Epoch [184/200] Iter[ 81/391]\t\tLoss: 2.4434 Acc@1: 53.069% [5502/10368]\n",
      "| Epoch [184/200] Iter[101/391]\t\tLoss: 2.5492 Acc@1: 54.028% [6984/12928]\n",
      "| Epoch [184/200] Iter[121/391]\t\tLoss: 2.7769 Acc@1: 54.072% [8374/15488]\n",
      "| Epoch [184/200] Iter[141/391]\t\tLoss: 3.1494 Acc@1: 53.228% [9606/18048]\n",
      "| Epoch [184/200] Iter[161/391]\t\tLoss: 2.9929 Acc@1: 52.275% [10772/20608]\n",
      "| Epoch [184/200] Iter[181/391]\t\tLoss: 2.1001 Acc@1: 52.322% [12122/23168]\n",
      "| Epoch [184/200] Iter[201/391]\t\tLoss: 2.9723 Acc@1: 52.131% [13412/25728]\n",
      "| Epoch [184/200] Iter[221/391]\t\tLoss: 3.1740 Acc@1: 52.071% [14729/28288]\n",
      "| Epoch [184/200] Iter[241/391]\t\tLoss: 2.9610 Acc@1: 51.991% [16038/30848]\n",
      "| Epoch [184/200] Iter[261/391]\t\tLoss: 2.3806 Acc@1: 52.135% [17417/33408]\n",
      "| Epoch [184/200] Iter[281/391]\t\tLoss: 1.8651 Acc@1: 51.793% [18629/35968]\n",
      "| Epoch [184/200] Iter[301/391]\t\tLoss: 2.0487 Acc@1: 51.918% [20002/38528]\n",
      "| Epoch [184/200] Iter[321/391]\t\tLoss: 2.0089 Acc@1: 51.693% [21239/41088]\n",
      "| Epoch [184/200] Iter[341/391]\t\tLoss: 2.4785 Acc@1: 51.269% [22377/43648]\n",
      "| Epoch [184/200] Iter[361/391]\t\tLoss: 2.7336 Acc@1: 51.313% [23710/46208]\n",
      "| Epoch [184/200] Iter[381/391]\t\tLoss: 2.5449 Acc@1: 51.107% [24923/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #184\n",
      "\n",
      "| Validation Epoch #184\t\t\tLoss: 1.3077 Acc@1: 66.45%\n",
      "\n",
      "----- Elapsed time : 0:54:41\n",
      "\n",
      "\n",
      "=> Training Epoch #185\n",
      "| Epoch [185/200] Iter[  1/391]\t\tLoss: 2.8309 Acc@1: 33.219% [ 42/128]\n",
      "| Epoch [185/200] Iter[ 21/391]\t\tLoss: 2.4821 Acc@1: 51.197% [1376/2688]\n",
      "| Epoch [185/200] Iter[ 41/391]\t\tLoss: 1.6746 Acc@1: 51.859% [2721/5248]\n",
      "| Epoch [185/200] Iter[ 61/391]\t\tLoss: 0.9925 Acc@1: 51.567% [4026/7808]\n",
      "| Epoch [185/200] Iter[ 81/391]\t\tLoss: 1.9912 Acc@1: 51.748% [5365/10368]\n",
      "| Epoch [185/200] Iter[101/391]\t\tLoss: 2.9606 Acc@1: 51.894% [6708/12928]\n",
      "| Epoch [185/200] Iter[121/391]\t\tLoss: 0.9738 Acc@1: 51.413% [7962/15488]\n",
      "| Epoch [185/200] Iter[141/391]\t\tLoss: 1.5019 Acc@1: 51.745% [9338/18048]\n",
      "| Epoch [185/200] Iter[161/391]\t\tLoss: 1.7470 Acc@1: 52.276% [10773/20608]\n",
      "| Epoch [185/200] Iter[181/391]\t\tLoss: 1.1418 Acc@1: 52.842% [12242/23168]\n",
      "| Epoch [185/200] Iter[201/391]\t\tLoss: 2.8315 Acc@1: 53.008% [13637/25728]\n",
      "| Epoch [185/200] Iter[221/391]\t\tLoss: 3.1084 Acc@1: 52.662% [14897/28288]\n",
      "| Epoch [185/200] Iter[241/391]\t\tLoss: 1.1055 Acc@1: 52.219% [16108/30848]\n",
      "| Epoch [185/200] Iter[261/391]\t\tLoss: 2.9973 Acc@1: 52.040% [17385/33408]\n",
      "| Epoch [185/200] Iter[281/391]\t\tLoss: 1.7138 Acc@1: 51.641% [18574/35968]\n",
      "| Epoch [185/200] Iter[301/391]\t\tLoss: 2.9545 Acc@1: 51.596% [19878/38528]\n",
      "| Epoch [185/200] Iter[321/391]\t\tLoss: 2.5103 Acc@1: 51.625% [21211/41088]\n",
      "| Epoch [185/200] Iter[341/391]\t\tLoss: 2.8029 Acc@1: 51.435% [22450/43648]\n",
      "| Epoch [185/200] Iter[361/391]\t\tLoss: 2.5256 Acc@1: 51.405% [23753/46208]\n",
      "| Epoch [185/200] Iter[381/391]\t\tLoss: 3.1436 Acc@1: 51.226% [24981/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #185\n",
      "\n",
      "| Validation Epoch #185\t\t\tLoss: 1.3270 Acc@1: 66.47%\n",
      "\n",
      "----- Elapsed time : 0:54:59\n",
      "\n",
      "\n",
      "=> Training Epoch #186\n",
      "| Epoch [186/200] Iter[  1/391]\t\tLoss: 2.6190 Acc@1: 47.268% [ 60/128]\n",
      "| Epoch [186/200] Iter[ 21/391]\t\tLoss: 2.8271 Acc@1: 50.081% [1346/2688]\n",
      "| Epoch [186/200] Iter[ 41/391]\t\tLoss: 1.3753 Acc@1: 52.845% [2773/5248]\n",
      "| Epoch [186/200] Iter[ 61/391]\t\tLoss: 2.7885 Acc@1: 50.036% [3906/7808]\n",
      "| Epoch [186/200] Iter[ 81/391]\t\tLoss: 1.9043 Acc@1: 51.020% [5289/10368]\n",
      "| Epoch [186/200] Iter[101/391]\t\tLoss: 3.1213 Acc@1: 50.992% [6592/12928]\n",
      "| Epoch [186/200] Iter[121/391]\t\tLoss: 2.0177 Acc@1: 50.906% [7884/15488]\n",
      "| Epoch [186/200] Iter[141/391]\t\tLoss: 3.0096 Acc@1: 50.723% [9154/18048]\n",
      "| Epoch [186/200] Iter[161/391]\t\tLoss: 1.0615 Acc@1: 50.541% [10415/20608]\n",
      "| Epoch [186/200] Iter[181/391]\t\tLoss: 2.9932 Acc@1: 50.500% [11699/23168]\n",
      "| Epoch [186/200] Iter[201/391]\t\tLoss: 1.8135 Acc@1: 50.703% [13044/25728]\n",
      "| Epoch [186/200] Iter[221/391]\t\tLoss: 2.4586 Acc@1: 51.124% [14461/28288]\n",
      "| Epoch [186/200] Iter[241/391]\t\tLoss: 1.8876 Acc@1: 51.034% [15743/30848]\n",
      "| Epoch [186/200] Iter[261/391]\t\tLoss: 3.0312 Acc@1: 50.956% [17023/33408]\n",
      "| Epoch [186/200] Iter[281/391]\t\tLoss: 2.1675 Acc@1: 51.045% [18359/35968]\n",
      "| Epoch [186/200] Iter[301/391]\t\tLoss: 3.0040 Acc@1: 50.964% [19635/38528]\n",
      "| Epoch [186/200] Iter[321/391]\t\tLoss: 3.1294 Acc@1: 50.828% [20884/41088]\n",
      "| Epoch [186/200] Iter[341/391]\t\tLoss: 1.6533 Acc@1: 51.324% [22402/43648]\n",
      "| Epoch [186/200] Iter[361/391]\t\tLoss: 2.6249 Acc@1: 51.656% [23869/46208]\n",
      "| Epoch [186/200] Iter[381/391]\t\tLoss: 1.3603 Acc@1: 51.708% [25216/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #186\n",
      "\n",
      "| Validation Epoch #186\t\t\tLoss: 1.3209 Acc@1: 66.11%\n",
      "\n",
      "----- Elapsed time : 0:55:16\n",
      "\n",
      "\n",
      "=> Training Epoch #187\n",
      "| Epoch [187/200] Iter[  1/391]\t\tLoss: 3.0017 Acc@1: 30.527% [ 39/128]\n",
      "| Epoch [187/200] Iter[ 21/391]\t\tLoss: 2.4093 Acc@1: 45.191% [1214/2688]\n",
      "| Epoch [187/200] Iter[ 41/391]\t\tLoss: 2.6590 Acc@1: 46.079% [2418/5248]\n",
      "| Epoch [187/200] Iter[ 61/391]\t\tLoss: 2.3094 Acc@1: 47.451% [3704/7808]\n",
      "| Epoch [187/200] Iter[ 81/391]\t\tLoss: 2.1098 Acc@1: 48.533% [5031/10368]\n",
      "| Epoch [187/200] Iter[101/391]\t\tLoss: 2.0803 Acc@1: 49.577% [6409/12928]\n",
      "| Epoch [187/200] Iter[121/391]\t\tLoss: 2.1134 Acc@1: 50.107% [7760/15488]\n",
      "| Epoch [187/200] Iter[141/391]\t\tLoss: 2.8808 Acc@1: 50.858% [9178/18048]\n",
      "| Epoch [187/200] Iter[161/391]\t\tLoss: 1.4481 Acc@1: 50.948% [10499/20608]\n",
      "| Epoch [187/200] Iter[181/391]\t\tLoss: 3.0419 Acc@1: 50.799% [11769/23168]\n",
      "| Epoch [187/200] Iter[201/391]\t\tLoss: 1.2136 Acc@1: 50.913% [13098/25728]\n",
      "| Epoch [187/200] Iter[221/391]\t\tLoss: 1.7243 Acc@1: 51.141% [14466/28288]\n",
      "| Epoch [187/200] Iter[241/391]\t\tLoss: 2.7451 Acc@1: 51.573% [15909/30848]\n",
      "| Epoch [187/200] Iter[261/391]\t\tLoss: 1.6795 Acc@1: 51.415% [17176/33408]\n",
      "| Epoch [187/200] Iter[281/391]\t\tLoss: 1.9326 Acc@1: 51.544% [18539/35968]\n",
      "| Epoch [187/200] Iter[301/391]\t\tLoss: 2.9130 Acc@1: 51.580% [19872/38528]\n",
      "| Epoch [187/200] Iter[321/391]\t\tLoss: 2.8381 Acc@1: 51.553% [21182/41088]\n",
      "| Epoch [187/200] Iter[341/391]\t\tLoss: 2.4723 Acc@1: 51.548% [22499/43648]\n",
      "| Epoch [187/200] Iter[361/391]\t\tLoss: 2.2005 Acc@1: 51.824% [23946/46208]\n",
      "| Epoch [187/200] Iter[381/391]\t\tLoss: 2.2589 Acc@1: 52.009% [25363/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #187\n",
      "\n",
      "| Validation Epoch #187\t\t\tLoss: 1.3213 Acc@1: 66.24%\n",
      "\n",
      "----- Elapsed time : 0:55:34\n",
      "\n",
      "\n",
      "=> Training Epoch #188\n",
      "| Epoch [188/200] Iter[  1/391]\t\tLoss: 2.9141 Acc@1: 31.387% [ 40/128]\n",
      "| Epoch [188/200] Iter[ 21/391]\t\tLoss: 1.3801 Acc@1: 56.777% [1526/2688]\n",
      "| Epoch [188/200] Iter[ 41/391]\t\tLoss: 0.9762 Acc@1: 53.451% [2805/5248]\n",
      "| Epoch [188/200] Iter[ 61/391]\t\tLoss: 2.0341 Acc@1: 51.675% [4034/7808]\n",
      "| Epoch [188/200] Iter[ 81/391]\t\tLoss: 2.6026 Acc@1: 53.071% [5502/10368]\n",
      "| Epoch [188/200] Iter[101/391]\t\tLoss: 1.7991 Acc@1: 52.242% [6753/12928]\n",
      "| Epoch [188/200] Iter[121/391]\t\tLoss: 2.8326 Acc@1: 52.906% [8194/15488]\n",
      "| Epoch [188/200] Iter[141/391]\t\tLoss: 2.3336 Acc@1: 52.458% [9467/18048]\n",
      "| Epoch [188/200] Iter[161/391]\t\tLoss: 1.6047 Acc@1: 51.627% [10639/20608]\n",
      "| Epoch [188/200] Iter[181/391]\t\tLoss: 2.2852 Acc@1: 51.643% [11964/23168]\n",
      "| Epoch [188/200] Iter[201/391]\t\tLoss: 3.1609 Acc@1: 51.714% [13304/25728]\n",
      "| Epoch [188/200] Iter[221/391]\t\tLoss: 2.3399 Acc@1: 52.089% [14734/28288]\n",
      "| Epoch [188/200] Iter[241/391]\t\tLoss: 2.7999 Acc@1: 51.608% [15920/30848]\n",
      "| Epoch [188/200] Iter[261/391]\t\tLoss: 1.0805 Acc@1: 50.921% [17011/33408]\n",
      "| Epoch [188/200] Iter[281/391]\t\tLoss: 2.3902 Acc@1: 51.205% [18417/35968]\n",
      "| Epoch [188/200] Iter[301/391]\t\tLoss: 2.0606 Acc@1: 50.938% [19625/38528]\n",
      "| Epoch [188/200] Iter[321/391]\t\tLoss: 3.0298 Acc@1: 50.474% [20738/41088]\n",
      "| Epoch [188/200] Iter[341/391]\t\tLoss: 3.0667 Acc@1: 50.339% [21972/43648]\n",
      "| Epoch [188/200] Iter[361/391]\t\tLoss: 2.5039 Acc@1: 50.827% [23486/46208]\n",
      "| Epoch [188/200] Iter[381/391]\t\tLoss: 1.0496 Acc@1: 50.739% [24744/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #188\n",
      "\n",
      "| Validation Epoch #188\t\t\tLoss: 1.3513 Acc@1: 66.28%\n",
      "\n",
      "----- Elapsed time : 0:55:51\n",
      "\n",
      "\n",
      "=> Training Epoch #189\n",
      "| Epoch [189/200] Iter[  1/391]\t\tLoss: 2.5710 Acc@1: 45.882% [ 58/128]\n",
      "| Epoch [189/200] Iter[ 21/391]\t\tLoss: 1.3254 Acc@1: 57.031% [1532/2688]\n",
      "| Epoch [189/200] Iter[ 41/391]\t\tLoss: 1.9671 Acc@1: 52.631% [2762/5248]\n",
      "| Epoch [189/200] Iter[ 61/391]\t\tLoss: 3.1390 Acc@1: 52.062% [4064/7808]\n",
      "| Epoch [189/200] Iter[ 81/391]\t\tLoss: 2.7743 Acc@1: 52.935% [5488/10368]\n",
      "| Epoch [189/200] Iter[101/391]\t\tLoss: 2.9641 Acc@1: 51.684% [6681/12928]\n",
      "| Epoch [189/200] Iter[121/391]\t\tLoss: 3.0746 Acc@1: 52.190% [8083/15488]\n",
      "| Epoch [189/200] Iter[141/391]\t\tLoss: 1.6565 Acc@1: 51.319% [9261/18048]\n",
      "| Epoch [189/200] Iter[161/391]\t\tLoss: 2.1935 Acc@1: 51.176% [10546/20608]\n",
      "| Epoch [189/200] Iter[181/391]\t\tLoss: 1.2405 Acc@1: 51.455% [11921/23168]\n",
      "| Epoch [189/200] Iter[201/391]\t\tLoss: 2.9112 Acc@1: 51.765% [13318/25728]\n",
      "| Epoch [189/200] Iter[221/391]\t\tLoss: 2.9707 Acc@1: 50.963% [14416/28288]\n",
      "| Epoch [189/200] Iter[241/391]\t\tLoss: 2.3298 Acc@1: 50.978% [15725/30848]\n",
      "| Epoch [189/200] Iter[261/391]\t\tLoss: 3.0497 Acc@1: 50.570% [16894/33408]\n",
      "| Epoch [189/200] Iter[281/391]\t\tLoss: 2.0358 Acc@1: 50.784% [18266/35968]\n",
      "| Epoch [189/200] Iter[301/391]\t\tLoss: 2.1085 Acc@1: 50.697% [19532/38528]\n",
      "| Epoch [189/200] Iter[321/391]\t\tLoss: 3.0951 Acc@1: 50.429% [20720/41088]\n",
      "| Epoch [189/200] Iter[341/391]\t\tLoss: 3.0256 Acc@1: 50.391% [21994/43648]\n",
      "| Epoch [189/200] Iter[361/391]\t\tLoss: 2.8048 Acc@1: 50.444% [23309/46208]\n",
      "| Epoch [189/200] Iter[381/391]\t\tLoss: 2.5641 Acc@1: 50.738% [24743/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #189\n",
      "\n",
      "| Validation Epoch #189\t\t\tLoss: 1.2950 Acc@1: 66.31%\n",
      "\n",
      "----- Elapsed time : 0:56:08\n",
      "\n",
      "\n",
      "=> Training Epoch #190\n",
      "| Epoch [190/200] Iter[  1/391]\t\tLoss: 1.8130 Acc@1: 66.450% [ 85/128]\n",
      "| Epoch [190/200] Iter[ 21/391]\t\tLoss: 2.9559 Acc@1: 46.786% [1257/2688]\n",
      "| Epoch [190/200] Iter[ 41/391]\t\tLoss: 2.9021 Acc@1: 48.515% [2546/5248]\n",
      "| Epoch [190/200] Iter[ 61/391]\t\tLoss: 2.1713 Acc@1: 49.725% [3882/7808]\n",
      "| Epoch [190/200] Iter[ 81/391]\t\tLoss: 1.5225 Acc@1: 49.767% [5159/10368]\n",
      "| Epoch [190/200] Iter[101/391]\t\tLoss: 2.8143 Acc@1: 50.471% [6524/12928]\n",
      "| Epoch [190/200] Iter[121/391]\t\tLoss: 2.2711 Acc@1: 49.666% [7692/15488]\n",
      "| Epoch [190/200] Iter[141/391]\t\tLoss: 2.0442 Acc@1: 50.452% [9105/18048]\n",
      "| Epoch [190/200] Iter[161/391]\t\tLoss: 1.4402 Acc@1: 50.217% [10348/20608]\n",
      "| Epoch [190/200] Iter[181/391]\t\tLoss: 2.5567 Acc@1: 49.899% [11560/23168]\n",
      "| Epoch [190/200] Iter[201/391]\t\tLoss: 3.0110 Acc@1: 50.032% [12872/25728]\n",
      "| Epoch [190/200] Iter[221/391]\t\tLoss: 1.5029 Acc@1: 50.472% [14277/28288]\n",
      "| Epoch [190/200] Iter[241/391]\t\tLoss: 2.7304 Acc@1: 50.317% [15521/30848]\n",
      "| Epoch [190/200] Iter[261/391]\t\tLoss: 2.6219 Acc@1: 50.030% [16714/33408]\n",
      "| Epoch [190/200] Iter[281/391]\t\tLoss: 2.7769 Acc@1: 49.982% [17977/35968]\n",
      "| Epoch [190/200] Iter[301/391]\t\tLoss: 1.9758 Acc@1: 50.160% [19325/38528]\n",
      "| Epoch [190/200] Iter[321/391]\t\tLoss: 1.5491 Acc@1: 50.033% [20557/41088]\n",
      "| Epoch [190/200] Iter[341/391]\t\tLoss: 3.0912 Acc@1: 49.912% [21785/43648]\n",
      "| Epoch [190/200] Iter[361/391]\t\tLoss: 2.8049 Acc@1: 50.328% [23255/46208]\n",
      "| Epoch [190/200] Iter[381/391]\t\tLoss: 2.0428 Acc@1: 50.147% [24455/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #190\n",
      "\n",
      "| Validation Epoch #190\t\t\tLoss: 1.3197 Acc@1: 66.19%\n",
      "\n",
      "----- Elapsed time : 0:56:26\n",
      "\n",
      "\n",
      "=> Training Epoch #191\n",
      "| Epoch [191/200] Iter[  1/391]\t\tLoss: 2.9990 Acc@1: 31.723% [ 40/128]\n",
      "| Epoch [191/200] Iter[ 21/391]\t\tLoss: 2.2582 Acc@1: 47.575% [1278/2688]\n",
      "| Epoch [191/200] Iter[ 41/391]\t\tLoss: 2.0797 Acc@1: 52.848% [2773/5248]\n",
      "| Epoch [191/200] Iter[ 61/391]\t\tLoss: 2.6793 Acc@1: 49.140% [3836/7808]\n",
      "| Epoch [191/200] Iter[ 81/391]\t\tLoss: 2.3803 Acc@1: 50.780% [5264/10368]\n",
      "| Epoch [191/200] Iter[101/391]\t\tLoss: 1.1161 Acc@1: 50.149% [6483/12928]\n",
      "| Epoch [191/200] Iter[121/391]\t\tLoss: 2.8908 Acc@1: 50.981% [7895/15488]\n",
      "| Epoch [191/200] Iter[141/391]\t\tLoss: 1.9806 Acc@1: 51.019% [9207/18048]\n",
      "| Epoch [191/200] Iter[161/391]\t\tLoss: 2.9147 Acc@1: 51.326% [10577/20608]\n",
      "| Epoch [191/200] Iter[181/391]\t\tLoss: 3.0073 Acc@1: 52.249% [12105/23168]\n",
      "| Epoch [191/200] Iter[201/391]\t\tLoss: 3.0401 Acc@1: 51.856% [13341/25728]\n",
      "| Epoch [191/200] Iter[221/391]\t\tLoss: 3.1155 Acc@1: 51.683% [14620/28288]\n",
      "| Epoch [191/200] Iter[241/391]\t\tLoss: 2.6964 Acc@1: 51.558% [15904/30848]\n",
      "| Epoch [191/200] Iter[261/391]\t\tLoss: 1.0422 Acc@1: 51.809% [17308/33408]\n",
      "| Epoch [191/200] Iter[281/391]\t\tLoss: 2.9759 Acc@1: 52.080% [18732/35968]\n",
      "| Epoch [191/200] Iter[301/391]\t\tLoss: 1.8219 Acc@1: 51.923% [20004/38528]\n",
      "| Epoch [191/200] Iter[321/391]\t\tLoss: 2.0406 Acc@1: 52.118% [21414/41088]\n",
      "| Epoch [191/200] Iter[341/391]\t\tLoss: 2.7660 Acc@1: 52.154% [22763/43648]\n",
      "| Epoch [191/200] Iter[361/391]\t\tLoss: 1.0239 Acc@1: 52.083% [24066/46208]\n",
      "| Epoch [191/200] Iter[381/391]\t\tLoss: 0.9917 Acc@1: 52.187% [25450/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #191\n",
      "\n",
      "| Validation Epoch #191\t\t\tLoss: 1.2802 Acc@1: 66.38%\n",
      "\n",
      "----- Elapsed time : 0:56:43\n",
      "\n",
      "\n",
      "=> Training Epoch #192\n",
      "| Epoch [192/200] Iter[  1/391]\t\tLoss: 2.9752 Acc@1: 30.196% [ 38/128]\n",
      "| Epoch [192/200] Iter[ 21/391]\t\tLoss: 2.9304 Acc@1: 48.029% [1291/2688]\n",
      "| Epoch [192/200] Iter[ 41/391]\t\tLoss: 2.3147 Acc@1: 51.117% [2682/5248]\n",
      "| Epoch [192/200] Iter[ 61/391]\t\tLoss: 1.3494 Acc@1: 51.827% [4046/7808]\n",
      "| Epoch [192/200] Iter[ 81/391]\t\tLoss: 2.3648 Acc@1: 52.108% [5402/10368]\n",
      "| Epoch [192/200] Iter[101/391]\t\tLoss: 1.7035 Acc@1: 51.695% [6683/12928]\n",
      "| Epoch [192/200] Iter[121/391]\t\tLoss: 1.7947 Acc@1: 52.163% [8078/15488]\n",
      "| Epoch [192/200] Iter[141/391]\t\tLoss: 2.1245 Acc@1: 52.963% [9558/18048]\n",
      "| Epoch [192/200] Iter[161/391]\t\tLoss: 1.5506 Acc@1: 53.363% [10997/20608]\n",
      "| Epoch [192/200] Iter[181/391]\t\tLoss: 2.6215 Acc@1: 52.899% [12255/23168]\n",
      "| Epoch [192/200] Iter[201/391]\t\tLoss: 1.6829 Acc@1: 52.579% [13527/25728]\n",
      "| Epoch [192/200] Iter[221/391]\t\tLoss: 2.8083 Acc@1: 52.848% [14949/28288]\n",
      "| Epoch [192/200] Iter[241/391]\t\tLoss: 2.5198 Acc@1: 52.511% [16198/30848]\n",
      "| Epoch [192/200] Iter[261/391]\t\tLoss: 1.3415 Acc@1: 53.079% [17732/33408]\n",
      "| Epoch [192/200] Iter[281/391]\t\tLoss: 3.0334 Acc@1: 52.598% [18918/35968]\n",
      "| Epoch [192/200] Iter[301/391]\t\tLoss: 1.8115 Acc@1: 52.656% [20287/38528]\n",
      "| Epoch [192/200] Iter[321/391]\t\tLoss: 2.0578 Acc@1: 52.709% [21657/41088]\n",
      "| Epoch [192/200] Iter[341/391]\t\tLoss: 1.6075 Acc@1: 53.113% [23182/43648]\n",
      "| Epoch [192/200] Iter[361/391]\t\tLoss: 3.1626 Acc@1: 53.187% [24576/46208]\n",
      "| Epoch [192/200] Iter[381/391]\t\tLoss: 1.5826 Acc@1: 53.247% [25967/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #192\n",
      "\n",
      "| Validation Epoch #192\t\t\tLoss: 1.2716 Acc@1: 66.34%\n",
      "\n",
      "----- Elapsed time : 0:57:01\n",
      "\n",
      "\n",
      "=> Training Epoch #193\n",
      "| Epoch [193/200] Iter[  1/391]\t\tLoss: 2.6180 Acc@1: 43.362% [ 55/128]\n",
      "| Epoch [193/200] Iter[ 21/391]\t\tLoss: 3.0398 Acc@1: 52.035% [1398/2688]\n",
      "| Epoch [193/200] Iter[ 41/391]\t\tLoss: 1.2807 Acc@1: 52.385% [2749/5248]\n",
      "| Epoch [193/200] Iter[ 61/391]\t\tLoss: 1.5034 Acc@1: 52.514% [4100/7808]\n",
      "| Epoch [193/200] Iter[ 81/391]\t\tLoss: 2.8741 Acc@1: 50.893% [5276/10368]\n",
      "| Epoch [193/200] Iter[101/391]\t\tLoss: 2.5132 Acc@1: 51.502% [6658/12928]\n",
      "| Epoch [193/200] Iter[121/391]\t\tLoss: 1.9371 Acc@1: 52.100% [8069/15488]\n",
      "| Epoch [193/200] Iter[141/391]\t\tLoss: 3.0619 Acc@1: 52.063% [9396/18048]\n",
      "| Epoch [193/200] Iter[161/391]\t\tLoss: 1.0996 Acc@1: 52.581% [10835/20608]\n",
      "| Epoch [193/200] Iter[181/391]\t\tLoss: 3.0046 Acc@1: 52.718% [12213/23168]\n",
      "| Epoch [193/200] Iter[201/391]\t\tLoss: 2.8100 Acc@1: 52.224% [13436/25728]\n",
      "| Epoch [193/200] Iter[221/391]\t\tLoss: 2.8818 Acc@1: 52.467% [14841/28288]\n",
      "| Epoch [193/200] Iter[241/391]\t\tLoss: 3.0318 Acc@1: 52.276% [16126/30848]\n",
      "| Epoch [193/200] Iter[261/391]\t\tLoss: 2.1997 Acc@1: 52.219% [17445/33408]\n",
      "| Epoch [193/200] Iter[281/391]\t\tLoss: 1.6640 Acc@1: 52.041% [18718/35968]\n",
      "| Epoch [193/200] Iter[301/391]\t\tLoss: 2.1009 Acc@1: 51.985% [20028/38528]\n",
      "| Epoch [193/200] Iter[321/391]\t\tLoss: 1.9361 Acc@1: 51.948% [21344/41088]\n",
      "| Epoch [193/200] Iter[341/391]\t\tLoss: 2.4332 Acc@1: 51.572% [22510/43648]\n",
      "| Epoch [193/200] Iter[361/391]\t\tLoss: 2.3439 Acc@1: 51.904% [23983/46208]\n",
      "| Epoch [193/200] Iter[381/391]\t\tLoss: 3.1167 Acc@1: 51.924% [25322/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #193\n",
      "\n",
      "| Validation Epoch #193\t\t\tLoss: 1.2966 Acc@1: 66.43%\n",
      "\n",
      "----- Elapsed time : 0:57:18\n",
      "\n",
      "\n",
      "=> Training Epoch #194\n",
      "| Epoch [194/200] Iter[  1/391]\t\tLoss: 2.8064 Acc@1: 34.422% [ 44/128]\n",
      "| Epoch [194/200] Iter[ 21/391]\t\tLoss: 2.4602 Acc@1: 57.743% [1552/2688]\n",
      "| Epoch [194/200] Iter[ 41/391]\t\tLoss: 2.3592 Acc@1: 54.235% [2846/5248]\n",
      "| Epoch [194/200] Iter[ 61/391]\t\tLoss: 3.0868 Acc@1: 54.430% [4249/7808]\n",
      "| Epoch [194/200] Iter[ 81/391]\t\tLoss: 2.6700 Acc@1: 53.361% [5532/10368]\n",
      "| Epoch [194/200] Iter[101/391]\t\tLoss: 2.2209 Acc@1: 53.419% [6906/12928]\n",
      "| Epoch [194/200] Iter[121/391]\t\tLoss: 2.8760 Acc@1: 52.637% [8152/15488]\n",
      "| Epoch [194/200] Iter[141/391]\t\tLoss: 1.3588 Acc@1: 53.686% [9689/18048]\n",
      "| Epoch [194/200] Iter[161/391]\t\tLoss: 2.5239 Acc@1: 53.369% [10998/20608]\n",
      "| Epoch [194/200] Iter[181/391]\t\tLoss: 2.9171 Acc@1: 53.768% [12456/23168]\n",
      "| Epoch [194/200] Iter[201/391]\t\tLoss: 2.8865 Acc@1: 52.977% [13629/25728]\n",
      "| Epoch [194/200] Iter[221/391]\t\tLoss: 2.0331 Acc@1: 52.792% [14933/28288]\n",
      "| Epoch [194/200] Iter[241/391]\t\tLoss: 1.6453 Acc@1: 52.987% [16345/30848]\n",
      "| Epoch [194/200] Iter[261/391]\t\tLoss: 2.5459 Acc@1: 53.324% [17814/33408]\n",
      "| Epoch [194/200] Iter[281/391]\t\tLoss: 1.9616 Acc@1: 53.209% [19138/35968]\n",
      "| Epoch [194/200] Iter[301/391]\t\tLoss: 2.5095 Acc@1: 52.940% [20396/38528]\n",
      "| Epoch [194/200] Iter[321/391]\t\tLoss: 2.9504 Acc@1: 52.507% [21573/41088]\n",
      "| Epoch [194/200] Iter[341/391]\t\tLoss: 1.6502 Acc@1: 52.598% [22957/43648]\n",
      "| Epoch [194/200] Iter[361/391]\t\tLoss: 1.3501 Acc@1: 52.623% [24315/46208]\n",
      "| Epoch [194/200] Iter[381/391]\t\tLoss: 1.0300 Acc@1: 52.536% [25620/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #194\n",
      "\n",
      "| Validation Epoch #194\t\t\tLoss: 1.3026 Acc@1: 66.30%\n",
      "\n",
      "----- Elapsed time : 0:57:35\n",
      "\n",
      "\n",
      "=> Training Epoch #195\n",
      "| Epoch [195/200] Iter[  1/391]\t\tLoss: 3.1696 Acc@1: 27.077% [ 34/128]\n",
      "| Epoch [195/200] Iter[ 21/391]\t\tLoss: 2.3274 Acc@1: 53.558% [1439/2688]\n",
      "| Epoch [195/200] Iter[ 41/391]\t\tLoss: 2.5181 Acc@1: 53.979% [2832/5248]\n",
      "| Epoch [195/200] Iter[ 61/391]\t\tLoss: 1.9979 Acc@1: 53.118% [4147/7808]\n",
      "| Epoch [195/200] Iter[ 81/391]\t\tLoss: 1.9765 Acc@1: 53.625% [5559/10368]\n",
      "| Epoch [195/200] Iter[101/391]\t\tLoss: 2.0998 Acc@1: 53.573% [6925/12928]\n",
      "| Epoch [195/200] Iter[121/391]\t\tLoss: 2.6721 Acc@1: 53.645% [8308/15488]\n",
      "| Epoch [195/200] Iter[141/391]\t\tLoss: 3.1647 Acc@1: 52.824% [9533/18048]\n",
      "| Epoch [195/200] Iter[161/391]\t\tLoss: 2.5735 Acc@1: 52.507% [10820/20608]\n",
      "| Epoch [195/200] Iter[181/391]\t\tLoss: 2.8627 Acc@1: 53.043% [12289/23168]\n",
      "| Epoch [195/200] Iter[201/391]\t\tLoss: 2.9811 Acc@1: 52.760% [13574/25728]\n",
      "| Epoch [195/200] Iter[221/391]\t\tLoss: 2.5837 Acc@1: 52.913% [14967/28288]\n",
      "| Epoch [195/200] Iter[241/391]\t\tLoss: 3.1127 Acc@1: 52.861% [16306/30848]\n",
      "| Epoch [195/200] Iter[261/391]\t\tLoss: 2.9214 Acc@1: 53.435% [17851/33408]\n",
      "| Epoch [195/200] Iter[281/391]\t\tLoss: 1.8006 Acc@1: 53.143% [19114/35968]\n",
      "| Epoch [195/200] Iter[301/391]\t\tLoss: 2.4500 Acc@1: 53.338% [20550/38528]\n",
      "| Epoch [195/200] Iter[321/391]\t\tLoss: 1.7273 Acc@1: 53.507% [21984/41088]\n",
      "| Epoch [195/200] Iter[341/391]\t\tLoss: 1.5959 Acc@1: 53.564% [23379/43648]\n",
      "| Epoch [195/200] Iter[361/391]\t\tLoss: 2.1641 Acc@1: 53.331% [24643/46208]\n",
      "| Epoch [195/200] Iter[381/391]\t\tLoss: 1.5991 Acc@1: 52.939% [25817/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #195\n",
      "\n",
      "| Validation Epoch #195\t\t\tLoss: 1.3163 Acc@1: 66.28%\n",
      "\n",
      "----- Elapsed time : 0:57:53\n",
      "\n",
      "\n",
      "=> Training Epoch #196\n",
      "| Epoch [196/200] Iter[  1/391]\t\tLoss: 2.2100 Acc@1: 59.162% [ 75/128]\n",
      "| Epoch [196/200] Iter[ 21/391]\t\tLoss: 2.5637 Acc@1: 47.011% [1263/2688]\n",
      "| Epoch [196/200] Iter[ 41/391]\t\tLoss: 1.5505 Acc@1: 50.778% [2664/5248]\n",
      "| Epoch [196/200] Iter[ 61/391]\t\tLoss: 2.9243 Acc@1: 52.193% [4075/7808]\n",
      "| Epoch [196/200] Iter[ 81/391]\t\tLoss: 2.5298 Acc@1: 51.248% [5313/10368]\n",
      "| Epoch [196/200] Iter[101/391]\t\tLoss: 2.5782 Acc@1: 50.424% [6518/12928]\n",
      "| Epoch [196/200] Iter[121/391]\t\tLoss: 3.2754 Acc@1: 49.830% [7717/15488]\n",
      "| Epoch [196/200] Iter[141/391]\t\tLoss: 2.7729 Acc@1: 50.481% [9110/18048]\n",
      "| Epoch [196/200] Iter[161/391]\t\tLoss: 3.0183 Acc@1: 50.718% [10451/20608]\n",
      "| Epoch [196/200] Iter[181/391]\t\tLoss: 2.5923 Acc@1: 51.840% [12010/23168]\n",
      "| Epoch [196/200] Iter[201/391]\t\tLoss: 2.7645 Acc@1: 51.974% [13371/25728]\n",
      "| Epoch [196/200] Iter[221/391]\t\tLoss: 2.1401 Acc@1: 52.460% [14839/28288]\n",
      "| Epoch [196/200] Iter[241/391]\t\tLoss: 0.9009 Acc@1: 52.765% [16277/30848]\n",
      "| Epoch [196/200] Iter[261/391]\t\tLoss: 2.9412 Acc@1: 53.153% [17757/33408]\n",
      "| Epoch [196/200] Iter[281/391]\t\tLoss: 2.2655 Acc@1: 53.171% [19124/35968]\n",
      "| Epoch [196/200] Iter[301/391]\t\tLoss: 3.1325 Acc@1: 53.270% [20523/38528]\n",
      "| Epoch [196/200] Iter[321/391]\t\tLoss: 2.9444 Acc@1: 53.057% [21800/41088]\n",
      "| Epoch [196/200] Iter[341/391]\t\tLoss: 2.1474 Acc@1: 53.111% [23182/43648]\n",
      "| Epoch [196/200] Iter[361/391]\t\tLoss: 2.3110 Acc@1: 53.085% [24529/46208]\n",
      "| Epoch [196/200] Iter[381/391]\t\tLoss: 2.9668 Acc@1: 52.842% [25770/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #196\n",
      "\n",
      "| Validation Epoch #196\t\t\tLoss: 1.3084 Acc@1: 66.26%\n",
      "\n",
      "----- Elapsed time : 0:58:10\n",
      "\n",
      "\n",
      "=> Training Epoch #197\n",
      "| Epoch [197/200] Iter[  1/391]\t\tLoss: 2.8831 Acc@1: 40.119% [ 51/128]\n",
      "| Epoch [197/200] Iter[ 21/391]\t\tLoss: 2.7886 Acc@1: 50.999% [1370/2688]\n",
      "| Epoch [197/200] Iter[ 41/391]\t\tLoss: 1.7811 Acc@1: 51.049% [2679/5248]\n",
      "| Epoch [197/200] Iter[ 61/391]\t\tLoss: 2.0774 Acc@1: 51.860% [4049/7808]\n",
      "| Epoch [197/200] Iter[ 81/391]\t\tLoss: 1.7242 Acc@1: 51.813% [5371/10368]\n",
      "| Epoch [197/200] Iter[101/391]\t\tLoss: 3.0061 Acc@1: 51.970% [6718/12928]\n",
      "| Epoch [197/200] Iter[121/391]\t\tLoss: 2.3900 Acc@1: 50.816% [7870/15488]\n",
      "| Epoch [197/200] Iter[141/391]\t\tLoss: 2.3237 Acc@1: 50.281% [9074/18048]\n",
      "| Epoch [197/200] Iter[161/391]\t\tLoss: 1.1547 Acc@1: 50.735% [10455/20608]\n",
      "| Epoch [197/200] Iter[181/391]\t\tLoss: 2.5721 Acc@1: 50.960% [11806/23168]\n",
      "| Epoch [197/200] Iter[201/391]\t\tLoss: 0.9531 Acc@1: 51.513% [13253/25728]\n",
      "| Epoch [197/200] Iter[221/391]\t\tLoss: 3.0288 Acc@1: 51.472% [14560/28288]\n",
      "| Epoch [197/200] Iter[241/391]\t\tLoss: 1.7995 Acc@1: 51.959% [16028/30848]\n",
      "| Epoch [197/200] Iter[261/391]\t\tLoss: 2.1539 Acc@1: 51.425% [17180/33408]\n",
      "| Epoch [197/200] Iter[281/391]\t\tLoss: 3.2177 Acc@1: 51.319% [18458/35968]\n",
      "| Epoch [197/200] Iter[301/391]\t\tLoss: 1.2033 Acc@1: 52.200% [20111/38528]\n",
      "| Epoch [197/200] Iter[321/391]\t\tLoss: 3.0852 Acc@1: 52.164% [21433/41088]\n",
      "| Epoch [197/200] Iter[341/391]\t\tLoss: 2.6597 Acc@1: 51.937% [22669/43648]\n",
      "| Epoch [197/200] Iter[361/391]\t\tLoss: 2.8839 Acc@1: 51.783% [23928/46208]\n",
      "| Epoch [197/200] Iter[381/391]\t\tLoss: 0.9759 Acc@1: 51.823% [25273/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #197\n",
      "\n",
      "| Validation Epoch #197\t\t\tLoss: 1.3003 Acc@1: 66.38%\n",
      "\n",
      "----- Elapsed time : 0:58:27\n",
      "\n",
      "\n",
      "=> Training Epoch #198\n",
      "| Epoch [198/200] Iter[  1/391]\t\tLoss: 1.4781 Acc@1: 69.008% [ 88/128]\n",
      "| Epoch [198/200] Iter[ 21/391]\t\tLoss: 2.1150 Acc@1: 51.513% [1384/2688]\n",
      "| Epoch [198/200] Iter[ 41/391]\t\tLoss: 1.3979 Acc@1: 50.845% [2668/5248]\n",
      "| Epoch [198/200] Iter[ 61/391]\t\tLoss: 1.0021 Acc@1: 50.350% [3931/7808]\n",
      "| Epoch [198/200] Iter[ 81/391]\t\tLoss: 2.3803 Acc@1: 51.473% [5336/10368]\n",
      "| Epoch [198/200] Iter[101/391]\t\tLoss: 1.9137 Acc@1: 52.210% [6749/12928]\n",
      "| Epoch [198/200] Iter[121/391]\t\tLoss: 2.6274 Acc@1: 51.749% [8014/15488]\n",
      "| Epoch [198/200] Iter[141/391]\t\tLoss: 2.4060 Acc@1: 51.792% [9347/18048]\n",
      "| Epoch [198/200] Iter[161/391]\t\tLoss: 1.9553 Acc@1: 52.160% [10749/20608]\n",
      "| Epoch [198/200] Iter[181/391]\t\tLoss: 1.4581 Acc@1: 52.643% [12196/23168]\n",
      "| Epoch [198/200] Iter[201/391]\t\tLoss: 0.9095 Acc@1: 53.210% [13689/25728]\n",
      "| Epoch [198/200] Iter[221/391]\t\tLoss: 2.6162 Acc@1: 53.742% [15202/28288]\n",
      "| Epoch [198/200] Iter[241/391]\t\tLoss: 2.9513 Acc@1: 53.715% [16569/30848]\n",
      "| Epoch [198/200] Iter[261/391]\t\tLoss: 0.8097 Acc@1: 53.572% [17897/33408]\n",
      "| Epoch [198/200] Iter[281/391]\t\tLoss: 3.0668 Acc@1: 53.704% [19316/35968]\n",
      "| Epoch [198/200] Iter[301/391]\t\tLoss: 2.3507 Acc@1: 53.635% [20664/38528]\n",
      "| Epoch [198/200] Iter[321/391]\t\tLoss: 1.1039 Acc@1: 53.839% [22121/41088]\n",
      "| Epoch [198/200] Iter[341/391]\t\tLoss: 3.0322 Acc@1: 53.653% [23418/43648]\n",
      "| Epoch [198/200] Iter[361/391]\t\tLoss: 2.8747 Acc@1: 53.697% [24812/46208]\n",
      "| Epoch [198/200] Iter[381/391]\t\tLoss: 3.1195 Acc@1: 53.550% [26115/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #198\n",
      "\n",
      "| Validation Epoch #198\t\t\tLoss: 1.2988 Acc@1: 66.40%\n",
      "\n",
      "----- Elapsed time : 0:58:45\n",
      "\n",
      "\n",
      "=> Training Epoch #199\n",
      "| Epoch [199/200] Iter[  1/391]\t\tLoss: 2.6787 Acc@1: 44.449% [ 56/128]\n",
      "| Epoch [199/200] Iter[ 21/391]\t\tLoss: 2.8101 Acc@1: 51.670% [1388/2688]\n",
      "| Epoch [199/200] Iter[ 41/391]\t\tLoss: 1.3330 Acc@1: 53.716% [2819/5248]\n",
      "| Epoch [199/200] Iter[ 61/391]\t\tLoss: 1.6006 Acc@1: 53.882% [4207/7808]\n",
      "| Epoch [199/200] Iter[ 81/391]\t\tLoss: 2.5122 Acc@1: 52.985% [5493/10368]\n",
      "| Epoch [199/200] Iter[101/391]\t\tLoss: 1.1684 Acc@1: 52.236% [6753/12928]\n",
      "| Epoch [199/200] Iter[121/391]\t\tLoss: 2.0579 Acc@1: 51.656% [8000/15488]\n",
      "| Epoch [199/200] Iter[141/391]\t\tLoss: 1.6491 Acc@1: 51.743% [9338/18048]\n",
      "| Epoch [199/200] Iter[161/391]\t\tLoss: 3.1155 Acc@1: 51.814% [10677/20608]\n",
      "| Epoch [199/200] Iter[181/391]\t\tLoss: 0.9325 Acc@1: 51.941% [12033/23168]\n",
      "| Epoch [199/200] Iter[201/391]\t\tLoss: 3.0208 Acc@1: 51.637% [13285/25728]\n",
      "| Epoch [199/200] Iter[221/391]\t\tLoss: 2.4239 Acc@1: 51.184% [14478/28288]\n",
      "| Epoch [199/200] Iter[241/391]\t\tLoss: 2.1944 Acc@1: 51.421% [15862/30848]\n",
      "| Epoch [199/200] Iter[261/391]\t\tLoss: 1.3387 Acc@1: 51.719% [17278/33408]\n",
      "| Epoch [199/200] Iter[281/391]\t\tLoss: 2.6862 Acc@1: 51.950% [18685/35968]\n",
      "| Epoch [199/200] Iter[301/391]\t\tLoss: 1.8942 Acc@1: 52.111% [20077/38528]\n",
      "| Epoch [199/200] Iter[321/391]\t\tLoss: 2.8953 Acc@1: 52.343% [21506/41088]\n",
      "| Epoch [199/200] Iter[341/391]\t\tLoss: 1.4626 Acc@1: 52.222% [22793/43648]\n",
      "| Epoch [199/200] Iter[361/391]\t\tLoss: 2.7637 Acc@1: 52.481% [24250/46208]\n",
      "| Epoch [199/200] Iter[381/391]\t\tLoss: 3.2227 Acc@1: 52.509% [25607/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #199\n",
      "\n",
      "| Validation Epoch #199\t\t\tLoss: 1.2994 Acc@1: 66.36%\n",
      "\n",
      "----- Elapsed time : 0:59:03\n",
      "\n",
      "\n",
      "=> Training Epoch #200\n",
      "| Epoch [200/200] Iter[  1/391]\t\tLoss: 2.9892 Acc@1: 33.819% [ 43/128]\n",
      "| Epoch [200/200] Iter[ 21/391]\t\tLoss: 2.0139 Acc@1: 48.833% [1312/2688]\n",
      "| Epoch [200/200] Iter[ 41/391]\t\tLoss: 1.2578 Acc@1: 50.982% [2675/5248]\n",
      "| Epoch [200/200] Iter[ 61/391]\t\tLoss: 0.9870 Acc@1: 51.889% [4051/7808]\n",
      "| Epoch [200/200] Iter[ 81/391]\t\tLoss: 2.6098 Acc@1: 52.048% [5396/10368]\n",
      "| Epoch [200/200] Iter[101/391]\t\tLoss: 2.6380 Acc@1: 52.225% [6751/12928]\n",
      "| Epoch [200/200] Iter[121/391]\t\tLoss: 1.9996 Acc@1: 51.298% [7945/15488]\n",
      "| Epoch [200/200] Iter[141/391]\t\tLoss: 1.8445 Acc@1: 51.262% [9251/18048]\n",
      "| Epoch [200/200] Iter[161/391]\t\tLoss: 1.1519 Acc@1: 51.496% [10612/20608]\n",
      "| Epoch [200/200] Iter[181/391]\t\tLoss: 2.6540 Acc@1: 51.602% [11955/23168]\n",
      "| Epoch [200/200] Iter[201/391]\t\tLoss: 3.0020 Acc@1: 50.669% [13036/25728]\n",
      "| Epoch [200/200] Iter[221/391]\t\tLoss: 0.7362 Acc@1: 51.105% [14456/28288]\n",
      "| Epoch [200/200] Iter[241/391]\t\tLoss: 1.2993 Acc@1: 51.622% [15924/30848]\n",
      "| Epoch [200/200] Iter[261/391]\t\tLoss: 2.9768 Acc@1: 51.769% [17295/33408]\n",
      "| Epoch [200/200] Iter[281/391]\t\tLoss: 1.0450 Acc@1: 51.897% [18666/35968]\n",
      "| Epoch [200/200] Iter[301/391]\t\tLoss: 3.0661 Acc@1: 51.947% [20014/38528]\n",
      "| Epoch [200/200] Iter[321/391]\t\tLoss: 2.4648 Acc@1: 51.726% [21253/41088]\n",
      "| Epoch [200/200] Iter[341/391]\t\tLoss: 1.4483 Acc@1: 51.650% [22544/43648]\n",
      "| Epoch [200/200] Iter[361/391]\t\tLoss: 1.1813 Acc@1: 51.648% [23865/46208]\n",
      "| Epoch [200/200] Iter[381/391]\t\tLoss: 2.7409 Acc@1: 51.747% [25235/48768]\n",
      "\n",
      "\n",
      "=> Testing Epoch #200\n",
      "\n",
      "| Validation Epoch #200\t\t\tLoss: 1.2999 Acc@1: 66.36%\n",
      "\n",
      "----- Elapsed time : 0:59:20\n",
      "\n",
      "[Phase 3] : Save results\n",
      "| accs:  200\n",
      "| targs: (200, 10000)\n",
      "| preds: (200, 10000)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accs_paths = []\n",
    "\n",
    "for s in random_seed_list:\n",
    "    print('Seed = {} =============================================='.format(s))\n",
    "    print()\n",
    "    \n",
    "    # Set Seed Value\n",
    "    torch.manual_seed(s)\n",
    "    torch.cuda.manual_seed(s)\n",
    "    np.random.seed(s)\n",
    "    random.seed(s)\n",
    "    \n",
    "    # Create Model\n",
    "    print('[Phase 1] : Model setup')\n",
    "    net = fixup_resnet20()\n",
    "\n",
    "    if use_cuda:\n",
    "        net.cuda()\n",
    "        print('Using', torch.cuda.device_count(), 'GPUs.')\n",
    "      \n",
    "    print()\n",
    "\n",
    "    cel = nn.CrossEntropyLoss()\n",
    "    criterion = lambda pred, target, lam: (-F.log_softmax(pred, dim=1) * torch.zeros(pred.size()).cuda().scatter_(1, target.data.view(-1, 1), lam.view(-1, 1))).sum(dim=1).mean()\n",
    "    parameters_bias = [p[1] for p in net.named_parameters() if 'bias' in p[0]]\n",
    "    parameters_scale = [p[1] for p in net.named_parameters() if 'scale' in p[0]]\n",
    "    parameters_others = [p[1] for p in net.named_parameters() if not ('bias' in p[0] or 'scale' in p[0])]\n",
    "    optimizer = optim.SGD(\n",
    "        [{'params': parameters_bias, 'lr': base_lr/10.}, \n",
    "        {'params': parameters_scale, 'lr': base_lr/10.}, \n",
    "        {'params': parameters_others}], \n",
    "        lr=base_learning_rate, \n",
    "        momentum=mom, \n",
    "        weight_decay=decay\n",
    "    )\n",
    "\n",
    "    sgdr = CosineAnnealingLR(optimizer, n_epoch, eta_min=0, last_epoch=-1)\n",
    "\n",
    "    # Set Patches as Weight Tensor\n",
    "    print()\n",
    "    print('----- Layer Weight Statistics --- Begin')\n",
    "    print()\n",
    "    print('layer1 - requires grad = {}'.format(net.conv1.weight.requires_grad))\n",
    "    print('layer1 weights shape: ' + str(net.conv1.weight.shape))\n",
    "    print('min value:            ' + str(torch.min(net.conv1.weight).item()))\n",
    "    print('max value:            ' + str(torch.max(net.conv1.weight).item()))\n",
    "    print()\n",
    "    print('layer2 - requires grad = {}'.format(net.layer1[2].conv2.weight.requires_grad))\n",
    "    print('layer2 weights shape: ' + str(net.layer1[2].conv2.weight.shape))\n",
    "    print('min value:            ' + str(torch.min(net.layer1[2].conv2.weight).item()))\n",
    "    print('max value:            ' + str(torch.max(net.layer1[2].conv2.weight).item()))\n",
    "    print()\n",
    "    #print('layer3 - requires grad = {}'.format(net.layer2[2].conv2.weight.requires_grad))\n",
    "    #print('layer3 weights shape: ' + str(net.layer2[2].conv2.weight.shape))\n",
    "    #print('min value:            ' + str(torch.min(net.layer2[2].conv2.weight).item()))\n",
    "    #print('max value:            ' + str(torch.max(net.layer2[2].conv2.weight).item()))\n",
    "    #print()\n",
    "\n",
    "\n",
    "    print('[Phase 2] : Init Weights with Patches')\n",
    "    print()\n",
    "\n",
    "    patches_layer1.requires_grad = True\n",
    "    patches_layer2.requires_grad = True\n",
    "    #patches_layer3.requires_grad = True\n",
    "\n",
    "    with torch.no_grad():\n",
    "        net.conv1.weight = nn.Parameter(patches_layer1.clone().cuda())\n",
    "        net.layer1[2].conv2.weight = nn.Parameter(patches_layer2.clone().cuda())\n",
    "        #net.layer2[2].conv2.weight = nn.Parameter(patches_layer3.clone().cuda())\n",
    "\n",
    "    print('layer1 - requires grad = {}'.format(net.conv1.weight.requires_grad))\n",
    "    print('layer1 weights shape: ' + str(net.conv1.weight.shape))\n",
    "    print('min value:            ' + str(torch.min(net.conv1.weight).item()))\n",
    "    print('max value:            ' + str(torch.max(net.conv1.weight).item()))\n",
    "    print()\n",
    "    print('layer2 - requires grad = {}'.format(net.layer1[2].conv2.weight.requires_grad))\n",
    "    print('layer2 weights shape: ' + str(net.layer1[2].conv2.weight.shape))\n",
    "    print('min value:            ' + str(torch.min(net.layer1[2].conv2.weight).item()))\n",
    "    print('max value:            ' + str(torch.max(net.layer1[2].conv2.weight).item()))\n",
    "    print()\n",
    "    #print('layer3 - requires grad = {}'.format(net.layer2[2].conv2.weight.requires_grad))\n",
    "    #print('layer3 weights shape: ' + str(net.layer2[2].conv2.weight.shape))\n",
    "    #print('min value:            ' + str(torch.min(net.layer2[2].conv2.weight).item()))\n",
    "    #print('max value:            ' + str(torch.max(net.layer2[2].conv2.weight).item()))\n",
    "    #print()\n",
    "    print('----- Layer Weight Statistics --- End')\n",
    "    print()\n",
    "    \n",
    "    # Start Training\n",
    "    print('[Phase 2] : Training model')\n",
    "    print('| Training Epochs = ' + str(n_epoch))\n",
    "    print()\n",
    "\n",
    "    test_accs = []\n",
    "    test_targs = []\n",
    "    test_preds = []\n",
    "\n",
    "    start_epoch = 1\n",
    "    elapsed_time = 0\n",
    "\n",
    "    for epoch in range(start_epoch, start_epoch+n_epoch):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Train\n",
    "        train(epoch)\n",
    "        print()\n",
    "\n",
    "        lr = 0.\n",
    "        if use_sgdr:\n",
    "            sgdr.step()\n",
    "            for param_group in optimizer.param_groups:\n",
    "                lr = param_group['lr']\n",
    "                break\n",
    "        else:\n",
    "            lr = adjust_learning_rate(optimizer, epoch)\n",
    "    \n",
    "        # Test\n",
    "        test_acc, all_preds, all_targs  = test(epoch)\n",
    "        test_accs.append(test_acc)\n",
    "        test_preds.append(all_preds)\n",
    "        test_targs.append(all_targs)\n",
    "        print()\n",
    "\n",
    "        # Measure time\n",
    "        epoch_time = time.time() - start_time\n",
    "        elapsed_time += epoch_time\n",
    "        print('----- Elapsed time : %d:%02d:%02d'  % (get_hms(elapsed_time)))\n",
    "        print()\n",
    "    \n",
    "    # Save Result\n",
    "    test_preds = np.vstack(np.array(test_preds))\n",
    "    test_targs = np.vstack(np.array(test_targs))\n",
    "\n",
    "    print('[Phase 3] : Save results')\n",
    "    print('| accs:  {}'.format(len(test_accs)))\n",
    "    print('| targs: {}'.format(test_preds.shape))\n",
    "    print('| preds: {}'.format(test_targs.shape))\n",
    "\n",
    "    result_accs_path = result_accs_base_path + '-seed{}.txt'.format(s)\n",
    "    result_preds_path = result_preds_base_path + '-seed{}.txt'.format(s)\n",
    "    result_targs_path = result_targs_base_path + '-seed{}.txt'.format(s)\n",
    "\n",
    "    accs_paths.append(result_accs_path)\n",
    "\n",
    "    with open(result_accs_path, \"wb\") as fp:\n",
    "        pickle.dump(test_accs, fp)\n",
    "\n",
    "    with open(result_preds_path, \"wb\") as fp:\n",
    "        pickle.dump(test_preds, fp)\n",
    "\n",
    "    with open(result_targs_path, \"wb\") as fp:\n",
    "        pickle.dump(test_targs, fp)\n",
    "\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DABRWR23RSz2"
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "77AGWkMbRYL0"
   },
   "outputs": [],
   "source": [
    "def plot_results(results, names, stds, ymin):\n",
    "    for i in range(len(results)):\n",
    "        x = np.arange(len(results[i]))\n",
    "        plt.errorbar(x, results[i], yerr=stds[i])\n",
    "    \n",
    "    plt.legend(names)\n",
    "    plt.ylim(ymin, 100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T-qLJErRSNSn"
   },
   "outputs": [],
   "source": [
    "accs_sota_results = []\n",
    "\n",
    "for p in accs_paths:\n",
    "    # Load Results\n",
    "    with open(p, \"rb\") as fp:\n",
    "        test_accs_tmp = pickle.load(fp)\n",
    "        \n",
    "    accs_sota_results.append(test_accs_tmp)\n",
    "\n",
    "accs_sota_results = np.array(accs_sota_results)\n",
    "avg_accs_sota_results = np.mean(accs_sota_results, axis=0)\n",
    "std_accs_sota_results = np.std(accs_sota_results, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 729,
     "status": "ok",
     "timestamp": 1599745292462,
     "user": {
      "displayName": "Bam Power",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgSMnvpPSpQHyFMhcdXXX00QS82LMCMbDQiszLr=s64",
      "userId": "17908920294202220192"
     },
     "user_tz": -120
    },
    "id": "t2dQi1amSbA-",
    "outputId": "ae4098af-598f-4ca1-e278-5088e8397ea1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_results([avg_accs_sota_results], ['sota'], [std_accs_sota_results], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tL1xtlrHzHXh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMqkAEttp8IWlATG4ANAjB5",
   "collapsed_sections": [],
   "name": "cifar100-fixup-patch.ipynb",
   "provenance": [
    {
     "file_id": "1q4SfCMy08CLoh2gmcfpK9Czass64y0rj",
     "timestamp": 1599738026724
    },
    {
     "file_id": "1Zonjv8MhbkN1huRf0z0sQiyBOhxqZyxW",
     "timestamp": 1599693003995
    },
    {
     "file_id": "1fztxAmVV59lekCT_Ze5ZF3Xygf32Fc66",
     "timestamp": 1599687885699
    },
    {
     "file_id": "1V14U70cea1M3MdbUWyXeLFXUecuaWaRc",
     "timestamp": 1599671580522
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
