{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Projections\n",
    "\n",
    "<br/>\n",
    "\n",
    "<pre>\n",
    "model name:            imagenette_128_resnet18_model.pth\n",
    "network architecture:  resnet18\n",
    "dataset:               imagenette training set\n",
    "image size:            128x128 (resized beforehand)\n",
    "</pre>\n",
    "\n",
    "<br/>\n",
    "\n",
    "We want to test our Out-of-Distribution (OoD) detection method __Layer-wise Activation Cluster Analysis (LACA)__ on a dataset that is more complex than the MNIST, SVHN or the CIFAR-10 dataset which have been used so far. We chose the [Imagenette dataset](https://github.com/fastai/imagenette) as it contains images showing more complex scenes. The [Imagenette dataset](https://github.com/fastai/imagenette) is a subset of 10 classes of the [ImageNet dataset](https://www.image-net.org/). \n",
    "\n",
    "The first step of our OoD detection method is executed before inference. Here we measure in-distribution statistics from the training data and OoD statistics from the calibration data. Both kind of statistics are necessary to calculate the credibility of a test sample at inference. \n",
    "\n",
    "After fetching (see __01_fetch_activations_imagenette_128_resnet18.ipynb__) and vectorizing the activations (see __02_vectorize_activations_imagenette_128_resnet18.ipynb__) from the data samples we get the 2D projections of the activations. This is necessary because to obtain the statistics we need to find clusters in the data. However, finding clusters in high-dimensional spaces is difficult. Thus, we get the projections.\n",
    "\n",
    "<br/>\n",
    "\n",
    "_Sources:_\n",
    "* [Imagenette dataset](https://github.com/fastai/imagenette)\n",
    "* [Deep kNN paper](https://arxiv.org/abs/1803.04765)\n",
    "* [Deep kNN sample code](https://github.com/cleverhans-lab/cleverhans/blob/master/cleverhans_v3.1.0/cleverhans/model_zoo/deep_k_nearest_neighbors/dknn.py)\n",
    "* [Deep kNN sample code (PyTorch)](https://github.com/bam098/deep_knn/blob/master/dknn_mnist.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version:      3.6.9\n",
      "torch version:       1.7.0\n",
      "torchvision version: 0.8.1\n",
      "sklearn version:     0.23.2\n",
      "skimage version:     0.17.2\n",
      "numpy version:       1.19.5\n",
      "matplotlib version:  3.2.2\n",
      "seaborn version:     0.11.0\n",
      "pandas version:      1.1.4\n",
      "pickle version:      4.0\n",
      "CUDA available:      False\n",
      "cuDNN enabled:       True\n",
      "num gpus:            0\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import adaptive_avg_pool2d\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms, models, datasets\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "import skimage\n",
    "from skimage.measure import block_reduce\n",
    "from umap import UMAP\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import platform\n",
    "from pathlib import Path\n",
    "import random\n",
    "import warnings\n",
    "import pprint\n",
    "from collections import Counter\n",
    "\n",
    "sns.set()\n",
    "sns.set_context(\"notebook\", font_scale=1.1)\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "print('python version:      {}'.format(platform.python_version()))\n",
    "print('torch version:       {}'.format(torch.__version__))\n",
    "print('torchvision version: {}'.format(torchvision.__version__))\n",
    "print('sklearn version:     {}'.format(sklearn.__version__))\n",
    "print('skimage version:     {}'.format(skimage.__version__))\n",
    "print('numpy version:       {}'.format(np.__version__))\n",
    "print('matplotlib version:  {}'.format(matplotlib.__version__))\n",
    "print('seaborn version:     {}'.format(sns.__version__))\n",
    "print('pandas version:      {}'.format(pd.__version__))\n",
    "print('pickle version:      {}'.format(pickle.format_version))\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('CUDA available:      {}'.format(use_cuda))\n",
    "print('cuDNN enabled:       {}'.format(torch.backends.cudnn.enabled))\n",
    "print('num gpus:            {}'.format(torch.cuda.device_count()))\n",
    "\n",
    "if use_cuda:\n",
    "    print('gpu:                 {}'.format(torch.cuda.get_device_name(0)))\n",
    "\n",
    "    print()\n",
    "    print('------------------------- CUDA -------------------------')\n",
    "    ! nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the seed values to obtain reproducible results. For more information how to set seed values in Python and Pytorch see the [Pytorch documentation](https://pytorch.org/docs/1.7.0/notes/randomness.html?highlight=repro)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.set_deterministic(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-26T12:01:04.424348Z",
     "iopub.status.busy": "2022-09-26T12:01:04.423665Z",
     "iopub.status.idle": "2022-09-26T12:01:04.646403Z",
     "shell.execute_reply": "2022-09-26T12:01:04.645378Z",
     "shell.execute_reply.started": "2022-09-26T12:01:04.424311Z"
    }
   },
   "outputs": [],
   "source": [
    "# Activations\n",
    "img_size          = 128                                                             # Image size\n",
    "base_act_folder   = Path('/Users/lehmann/research/laca3/activations/imagenette')    # Base activations folder\n",
    "afname_string     = 'imagenette_{}_resnet18_acts'.format(img_size)                  # Activations file name\n",
    "acts_path         = base_act_folder/afname_string                                   # Activations path\n",
    "\n",
    "\n",
    "# Projections\n",
    "base_projs_folder = Path('/Users/lehmann/research/laca3/projections/imagenette')    # Base projection folder\n",
    "pfname_string     = 'imagenette_{}_resnet18_projs'.format(img_size)                 # Projections file name\n",
    "projs_path        = base_projs_folder/pfname_string                                 # Activations path\n",
    "layer_names       = [                                                               # List of layer names \n",
    "    'relu',\n",
    "    'maxpool',\n",
    "    'layer1-0',\n",
    "    'layer1-1',\n",
    "    'layer2-0',\n",
    "    'layer2-1',\n",
    "    'layer3-0',\n",
    "    'layer3-1',\n",
    "    'layer4-0',\n",
    "    'layer4-1',\n",
    "    'avgpool'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Function for Getting the Projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_projections(dataset_name, reduce, ext_scaler=None, ext_reducer=None):\n",
    "\n",
    "    for layer_name in layer_names:\n",
    "        print('## Computing Projections for Layer {}'.format(layer_name))\n",
    "        \n",
    "        # Load activations\n",
    "        fname = str(acts_path) + '_{}_{}_vectors.pkl'.format(dataset_name, layer_name)\n",
    "        with open(fname, 'rb') as pickle_file:\n",
    "            loaded_activations = pickle.load(pickle_file)\n",
    "        \n",
    "        # Normalize activations        \n",
    "        if ext_scaler:\n",
    "            print('- external scaler is used')\n",
    "            scaler = ext_scaler[layer_name]\n",
    "        else:\n",
    "            scaler = preprocessing.StandardScaler()       \n",
    "            scaler.fit(loaded_activations['activations'])\n",
    "            \n",
    "        layer_activations_norm = scaler.transform(loaded_activations['activations'])\n",
    "        \n",
    "        print('- activations normalized: {}'.format(layer_activations_norm.shape))\n",
    "        \n",
    "        # Reduce activations\n",
    "        if ext_reducer:\n",
    "            print('- external reducer is used')\n",
    "            layer_projections, reducer = reduce(layer_activations_norm, ext_reducer[layer_name])\n",
    "        else:\n",
    "            layer_projections, reducer = reduce(layer_activations_norm)\n",
    "            \n",
    "        print('- activations reduced: {}'.format(layer_projections.shape))\n",
    "        \n",
    "        # Save projections and reducer\n",
    "        projections = {}\n",
    "        projections['projections'] = layer_projections\n",
    "        projections['targets'] = loaded_activations['targets']\n",
    "        projections['scaler'] = scaler\n",
    "        projections['reducer'] = reducer\n",
    "        \n",
    "        fname = str(projs_path) + '_{}_{}.pkl'.format(dataset_name, layer_name)\n",
    "        with open(fname, 'wb') as pickle_file:\n",
    "            pickle.dump(projections, pickle_file, protocol=4)\n",
    "\n",
    "        print(\"done!\")        \n",
    "        print()\n",
    "\n",
    "def pca_umap_reduce(activations, ext_reducer=None):\n",
    "    # An external reducer is used\n",
    "    if ext_reducer:\n",
    "        proj_temp = activations\n",
    "        for reducer in ext_reducer:\n",
    "            proj_temp = reducer.transform(proj_temp)\n",
    "            \n",
    "        return proj_temp, ext_reducer\n",
    "    \n",
    "    # No external reducer available, we must fit a reducer\n",
    "    umap_reducer = UMAP(n_components=2, metric='cosine', n_neighbors=15, min_dist=0.0)\n",
    "    \n",
    "    if activations.shape[1] > 50:\n",
    "        pca_reducer = PCA(n_components=50)\n",
    "        pca_reducer.fit(activations)\n",
    "        proj_temp = pca_reducer.transform(activations)\n",
    "        umap_reducer.fit(proj_temp)\n",
    "        return umap_reducer.transform(proj_temp), [pca_reducer, umap_reducer]\n",
    "    else:\n",
    "        umap_reducer.fit(activations)\n",
    "        return umap_reducer.transform(activations), [umap_reducer]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Computing Projections for Layer relu\n",
      "- activations normalized: (9469, 262144)\n",
      "- activations reduced: (9469, 2)\n",
      "done!\n",
      "\n",
      "## Computing Projections for Layer maxpool\n",
      "- activations normalized: (9469, 65536)\n",
      "- activations reduced: (9469, 2)\n",
      "done!\n",
      "\n",
      "## Computing Projections for Layer layer1-0\n",
      "- activations normalized: (9469, 65536)\n",
      "- activations reduced: (9469, 2)\n",
      "done!\n",
      "\n",
      "## Computing Projections for Layer layer1-1\n",
      "- activations normalized: (9469, 65536)\n",
      "- activations reduced: (9469, 2)\n",
      "done!\n",
      "\n",
      "## Computing Projections for Layer layer2-0\n",
      "- activations normalized: (9469, 32768)\n",
      "- activations reduced: (9469, 2)\n",
      "done!\n",
      "\n",
      "## Computing Projections for Layer layer2-1\n",
      "- activations normalized: (9469, 32768)\n",
      "- activations reduced: (9469, 2)\n",
      "done!\n",
      "\n",
      "## Computing Projections for Layer layer3-0\n",
      "- activations normalized: (9469, 16384)\n",
      "- activations reduced: (9469, 2)\n",
      "done!\n",
      "\n",
      "## Computing Projections for Layer layer3-1\n",
      "- activations normalized: (9469, 16384)\n",
      "- activations reduced: (9469, 2)\n",
      "done!\n",
      "\n",
      "## Computing Projections for Layer layer4-0\n",
      "- activations normalized: (9469, 8192)\n",
      "- activations reduced: (9469, 2)\n",
      "done!\n",
      "\n",
      "## Computing Projections for Layer layer4-1\n",
      "- activations normalized: (9469, 8192)\n",
      "- activations reduced: (9469, 2)\n",
      "done!\n",
      "\n",
      "## Computing Projections for Layer avgpool\n",
      "- activations normalized: (9469, 512)\n",
      "- activations reduced: (9469, 2)\n",
      "done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainset_name = 'trainset'\n",
    "get_projections(trainset_name, pca_umap_reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## layer relu\n",
      "activations: (9469, 2), targets: (9469,)\n",
      "scaler:      StandardScaler()\n",
      "reducer:     [PCA(n_components=50), UMAP(angular_rp_forest=True, metric='cosine', min_dist=0.0)]\n",
      "\n",
      "## layer maxpool\n",
      "activations: (9469, 2), targets: (9469,)\n",
      "scaler:      StandardScaler()\n",
      "reducer:     [PCA(n_components=50), UMAP(angular_rp_forest=True, metric='cosine', min_dist=0.0)]\n",
      "\n",
      "## layer layer1-0\n",
      "activations: (9469, 2), targets: (9469,)\n",
      "scaler:      StandardScaler()\n",
      "reducer:     [PCA(n_components=50), UMAP(angular_rp_forest=True, metric='cosine', min_dist=0.0)]\n",
      "\n",
      "## layer layer1-1\n",
      "activations: (9469, 2), targets: (9469,)\n",
      "scaler:      StandardScaler()\n",
      "reducer:     [PCA(n_components=50), UMAP(angular_rp_forest=True, metric='cosine', min_dist=0.0)]\n",
      "\n",
      "## layer layer2-0\n",
      "activations: (9469, 2), targets: (9469,)\n",
      "scaler:      StandardScaler()\n",
      "reducer:     [PCA(n_components=50), UMAP(angular_rp_forest=True, metric='cosine', min_dist=0.0)]\n",
      "\n",
      "## layer layer2-1\n",
      "activations: (9469, 2), targets: (9469,)\n",
      "scaler:      StandardScaler()\n",
      "reducer:     [PCA(n_components=50), UMAP(angular_rp_forest=True, metric='cosine', min_dist=0.0)]\n",
      "\n",
      "## layer layer3-0\n",
      "activations: (9469, 2), targets: (9469,)\n",
      "scaler:      StandardScaler()\n",
      "reducer:     [PCA(n_components=50), UMAP(angular_rp_forest=True, metric='cosine', min_dist=0.0)]\n",
      "\n",
      "## layer layer3-1\n",
      "activations: (9469, 2), targets: (9469,)\n",
      "scaler:      StandardScaler()\n",
      "reducer:     [PCA(n_components=50), UMAP(angular_rp_forest=True, metric='cosine', min_dist=0.0)]\n",
      "\n",
      "## layer layer4-0\n",
      "activations: (9469, 2), targets: (9469,)\n",
      "scaler:      StandardScaler()\n",
      "reducer:     [PCA(n_components=50), UMAP(angular_rp_forest=True, metric='cosine', min_dist=0.0)]\n",
      "\n",
      "## layer layer4-1\n",
      "activations: (9469, 2), targets: (9469,)\n",
      "scaler:      StandardScaler()\n",
      "reducer:     [PCA(n_components=50), UMAP(angular_rp_forest=True, metric='cosine', min_dist=0.0)]\n",
      "\n",
      "## layer avgpool\n",
      "activations: (9469, 2), targets: (9469,)\n",
      "scaler:      StandardScaler()\n",
      "reducer:     [PCA(n_components=50), UMAP(angular_rp_forest=True, metric='cosine', min_dist=0.0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loaded_train_projections = {}\n",
    "\n",
    "for layer_name in layer_names:\n",
    "    print('## layer {}'.format(layer_name))\n",
    "        \n",
    "    fname = str(projs_path) + '_{}_{}.pkl'.format(trainset_name, layer_name)\n",
    "    with open(fname, 'rb') as pickle_file:\n",
    "        loaded_train_projections = pickle.load(pickle_file)\n",
    "    \n",
    "    print('activations: {}, targets: {}'.format(\n",
    "        loaded_train_projections['projections'].shape, loaded_train_projections['targets'].shape\n",
    "    ))  \n",
    "    print('scaler:      {}'.format(loaded_train_projections['scaler']))     \n",
    "    print('reducer:     {}'.format(loaded_train_projections['reducer']))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
